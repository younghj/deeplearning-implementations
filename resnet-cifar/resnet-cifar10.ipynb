{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Add\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.layers import GlobalAveragePooling2D\n",
    "# from keras.optimizers import Adam, SGD\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "from keras import backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "xx_train = xx_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "mean = np.mean(xx_train, axis=0)\n",
    "xx_train -= mean\n",
    "x_test -= mean\n",
    "\n",
    "x_train = xx_train[:45000]\n",
    "y_train = yy_train[:45000]\n",
    "x_valid = xx_train[45000:50000]\n",
    "y_valid = yy_train[45000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    shape1 = backend.int_shape(a)\n",
    "    shape2 = backend.int_shape(b)\n",
    "    w = int(round(shape1[1]/shape2[1]))\n",
    "    h = int(round(shape1[2]/shape2[2]))\n",
    "    eq = shape1[3] == shape2[3]\n",
    "    \n",
    "    tmp = a\n",
    "    print w,h,eq\n",
    "    print shape1, shape2\n",
    "    if w>1 or h>1 or not eq:\n",
    "        tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(a)\n",
    "    print backend.int_shape(tmp)\n",
    "    print\n",
    "    return Add()([tmp, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "def layer(num_filt, size, strides, inp):\n",
    "    tmp = BatchNormalization(axis=3)(inp)\n",
    "    tmp = Activation('relu')(tmp)\n",
    "    tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(tmp)\n",
    "    return tmp\n",
    "\n",
    "l = Input(x_train.shape[1:])\n",
    "\n",
    "x = Conv2D(128, (7,7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#first layer\n",
    "num = 32\n",
    "xtmp = Conv2D(num, (1,1), strides=1, \n",
    "              padding='same', \n",
    "              kernel_initializer='he_normal', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "\n",
    "x = add(x,xtmp)\n",
    "\n",
    "#other layer\n",
    "xtmp = layer(num, (1,1), 1, x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "x = add(x,xtmp)\n",
    "\n",
    "for i in xrange(9):\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    #other layer\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "xshape = backend.int_shape(x)\n",
    "x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 128)  18944       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   4128        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 128)  4224        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 128)  0           activation_1[0][0]               \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 32)   4128        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 32)   9248        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 32)   128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  4224        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 128)  0           add_1[0][0]                      \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4128        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 128)  4224        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 128)  0           add_2[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   4128        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 128)  4224        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   4128        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 32)   128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  4224        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   4128        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 128)  4224        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   4128        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 128)  4224        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 128)  0           add_6[0][0]                      \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   4128        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 32)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 128)  4224        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 128)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 128)  512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   4128        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 32)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 128)  4224        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 128)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 128)  512         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   4128        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 32)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 32)   9248        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 32)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 128)  4224        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 128)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 128)  512         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 32)   4128        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 32)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 32)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 128)  4224        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 128)  0           add_10[0][0]                     \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 32)   4128        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 32)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 32)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 32)   9248        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 128)  4224        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 128)  0           add_11[0][0]                     \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 32)   4128        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 32)   128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 32)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 32)   9248        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 32)   128         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 32)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 128)  4224        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 128)  0           add_12[0][0]                     \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 32)   4128        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 32)   128         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 32)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 32)   9248        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 32)   128         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 32)   0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 128)  4224        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 128)  0           add_13[0][0]                     \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 32)   4128        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 32)   128         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 32)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 32)   9248        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 32)   128         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 16, 32)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 16, 16, 128)  4224        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 128)  0           add_14[0][0]                     \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 16, 16, 32)   4128        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 16, 16, 32)   128         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 32)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 16, 16, 32)   9248        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 16, 16, 32)   128         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 16, 32)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 16, 16, 128)  4224        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 128)  0           add_15[0][0]                     \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 16, 16, 128)  512         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 16, 16, 128)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 16, 16, 32)   4128        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 16, 16, 32)   128         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 16, 16, 32)   0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 16, 16, 32)   9248        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 16, 16, 32)   128         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 16, 32)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 128)  4224        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 16, 16, 128)  0           add_16[0][0]                     \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 16, 16, 128)  512         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 16, 16, 128)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 16, 16, 32)   4128        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 16, 16, 32)   128         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 16, 16, 32)   0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 16, 16, 32)   9248        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 16, 16, 32)   128         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 16, 16, 32)   0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 128)  4224        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 128)  0           add_17[0][0]                     \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 128)  512         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 128)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 32)   4128        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 32)   128         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 32)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 32)   9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 32)   128         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 32)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 128)  4224        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 16, 16, 128)  0           add_18[0][0]                     \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 128)  512         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 128)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 32)   4128        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 32)   128         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 32)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 32)   9248        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 32)   128         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 32)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 128)  4224        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 16, 16, 128)  0           add_19[0][0]                     \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 128)  512         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 128)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 128)    0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1290        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 388,106\n",
      "Trainable params: 380,170\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if training was cut short\n",
    "model = load_model('weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7966 - acc: 0.8058Epoch 00001: val_acc improved from -inf to 0.73160, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1545s 34ms/step - loss: 0.7966 - acc: 0.8058 - val_loss: 1.1426 - val_acc: 0.7316\n",
      "Epoch 2/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7692 - acc: 0.8158Epoch 00002: val_acc improved from 0.73160 to 0.77120, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1517s 34ms/step - loss: 0.7692 - acc: 0.8158 - val_loss: 0.9081 - val_acc: 0.7712\n",
      "Epoch 3/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7405 - acc: 0.8244Epoch 00003: val_acc did not improve\n",
      "45000/45000 [==============================] - 1514s 34ms/step - loss: 0.7405 - acc: 0.8244 - val_loss: 0.9733 - val_acc: 0.7486\n",
      "Epoch 4/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7213 - acc: 0.8326Epoch 00004: val_acc did not improve\n",
      "45000/45000 [==============================] - 1497s 33ms/step - loss: 0.7213 - acc: 0.8326 - val_loss: 0.9870 - val_acc: 0.7494\n",
      "Epoch 5/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6992 - acc: 0.8407Epoch 00005: val_acc did not improve\n",
      "45000/45000 [==============================] - 1500s 33ms/step - loss: 0.6991 - acc: 0.8407 - val_loss: 1.0767 - val_acc: 0.7344\n",
      "Epoch 6/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6850 - acc: 0.8477Epoch 00006: val_acc did not improve\n",
      "45000/45000 [==============================] - 1506s 33ms/step - loss: 0.6851 - acc: 0.8477 - val_loss: 0.9288 - val_acc: 0.7664\n",
      "Epoch 7/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8562Epoch 00007: val_acc did not improve\n",
      "45000/45000 [==============================] - 1508s 34ms/step - loss: 0.6659 - acc: 0.8562 - val_loss: 1.1291 - val_acc: 0.7152\n",
      "Epoch 8/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6531 - acc: 0.8593Epoch 00008: val_acc did not improve\n",
      "45000/45000 [==============================] - 1522s 34ms/step - loss: 0.6533 - acc: 0.8593 - val_loss: 1.1950 - val_acc: 0.7326\n",
      "Epoch 9/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.8677Epoch 00009: val_acc did not improve\n",
      "45000/45000 [==============================] - 1534s 34ms/step - loss: 0.6389 - acc: 0.8677 - val_loss: 1.0425 - val_acc: 0.7322\n",
      "Epoch 10/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6327 - acc: 0.8688Epoch 00010: val_acc did not improve\n",
      "45000/45000 [==============================] - 1522s 34ms/step - loss: 0.6327 - acc: 0.8688 - val_loss: 0.9930 - val_acc: 0.7662\n",
      "Epoch 11/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6249 - acc: 0.8726Epoch 00011: val_acc did not improve\n",
      "45000/45000 [==============================] - 1515s 34ms/step - loss: 0.6249 - acc: 0.8726 - val_loss: 1.1222 - val_acc: 0.7404\n",
      "Epoch 12/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6123 - acc: 0.8766Epoch 00012: val_acc improved from 0.77120 to 0.78300, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1488s 33ms/step - loss: 0.6123 - acc: 0.8766 - val_loss: 0.9948 - val_acc: 0.7830\n",
      "Epoch 13/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.8804Epoch 00013: val_acc improved from 0.78300 to 0.78580, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1490s 33ms/step - loss: 0.6025 - acc: 0.8804 - val_loss: 0.9612 - val_acc: 0.7858\n",
      "Epoch 14/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5981 - acc: 0.8844Epoch 00014: val_acc did not improve\n",
      "45000/45000 [==============================] - 1494s 33ms/step - loss: 0.5982 - acc: 0.8843 - val_loss: 1.1270 - val_acc: 0.7384\n",
      "Epoch 15/200\n",
      "44992/45000 [============================>.] - ETA: 9s - loss: 0.5840 - acc: 0.8897 Epoch 00015: val_acc did not improve\n",
      "45000/45000 [==============================] - 51633s 1s/step - loss: 0.5840 - acc: 0.8897 - val_loss: 1.0615 - val_acc: 0.7528\n",
      "Epoch 16/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5864 - acc: 0.8887Epoch 00016: val_acc improved from 0.78580 to 0.80260, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1551s 34ms/step - loss: 0.5863 - acc: 0.8888 - val_loss: 0.8870 - val_acc: 0.8026\n",
      "Epoch 17/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8929Epoch 00017: val_acc did not improve\n",
      "45000/45000 [==============================] - 2159s 48ms/step - loss: 0.5755 - acc: 0.8929 - val_loss: 0.9526 - val_acc: 0.7852\n",
      "Epoch 18/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.8927Epoch 00018: val_acc did not improve\n",
      "45000/45000 [==============================] - 2570s 57ms/step - loss: 0.5784 - acc: 0.8927 - val_loss: 1.3154 - val_acc: 0.7064\n",
      "Epoch 19/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5729 - acc: 0.8952Epoch 00019: val_acc did not improve\n",
      "45000/45000 [==============================] - 1735s 39ms/step - loss: 0.5731 - acc: 0.8952 - val_loss: 1.1324 - val_acc: 0.7510\n",
      "Epoch 20/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5679 - acc: 0.8989Epoch 00020: val_acc did not improve\n",
      "45000/45000 [==============================] - 1564s 35ms/step - loss: 0.5679 - acc: 0.8990 - val_loss: 1.0447 - val_acc: 0.7830\n",
      "Epoch 21/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5596 - acc: 0.9003Epoch 00021: val_acc did not improve\n",
      "45000/45000 [==============================] - 1540s 34ms/step - loss: 0.5597 - acc: 0.9002 - val_loss: 1.0056 - val_acc: 0.7796\n",
      "Epoch 22/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5573 - acc: 0.9034Epoch 00022: val_acc did not improve\n",
      "45000/45000 [==============================] - 1550s 34ms/step - loss: 0.5573 - acc: 0.9034 - val_loss: 1.0400 - val_acc: 0.7904\n",
      "Epoch 23/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5583 - acc: 0.9035Epoch 00023: val_acc did not improve\n",
      "45000/45000 [==============================] - 1541s 34ms/step - loss: 0.5585 - acc: 0.9034 - val_loss: 1.0821 - val_acc: 0.7630\n",
      "Epoch 24/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5486 - acc: 0.9066Epoch 00024: val_acc did not improve\n",
      "45000/45000 [==============================] - 1540s 34ms/step - loss: 0.5486 - acc: 0.9066 - val_loss: 1.2331 - val_acc: 0.7414\n",
      "Epoch 25/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5518 - acc: 0.9059Epoch 00025: val_acc did not improve\n",
      "45000/45000 [==============================] - 1539s 34ms/step - loss: 0.5519 - acc: 0.9059 - val_loss: 1.0709 - val_acc: 0.7722\n",
      "Epoch 26/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5475 - acc: 0.9078Epoch 00026: val_acc did not improve\n",
      "45000/45000 [==============================] - 1537s 34ms/step - loss: 0.5475 - acc: 0.9078 - val_loss: 1.1120 - val_acc: 0.7596\n",
      "Epoch 27/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5449 - acc: 0.9093Epoch 00027: val_acc did not improve\n",
      "45000/45000 [==============================] - 1537s 34ms/step - loss: 0.5450 - acc: 0.9093 - val_loss: 1.0862 - val_acc: 0.7800\n",
      "Epoch 28/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5415 - acc: 0.9111Epoch 00028: val_acc did not improve\n",
      "45000/45000 [==============================] - 1538s 34ms/step - loss: 0.5415 - acc: 0.9111 - val_loss: 1.1622 - val_acc: 0.7492\n",
      "Epoch 29/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.9141Epoch 00029: val_acc did not improve\n",
      "45000/45000 [==============================] - 1540s 34ms/step - loss: 0.5369 - acc: 0.9140 - val_loss: 1.1416 - val_acc: 0.7438\n",
      "Epoch 30/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5398 - acc: 0.9140Epoch 00030: val_acc did not improve\n",
      "45000/45000 [==============================] - 1537s 34ms/step - loss: 0.5398 - acc: 0.9140 - val_loss: 1.3592 - val_acc: 0.7372\n",
      "Epoch 31/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5339 - acc: 0.9154Epoch 00031: val_acc did not improve\n",
      "45000/45000 [==============================] - 1525s 34ms/step - loss: 0.5339 - acc: 0.9154 - val_loss: 1.3050 - val_acc: 0.7496\n",
      "Epoch 32/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5266 - acc: 0.9191Epoch 00032: val_acc did not improve\n",
      "45000/45000 [==============================] - 1521s 34ms/step - loss: 0.5267 - acc: 0.9190 - val_loss: 1.2806 - val_acc: 0.7516\n",
      "Epoch 33/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5384 - acc: 0.9137Epoch 00033: val_acc did not improve\n",
      "45000/45000 [==============================] - 1522s 34ms/step - loss: 0.5384 - acc: 0.9137 - val_loss: 1.1648 - val_acc: 0.7756\n",
      "Epoch 34/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5225 - acc: 0.9217Epoch 00034: val_acc did not improve\n",
      "45000/45000 [==============================] - 1526s 34ms/step - loss: 0.5225 - acc: 0.9217 - val_loss: 1.1943 - val_acc: 0.7670\n",
      "Epoch 35/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.9173Epoch 00035: val_acc improved from 0.80260 to 0.81120, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 4045s 90ms/step - loss: 0.5301 - acc: 0.9173 - val_loss: 0.9402 - val_acc: 0.8112\n",
      "Epoch 36/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.9194Epoch 00036: val_acc did not improve\n",
      "45000/45000 [==============================] - 2340s 52ms/step - loss: 0.5276 - acc: 0.9194 - val_loss: 1.4599 - val_acc: 0.7196\n",
      "Epoch 37/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5235 - acc: 0.9209Epoch 00037: val_acc did not improve\n",
      "45000/45000 [==============================] - 2307s 51ms/step - loss: 0.5234 - acc: 0.9209 - val_loss: 1.1051 - val_acc: 0.7782\n",
      "Epoch 38/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.9199Epoch 00038: val_acc did not improve\n",
      "45000/45000 [==============================] - 1571s 35ms/step - loss: 0.5257 - acc: 0.9199 - val_loss: 1.0640 - val_acc: 0.7802\n",
      "Epoch 39/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.9219Epoch 00039: val_acc did not improve\n",
      "45000/45000 [==============================] - 1506s 33ms/step - loss: 0.5186 - acc: 0.9219 - val_loss: 1.1884 - val_acc: 0.7588\n",
      "Epoch 40/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.9219Epoch 00040: val_acc did not improve\n",
      "45000/45000 [==============================] - 1499s 33ms/step - loss: 0.5222 - acc: 0.9219 - val_loss: 1.1349 - val_acc: 0.7690\n",
      "Epoch 41/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.9237Epoch 00041: val_acc did not improve\n",
      "45000/45000 [==============================] - 1500s 33ms/step - loss: 0.5155 - acc: 0.9237 - val_loss: 1.0774 - val_acc: 0.7742\n",
      "Epoch 42/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5145 - acc: 0.9249Epoch 00042: val_acc did not improve\n",
      "45000/45000 [==============================] - 1501s 33ms/step - loss: 0.5145 - acc: 0.9250 - val_loss: 1.2451 - val_acc: 0.7504\n",
      "Epoch 43/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5156 - acc: 0.9225Epoch 00043: val_acc did not improve\n",
      "45000/45000 [==============================] - 1488s 33ms/step - loss: 0.5156 - acc: 0.9225 - val_loss: 1.3323 - val_acc: 0.7482\n",
      "Epoch 44/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.9245Epoch 00044: val_acc did not improve\n",
      "45000/45000 [==============================] - 1481s 33ms/step - loss: 0.5152 - acc: 0.9246 - val_loss: 1.1812 - val_acc: 0.7764\n",
      "Epoch 45/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5092 - acc: 0.9273Epoch 00045: val_acc did not improve\n",
      "45000/45000 [==============================] - 1481s 33ms/step - loss: 0.5092 - acc: 0.9273 - val_loss: 1.1199 - val_acc: 0.7720\n",
      "Epoch 46/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5121 - acc: 0.9263Epoch 00046: val_acc did not improve\n",
      "45000/45000 [==============================] - 1485s 33ms/step - loss: 0.5120 - acc: 0.9263 - val_loss: 1.1194 - val_acc: 0.7842\n",
      "Epoch 47/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.9269Epoch 00047: val_acc did not improve\n",
      "45000/45000 [==============================] - 1492s 33ms/step - loss: 0.5080 - acc: 0.9269 - val_loss: 1.1916 - val_acc: 0.7634\n",
      "Epoch 48/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5056 - acc: 0.9292Epoch 00048: val_acc did not improve\n",
      "45000/45000 [==============================] - 1486s 33ms/step - loss: 0.5057 - acc: 0.9292 - val_loss: 1.3183 - val_acc: 0.7454\n",
      "Epoch 49/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5069 - acc: 0.9278Epoch 00049: val_acc did not improve\n",
      "45000/45000 [==============================] - 1494s 33ms/step - loss: 0.5070 - acc: 0.9278 - val_loss: 1.2055 - val_acc: 0.7710\n",
      "Epoch 50/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5059 - acc: 0.9294Epoch 00050: val_acc did not improve\n",
      "45000/45000 [==============================] - 1491s 33ms/step - loss: 0.5059 - acc: 0.9294 - val_loss: 1.0072 - val_acc: 0.7936\n",
      "Epoch 51/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.9267Epoch 00051: val_acc did not improve\n",
      "45000/45000 [==============================] - 1491s 33ms/step - loss: 0.5076 - acc: 0.9268 - val_loss: 1.1653 - val_acc: 0.7672\n",
      "Epoch 52/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5044 - acc: 0.9308Epoch 00052: val_acc did not improve\n",
      "45000/45000 [==============================] - 1499s 33ms/step - loss: 0.5044 - acc: 0.9308 - val_loss: 1.0835 - val_acc: 0.7888\n",
      "Epoch 53/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5024 - acc: 0.9301Epoch 00053: val_acc did not improve\n",
      "45000/45000 [==============================] - 1489s 33ms/step - loss: 0.5024 - acc: 0.9301 - val_loss: 1.4121 - val_acc: 0.7418\n",
      "Epoch 54/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5047 - acc: 0.9284Epoch 00054: val_acc did not improve\n",
      "45000/45000 [==============================] - 1490s 33ms/step - loss: 0.5047 - acc: 0.9284 - val_loss: 1.4925 - val_acc: 0.7418\n",
      "Epoch 55/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5020 - acc: 0.9300Epoch 00055: val_acc did not improve\n",
      "45000/45000 [==============================] - 1496s 33ms/step - loss: 0.5020 - acc: 0.9300 - val_loss: 0.9662 - val_acc: 0.8028\n",
      "Epoch 56/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.9307Epoch 00056: val_acc did not improve\n",
      "45000/45000 [==============================] - 1502s 33ms/step - loss: 0.5019 - acc: 0.9307 - val_loss: 1.1084 - val_acc: 0.7786\n",
      "Epoch 57/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.9311Epoch 00057: val_acc did not improve\n",
      "45000/45000 [==============================] - 1505s 33ms/step - loss: 0.4958 - acc: 0.9311 - val_loss: 1.2395 - val_acc: 0.7578\n",
      "Epoch 58/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.5034 - acc: 0.9288Epoch 00058: val_acc did not improve\n",
      "45000/45000 [==============================] - 1502s 33ms/step - loss: 0.5034 - acc: 0.9288 - val_loss: 1.0472 - val_acc: 0.7850\n",
      "Epoch 59/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.9322Epoch 00059: val_acc did not improve\n",
      "45000/45000 [==============================] - 1583s 35ms/step - loss: 0.4927 - acc: 0.9322 - val_loss: 1.0392 - val_acc: 0.7838\n",
      "Epoch 60/200\n",
      "  256/45000 [..............................] - ETA: 24:29 - loss: 0.5610 - acc: 0.8945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-437a88dd224a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "          batch_size=32, epochs=200,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_valid, y_valid), \n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 185s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(x_test, y_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result after stopping at 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.10 Accuracy: 78.19%\n"
     ]
    }
   ],
   "source": [
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result of 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 142s 14ms/step\n",
      "Loss: 0.98 Accuracy: 80.06%\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model('weights.best.hdf5')\n",
    "res = model2.evaluate(x_test, y_test, batch_size=512)\n",
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-45a254586eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
