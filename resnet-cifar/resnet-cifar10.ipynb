{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Add\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.layers import GlobalAveragePooling2D\n",
    "# from keras.optimizers import Adam, SGD\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "from keras import backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "xx_train = xx_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "mean = np.mean(xx_train, axis=0)\n",
    "xx_train -= mean\n",
    "x_test -= mean\n",
    "\n",
    "x_train = xx_train[:45000]\n",
    "y_train = yy_train[:45000]\n",
    "x_valid = xx_train[45000:50000]\n",
    "y_valid = yy_train[45000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    shape1 = backend.int_shape(a)\n",
    "    shape2 = backend.int_shape(b)\n",
    "    w = int(round(shape1[1]/shape2[1]))\n",
    "    h = int(round(shape1[2]/shape2[2]))\n",
    "    eq = shape1[3] == shape2[3]\n",
    "    \n",
    "    tmp = a\n",
    "    print w,h,eq\n",
    "    print shape1, shape2\n",
    "    if w>1 or h>1 or not eq:\n",
    "        tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(a)\n",
    "    print backend.int_shape(tmp)\n",
    "    print\n",
    "    return Add()([tmp, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def layer(num_filt, size, strides, inp):\n",
    "    tmp = BatchNormalization(axis=3)(inp)\n",
    "    tmp = Activation('relu')(tmp)\n",
    "    tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(tmp)\n",
    "    return tmp\n",
    "\n",
    "l = Input(x_train.shape[1:])\n",
    "\n",
    "x = Conv2D(128, (7,7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#first layer\n",
    "num = 32\n",
    "xtmp = Conv2D(num, (1,1), strides=1, \n",
    "              padding='same', \n",
    "              kernel_initializer='he_normal', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "\n",
    "x = add(x,xtmp)\n",
    "\n",
    "#other layer\n",
    "xtmp = layer(num, (1,1), 1, x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "x = add(x,xtmp)\n",
    "\n",
    "for i in xrange(9):\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    #other layer\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "xshape = backend.int_shape(x)\n",
    "x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), monitor='val_acc', cooldown=0, patience=5, min_lr=0.5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 16, 16, 128)  18944       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 16, 16, 128)  512         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 16, 16, 128)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 16, 16, 32)   4128        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 16, 16, 32)   128         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 16, 16, 32)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 16, 16, 32)   9248        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 16, 16, 32)   128         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 16, 16, 32)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 16, 16, 128)  4224        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 16, 16, 128)  0           activation_123[0][0]             \n",
      "                                                                 conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 16, 16, 128)  512         add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 16, 16, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 16, 16, 32)   4128        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 16, 16, 32)   128         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 16, 16, 32)   0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 16, 16, 32)   9248        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 16, 16, 32)   128         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 16, 16, 32)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 16, 16, 128)  4224        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 16, 16, 128)  0           add_41[0][0]                     \n",
      "                                                                 conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 16, 16, 128)  512         add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 16, 16, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 16, 16, 32)   4128        activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 16, 16, 32)   128         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 16, 16, 32)   0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 16, 16, 32)   9248        activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 16, 16, 32)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 16, 16, 128)  4224        activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 16, 16, 128)  0           add_42[0][0]                     \n",
      "                                                                 conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 16, 16, 128)  512         add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 16, 16, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 16, 16, 32)   4128        activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 16, 16, 32)   128         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 16, 16, 32)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 16, 16, 32)   9248        activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 16, 16, 32)   128         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 16, 16, 32)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 16, 128)  4224        activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 16, 16, 128)  0           add_43[0][0]                     \n",
      "                                                                 conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 128)  512         add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 16, 16, 128)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 32)   4128        activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 32)   128         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 16, 16, 32)   0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 32)   9248        activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 16, 16, 32)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 16, 128)  4224        activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 16, 16, 128)  0           add_44[0][0]                     \n",
      "                                                                 conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 128)  512         add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 16, 16, 128)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 16, 32)   4128        activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 32)   128         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 16, 16, 32)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 16, 32)   9248        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 32)   128         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 16, 16, 32)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 16, 128)  4224        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 128)  0           add_45[0][0]                     \n",
      "                                                                 conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 128)  512         add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 128)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 32)   4128        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 32)   128         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 32)   0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 32)   9248        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 32)   128         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 32)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 128)  4224        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 128)  0           add_46[0][0]                     \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 128)  512         add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 128)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 32)   4128        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 16, 16, 32)   128         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 32)   0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 32)   9248        activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 16, 16, 32)   128         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 32)   0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 128)  4224        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 128)  0           add_47[0][0]                     \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 128)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 32)   4128        activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 16, 16, 32)   128         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 32)   0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 32)   9248        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 16, 16, 32)   128         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 32)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 128)  4224        activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
      "                                                                 conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 16, 16, 128)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 32)   4128        activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 16, 16, 32)   128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 16, 16, 32)   0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 32)   9248        activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 16, 16, 32)   128         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 16, 16, 32)   0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 128)  4224        activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 16, 16, 128)  0           add_49[0][0]                     \n",
      "                                                                 conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 16, 16, 128)  512         add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 16, 16, 128)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 32)   4128        activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 16, 16, 32)   128         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 16, 16, 32)   0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 16, 16, 32)   9248        activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 16, 16, 32)   128         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 16, 16, 32)   0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 16, 16, 128)  4224        activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 16, 16, 128)  0           add_50[0][0]                     \n",
      "                                                                 conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 16, 16, 128)  512         add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 16, 16, 128)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 16, 16, 32)   4128        activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 16, 16, 32)   128         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 16, 16, 32)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 16, 16, 32)   9248        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 16, 16, 32)   128         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 16, 16, 32)   0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 16, 16, 128)  4224        activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 16, 16, 128)  0           add_51[0][0]                     \n",
      "                                                                 conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 16, 16, 128)  512         add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 16, 16, 128)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 16, 16, 32)   4128        activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 16, 16, 32)   128         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 16, 16, 32)   0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 32)   9248        activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 16, 16, 32)   128         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 16, 16, 32)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 128)  4224        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 16, 16, 128)  0           add_52[0][0]                     \n",
      "                                                                 conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 16, 16, 128)  512         add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 16, 16, 128)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 32)   4128        activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 16, 16, 32)   128         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 16, 16, 32)   0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 16, 16, 32)   9248        activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 16, 16, 32)   128         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 16, 16, 32)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 16, 16, 128)  4224        activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 16, 16, 128)  0           add_53[0][0]                     \n",
      "                                                                 conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 128)  512         add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 16, 16, 128)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 16, 16, 32)   4128        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 32)   128         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 16, 16, 32)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 16, 16, 32)   9248        activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 32)   128         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 16, 16, 32)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 16, 16, 128)  4224        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 16, 16, 128)  0           add_54[0][0]                     \n",
      "                                                                 conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 16, 16, 128)  512         add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 16, 16, 128)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 16, 16, 32)   4128        activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 16, 16, 32)   128         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 16, 16, 32)   0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 16, 16, 32)   9248        activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 16, 16, 32)   128         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 16, 16, 32)   0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 16, 16, 128)  4224        activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 128)  0           add_55[0][0]                     \n",
      "                                                                 conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 16, 16, 128)  512         add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 16, 16, 128)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 16, 16, 32)   4128        activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 16, 16, 32)   128         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 16, 16, 32)   0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 16, 16, 32)   9248        activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 16, 16, 32)   128         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 16, 16, 32)   0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 16, 16, 128)  4224        activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 16, 16, 128)  0           add_56[0][0]                     \n",
      "                                                                 conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 128)  512         add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 16, 16, 128)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 16, 16, 32)   4128        activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 16, 16, 32)   128         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 16, 16, 32)   0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 16, 16, 32)   9248        activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 16, 16, 32)   128         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 16, 16, 32)   0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 16, 16, 128)  4224        activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 16, 16, 128)  0           add_57[0][0]                     \n",
      "                                                                 conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 16, 16, 128)  512         add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 16, 16, 128)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 16, 16, 32)   4128        activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 16, 16, 32)   128         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 16, 16, 32)   0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 32)   9248        activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 16, 16, 32)   128         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 16, 16, 32)   0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 128)  4224        activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 16, 16, 128)  0           add_58[0][0]                     \n",
      "                                                                 conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 16, 16, 128)  512         add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 16, 16, 128)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 32)   4128        activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 16, 16, 32)   128         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 16, 16, 32)   0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 16, 16, 32)   9248        activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 16, 16, 32)   128         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 16, 16, 32)   0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 16, 16, 128)  4224        activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 16, 16, 128)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 16, 16, 128)  512         add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 16, 16, 128)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 128)    0           activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 388,106\n",
      "Trainable params: 380,170\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if training was cut short\n",
    "model = load_model('weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 1.9321 - acc: 0.4634Epoch 00001: val_acc improved from -inf to 0.48460, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1543s 34ms/step - loss: 1.9320 - acc: 0.4634 - val_loss: 1.7904 - val_acc: 0.4846\n",
      "Epoch 2/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 1.4499 - acc: 0.5983Epoch 00002: val_acc improved from 0.48460 to 0.63820, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1526s 34ms/step - loss: 1.4498 - acc: 0.5984 - val_loss: 1.3116 - val_acc: 0.6382\n",
      "Epoch 3/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 1.2288 - acc: 0.6684Epoch 00003: val_acc improved from 0.63820 to 0.66960, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1513s 34ms/step - loss: 1.2289 - acc: 0.6683 - val_loss: 1.2266 - val_acc: 0.6696\n",
      "Epoch 4/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 1.0915 - acc: 0.7107Epoch 00004: val_acc did not improve\n",
      "45000/45000 [==============================] - 1518s 34ms/step - loss: 1.0916 - acc: 0.7107 - val_loss: 1.2761 - val_acc: 0.6426\n",
      "Epoch 5/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.9987 - acc: 0.7393Epoch 00005: val_acc improved from 0.66960 to 0.71300, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1525s 34ms/step - loss: 0.9988 - acc: 0.7392 - val_loss: 1.0750 - val_acc: 0.7130\n",
      "Epoch 6/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.9277 - acc: 0.7627Epoch 00006: val_acc did not improve\n",
      "45000/45000 [==============================] - 1524s 34ms/step - loss: 0.9278 - acc: 0.7627 - val_loss: 1.0963 - val_acc: 0.7004\n",
      "Epoch 7/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.8803 - acc: 0.7800Epoch 00007: val_acc improved from 0.71300 to 0.73060, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1536s 34ms/step - loss: 0.8804 - acc: 0.7800 - val_loss: 1.0133 - val_acc: 0.7306\n",
      "Epoch 8/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.8342 - acc: 0.7956Epoch 00008: val_acc did not improve\n",
      "45000/45000 [==============================] - 1523s 34ms/step - loss: 0.8342 - acc: 0.7957 - val_loss: 1.1443 - val_acc: 0.7030\n",
      "Epoch 9/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.8007 - acc: 0.8068Epoch 00009: val_acc did not improve\n",
      "45000/45000 [==============================] - 1521s 34ms/step - loss: 0.8007 - acc: 0.8069 - val_loss: 1.2322 - val_acc: 0.6794\n",
      "Epoch 10/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7711 - acc: 0.8184Epoch 00010: val_acc improved from 0.73060 to 0.73460, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1516s 34ms/step - loss: 0.7711 - acc: 0.8184 - val_loss: 1.0249 - val_acc: 0.7346\n",
      "Epoch 11/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7445 - acc: 0.8295Epoch 00011: val_acc improved from 0.73460 to 0.74460, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1519s 34ms/step - loss: 0.7445 - acc: 0.8295 - val_loss: 1.0155 - val_acc: 0.7446\n",
      "Epoch 12/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.7200 - acc: 0.8375Epoch 00012: val_acc improved from 0.74460 to 0.77880, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1495s 33ms/step - loss: 0.7201 - acc: 0.8375 - val_loss: 0.9191 - val_acc: 0.7788\n",
      "Epoch 13/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.8446Epoch 00013: val_acc improved from 0.77880 to 0.78000, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1502s 33ms/step - loss: 0.6990 - acc: 0.8446 - val_loss: 0.9048 - val_acc: 0.7800\n",
      "Epoch 14/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6832 - acc: 0.8517Epoch 00014: val_acc did not improve\n",
      "45000/45000 [==============================] - 1501s 33ms/step - loss: 0.6832 - acc: 0.8517 - val_loss: 0.9995 - val_acc: 0.7680\n",
      "Epoch 15/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6659 - acc: 0.8596Epoch 00015: val_acc improved from 0.78000 to 0.79180, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1499s 33ms/step - loss: 0.6659 - acc: 0.8596 - val_loss: 0.8712 - val_acc: 0.7918\n",
      "Epoch 16/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6551 - acc: 0.8613Epoch 00016: val_acc did not improve\n",
      "45000/45000 [==============================] - 1501s 33ms/step - loss: 0.6551 - acc: 0.8613 - val_loss: 0.9000 - val_acc: 0.7914\n",
      "Epoch 17/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6439 - acc: 0.8681Epoch 00017: val_acc did not improve\n",
      "45000/45000 [==============================] - 1501s 33ms/step - loss: 0.6439 - acc: 0.8681 - val_loss: 0.9739 - val_acc: 0.7810\n",
      "Epoch 18/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.8715Epoch 00018: val_acc did not improve\n",
      "45000/45000 [==============================] - 1503s 33ms/step - loss: 0.6292 - acc: 0.8715 - val_loss: 1.1463 - val_acc: 0.7282\n",
      "Epoch 19/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.8764Epoch 00019: val_acc did not improve\n",
      "45000/45000 [==============================] - 1503s 33ms/step - loss: 0.6205 - acc: 0.8764 - val_loss: 1.0370 - val_acc: 0.7580\n",
      "Epoch 20/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6090 - acc: 0.8819Epoch 00020: val_acc did not improve\n",
      "45000/45000 [==============================] - 1493s 33ms/step - loss: 0.6091 - acc: 0.8818 - val_loss: 1.0196 - val_acc: 0.7732\n",
      "Epoch 21/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.8831Epoch 00021: val_acc did not improve\n",
      "45000/45000 [==============================] - 1509s 34ms/step - loss: 0.6024 - acc: 0.8831 - val_loss: 1.0010 - val_acc: 0.7782\n",
      "Epoch 22/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.9439Epoch 00022: val_acc improved from 0.79180 to 0.84320, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1565s 35ms/step - loss: 0.4410 - acc: 0.9438 - val_loss: 0.7667 - val_acc: 0.8432\n",
      "Epoch 23/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.9600Epoch 00023: val_acc did not improve\n",
      "45000/45000 [==============================] - 2218s 49ms/step - loss: 0.3859 - acc: 0.9600 - val_loss: 0.8281 - val_acc: 0.8306\n",
      "Epoch 24/200\n",
      "44992/45000 [============================>.] - ETA: 3s - loss: 0.3594 - acc: 0.9659 Epoch 00024: val_acc did not improve\n",
      "45000/45000 [==============================] - 19324s 429ms/step - loss: 0.3595 - acc: 0.9658 - val_loss: 0.8761 - val_acc: 0.8226\n",
      "Epoch 25/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.3363 - acc: 0.9723Epoch 00025: val_acc did not improve\n",
      "45000/45000 [==============================] - 1863s 41ms/step - loss: 0.3364 - acc: 0.9723 - val_loss: 0.8858 - val_acc: 0.8236\n",
      "Epoch 26/200\n",
      "44992/45000 [============================>.] - ETA: 4s - loss: 0.3247 - acc: 0.9730 Epoch 00026: val_acc did not improve\n",
      "45000/45000 [==============================] - 26504s 589ms/step - loss: 0.3247 - acc: 0.9730 - val_loss: 0.8683 - val_acc: 0.8322\n",
      "Epoch 27/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.3114 - acc: 0.9757Epoch 00027: val_acc did not improve\n",
      "45000/45000 [==============================] - 2050s 46ms/step - loss: 0.3114 - acc: 0.9757 - val_loss: 1.0190 - val_acc: 0.8072\n",
      "Epoch 28/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.3043 - acc: 0.9771Epoch 00028: val_acc did not improve\n",
      "45000/45000 [==============================] - 2251s 50ms/step - loss: 0.3045 - acc: 0.9770 - val_loss: 0.9697 - val_acc: 0.8154\n",
      "Epoch 29/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2644 - acc: 0.9914Epoch 00029: val_acc did not improve\n",
      "45000/45000 [==============================] - 2044s 45ms/step - loss: 0.2645 - acc: 0.9914 - val_loss: 0.8451 - val_acc: 0.8420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9950Epoch 00030: val_acc did not improve\n",
      "45000/45000 [==============================] - 1511s 34ms/step - loss: 0.2502 - acc: 0.9950 - val_loss: 0.8612 - val_acc: 0.8382\n",
      "Epoch 31/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9954Epoch 00031: val_acc did not improve\n",
      "45000/45000 [==============================] - 1509s 34ms/step - loss: 0.2444 - acc: 0.9954 - val_loss: 0.8934 - val_acc: 0.8332\n",
      "Epoch 32/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9964Epoch 00032: val_acc did not improve\n",
      "45000/45000 [==============================] - 1510s 34ms/step - loss: 0.2371 - acc: 0.9964 - val_loss: 0.8799 - val_acc: 0.8358\n",
      "Epoch 33/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9970Epoch 00033: val_acc did not improve\n",
      "45000/45000 [==============================] - 1507s 33ms/step - loss: 0.2322 - acc: 0.9970 - val_loss: 0.8703 - val_acc: 0.8410\n",
      "Epoch 34/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9979Epoch 00034: val_acc improved from 0.84320 to 0.84440, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1497s 33ms/step - loss: 0.2250 - acc: 0.9979 - val_loss: 0.8491 - val_acc: 0.8444\n",
      "Epoch 35/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9988Epoch 00035: val_acc improved from 0.84440 to 0.84480, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1496s 33ms/step - loss: 0.2208 - acc: 0.9987 - val_loss: 0.8655 - val_acc: 0.8448\n",
      "Epoch 36/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9987Epoch 00036: val_acc improved from 0.84480 to 0.84520, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1493s 33ms/step - loss: 0.2189 - acc: 0.9987 - val_loss: 0.8558 - val_acc: 0.8452\n",
      "Epoch 37/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9989Epoch 00037: val_acc improved from 0.84520 to 0.84580, saving model to weights.best.hdf5\n",
      "45000/45000 [==============================] - 1489s 33ms/step - loss: 0.2166 - acc: 0.9989 - val_loss: 0.8513 - val_acc: 0.8458\n",
      "Epoch 38/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2138 - acc: 0.9991Epoch 00038: val_acc did not improve\n",
      "45000/45000 [==============================] - 1490s 33ms/step - loss: 0.2138 - acc: 0.9991 - val_loss: 0.8727 - val_acc: 0.8394\n",
      "Epoch 39/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9990Epoch 00039: val_acc did not improve\n",
      "45000/45000 [==============================] - 1490s 33ms/step - loss: 0.2120 - acc: 0.9990 - val_loss: 0.8686 - val_acc: 0.8436\n",
      "Epoch 40/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2102 - acc: 0.9989Epoch 00040: val_acc did not improve\n",
      "45000/45000 [==============================] - 1493s 33ms/step - loss: 0.2102 - acc: 0.9989 - val_loss: 0.8709 - val_acc: 0.8456\n",
      "Epoch 41/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9995Epoch 00041: val_acc did not improve\n",
      "45000/45000 [==============================] - 1484s 33ms/step - loss: 0.2072 - acc: 0.9995 - val_loss: 0.8717 - val_acc: 0.8416\n",
      "Epoch 42/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2049 - acc: 0.9993Epoch 00042: val_acc did not improve\n",
      "45000/45000 [==============================] - 1497s 33ms/step - loss: 0.2049 - acc: 0.9993 - val_loss: 0.8704 - val_acc: 0.8444\n",
      "Epoch 43/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9993Epoch 00043: val_acc did not improve\n",
      "45000/45000 [==============================] - 1489s 33ms/step - loss: 0.2028 - acc: 0.9993 - val_loss: 0.8743 - val_acc: 0.8430\n",
      "Epoch 44/200\n",
      "44992/45000 [============================>.] - ETA: 4s - loss: 0.2007 - acc: 0.9994 Epoch 00044: val_acc did not improve\n",
      "45000/45000 [==============================] - 22854s 508ms/step - loss: 0.2007 - acc: 0.9994 - val_loss: 0.8821 - val_acc: 0.8454\n",
      "Epoch 45/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9993Epoch 00045: val_acc did not improve\n",
      "45000/45000 [==============================] - 2122s 47ms/step - loss: 0.2001 - acc: 0.9993 - val_loss: 0.8796 - val_acc: 0.8430\n",
      "Epoch 46/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9995Epoch 00046: val_acc did not improve\n",
      "45000/45000 [==============================] - 2173s 48ms/step - loss: 0.1991 - acc: 0.9995 - val_loss: 0.8799 - val_acc: 0.8434\n",
      "Epoch 47/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9994Epoch 00047: val_acc did not improve\n",
      "45000/45000 [==============================] - 1946s 43ms/step - loss: 0.1981 - acc: 0.9994 - val_loss: 0.8797 - val_acc: 0.8430\n",
      "Epoch 48/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9993Epoch 00048: val_acc did not improve\n",
      "45000/45000 [==============================] - 1702s 38ms/step - loss: 0.1974 - acc: 0.9993 - val_loss: 0.8747 - val_acc: 0.8442\n",
      "Epoch 49/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9994Epoch 00049: val_acc did not improve\n",
      "45000/45000 [==============================] - 3026s 67ms/step - loss: 0.1972 - acc: 0.9994 - val_loss: 0.8741 - val_acc: 0.8448\n",
      "Epoch 50/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9996Epoch 00050: val_acc did not improve\n",
      "45000/45000 [==============================] - 3008s 67ms/step - loss: 0.1961 - acc: 0.9996 - val_loss: 0.8769 - val_acc: 0.8442\n",
      "Epoch 51/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9995Epoch 00051: val_acc did not improve\n",
      "45000/45000 [==============================] - 3118s 69ms/step - loss: 0.1962 - acc: 0.9995 - val_loss: 0.8772 - val_acc: 0.8434\n",
      "Epoch 52/200\n",
      "44992/45000 [============================>.] - ETA: 0s - loss: 0.1959 - acc: 0.9995Epoch 00052: val_acc did not improve\n",
      "45000/45000 [==============================] - 3118s 69ms/step - loss: 0.1959 - acc: 0.9995 - val_loss: 0.8752 - val_acc: 0.8446\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,\n",
    "          batch_size=32, epochs=200,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_valid, y_valid), \n",
    "          callbacks=[checkpoint, early, lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 172s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(x_test, y_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.89 Accuracy: 83.97%\n"
     ]
    }
   ],
   "source": [
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0ZtzybevSf0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d09bfa990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c33f00610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
