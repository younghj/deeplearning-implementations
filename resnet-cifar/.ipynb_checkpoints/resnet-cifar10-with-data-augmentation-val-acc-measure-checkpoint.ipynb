{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Add\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "xx_train = xx_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = xx_train[:45000]\n",
    "y_train = yy_train[:45000]\n",
    "x_valid = xx_train[45000:50000]\n",
    "y_valid = yy_train[45000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traingen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest')\n",
    "traingen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    shape1 = backend.int_shape(a)\n",
    "    shape2 = backend.int_shape(b)\n",
    "    w = int(round(shape1[1]/shape2[1]))\n",
    "    h = int(round(shape1[2]/shape2[2]))\n",
    "    eq = shape1[3] == shape2[3]\n",
    "    \n",
    "    tmp = a\n",
    "    print w,h,eq\n",
    "    print shape1, shape2\n",
    "    if w>1 or h>1 or not eq:\n",
    "        tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(a)\n",
    "    print backend.int_shape(tmp)\n",
    "    print\n",
    "    return Add()([tmp, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def layer(num_filt, size, strides, inp):\n",
    "    tmp = BatchNormalization(axis=3)(inp)\n",
    "    tmp = Activation('relu')(tmp)\n",
    "    tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(tmp)\n",
    "    return tmp\n",
    "\n",
    "l = Input(x_train.shape[1:])\n",
    "\n",
    "x = Conv2D(128, (7,7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#first layer\n",
    "num = 32\n",
    "xtmp = Conv2D(num, (1,1), strides=1, \n",
    "              padding='same', \n",
    "              kernel_initializer='he_normal', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "\n",
    "x = add(x,xtmp)\n",
    "\n",
    "#other layer\n",
    "xtmp = layer(num, (1,1), 1, x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "x = add(x,xtmp)\n",
    "\n",
    "for i in xrange(9):\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    #other layer\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "xshape = backend.int_shape(x)\n",
    "x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights-with-augmentation-val-acc-lr.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), monitor='val_acc', cooldown=0, patience=5, min_lr=0.5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 128)  18944       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 128)  512         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 128)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 32)   4128        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 32)   128         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 32)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 32)   9248        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 32)   128         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 32)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 128)  4224        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 16, 16, 128)  0           activation_62[0][0]              \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 128)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 32)   4128        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 32)   128         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 32)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 32)   9248        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 32)   128         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 32)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 128)  4224        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 16, 16, 128)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 32)   4128        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 16, 16, 32)   128         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 16, 16, 32)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 32)   9248        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 16, 16, 32)   128         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 16, 16, 32)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 16, 16, 128)  4224        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 16, 16, 32)   4128        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 16, 32)   128         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 16, 16, 32)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 16, 16, 32)   9248        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 16, 32)   128         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 16, 16, 32)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 128)  4224        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 16, 16, 128)  0           add_23[0][0]                     \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 32)   4128        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 16, 32)   128         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 16, 32)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 32)   9248        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 16, 32)   128         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 16, 32)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 128)  4224        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 16, 128)  0           add_24[0][0]                     \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 16, 16, 128)  512         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 32)   4128        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 16, 32)   128         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 16, 32)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 16, 16, 32)   9248        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 16, 16, 32)   128         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 16, 32)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 128)  4224        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 16, 128)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 128)  512         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 32)   4128        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 32)   128         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 32)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 32)   9248        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 32)   128         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 16, 32)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 128)  4224        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 16, 128)  0           add_26[0][0]                     \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 128)  512         add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 16, 16, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 32)   4128        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 32)   128         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 16, 16, 32)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 32)   9248        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 32)   128         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 16, 16, 32)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 128)  4224        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 16, 16, 128)  0           add_27[0][0]                     \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 128)  512         add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 16, 16, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 32)   4128        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 32)   128         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 16, 16, 32)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 32)   9248        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 32)   128         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 16, 16, 32)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 128)  4224        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 16, 128)  0           add_28[0][0]                     \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 16, 16, 128)  512         add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 16, 16, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 32)   4128        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 16, 16, 32)   128         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 16, 16, 32)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 32)   9248        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 16, 16, 32)   128         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 16, 16, 32)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 128)  4224        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 16, 16, 128)  0           add_29[0][0]                     \n",
      "                                                                 conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 16, 16, 128)  512         add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 16, 16, 128)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 32)   4128        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 16, 16, 32)   128         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 16, 16, 32)   0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 16, 16, 32)   9248        activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 16, 16, 32)   128         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 16, 16, 32)   0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 16, 16, 128)  4224        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 16, 16, 128)  0           add_30[0][0]                     \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 16, 16, 128)  512         add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 16, 16, 128)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 16, 16, 32)   4128        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 16, 16, 32)   128         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 16, 16, 32)   0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 32)   9248        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 16, 16, 32)   128         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 32)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 128)  4224        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 16, 16, 128)  0           add_31[0][0]                     \n",
      "                                                                 conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 16, 16, 128)  512         add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 128)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 16, 16, 32)   4128        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 16, 16, 32)   128         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 32)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 16, 16, 32)   9248        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 16, 16, 32)   128         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 32)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 128)  4224        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 16, 16, 128)  0           add_32[0][0]                     \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 16, 16, 128)  512         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 128)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 16, 16, 32)   4128        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 16, 16, 32)   128         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 32)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 32)   9248        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 32)   128         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 32)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 128)  4224        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 16, 16, 128)  0           add_33[0][0]                     \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 128)  512         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 128)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 16, 16, 32)   4128        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 32)   128         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 16, 32)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 16, 16, 32)   9248        activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 16, 32)   128         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 16, 32)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 16, 16, 128)  4224        activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 16, 16, 128)  0           add_34[0][0]                     \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 16, 16, 128)  512         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 16, 16, 128)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 16, 16, 32)   4128        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 16, 16, 32)   128         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 16, 16, 32)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 16, 16, 32)   9248        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 16, 16, 32)   128         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 16, 16, 32)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 16, 16, 128)  4224        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 16, 16, 128)  0           add_35[0][0]                     \n",
      "                                                                 conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 16, 16, 128)  512         add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 16, 16, 128)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 16, 16, 32)   4128        activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 16, 32)   128         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 16, 16, 32)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 16, 16, 32)   9248        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 16, 16, 32)   128         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 16, 16, 32)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 16, 16, 128)  4224        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 128)  0           add_36[0][0]                     \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 16, 16, 128)  512         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 16, 16, 128)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 16, 16, 32)   4128        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 16, 16, 32)   128         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 16, 16, 32)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 16, 32)   9248        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 16, 16, 32)   128         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 16, 16, 32)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 128)  4224        activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 16, 16, 128)  0           add_37[0][0]                     \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 16, 16, 128)  512         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 16, 16, 128)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 32)   4128        activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 16, 16, 32)   128         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 16, 16, 32)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 16, 32)   9248        activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 16, 16, 32)   128         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 16, 16, 32)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 16, 16, 128)  4224        activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 16, 16, 128)  0           add_38[0][0]                     \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 16, 16, 128)  512         add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 16, 16, 128)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 16, 16, 32)   4128        activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 16, 16, 32)   128         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 16, 16, 32)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 16, 16, 32)   9248        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 16, 16, 32)   128         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 16, 16, 32)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 16, 16, 128)  4224        activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 16, 16, 128)  0           add_39[0][0]                     \n",
      "                                                                 conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 16, 16, 128)  512         add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 16, 16, 128)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 128)    0           activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1290        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 388,106\n",
      "Trainable params: 380,170\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 2.1452 - acc: 0.3845Epoch 00001: val_acc improved from -inf to 0.41940, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 3190s 2s/step - loss: 2.1450 - acc: 0.3845 - val_loss: 1.9264 - val_acc: 0.4194\n",
      "Epoch 2/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 1.7435 - acc: 0.4831Epoch 00002: val_acc did not improve\n",
      "1406/1406 [==============================] - 3090s 2s/step - loss: 1.7434 - acc: 0.4831 - val_loss: 2.1180 - val_acc: 0.4090\n",
      "Epoch 3/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 1.5843 - acc: 0.5180Epoch 00003: val_acc improved from 0.41940 to 0.44200, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 3148s 2s/step - loss: 1.5843 - acc: 0.5180 - val_loss: 2.2441 - val_acc: 0.4420\n",
      "Epoch 4/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 1.4862 - acc: 0.5472Epoch 00004: val_acc improved from 0.44200 to 0.56600, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 3090s 2s/step - loss: 1.4863 - acc: 0.5472 - val_loss: 1.4555 - val_acc: 0.5660\n",
      "Epoch 5/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.4081 - acc: 0.5722Epoch 00005: val_acc improved from 0.56600 to 0.57540, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1519s 1s/step - loss: 1.4082 - acc: 0.5722 - val_loss: 1.3711 - val_acc: 0.5754\n",
      "Epoch 6/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.3451 - acc: 0.5972Epoch 00006: val_acc did not improve\n",
      "1406/1406 [==============================] - 1513s 1s/step - loss: 1.3451 - acc: 0.5972 - val_loss: 1.7228 - val_acc: 0.5218\n",
      "Epoch 7/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2986 - acc: 0.6120Epoch 00007: val_acc improved from 0.57540 to 0.58500, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1524s 1s/step - loss: 1.2988 - acc: 0.6120 - val_loss: 1.4445 - val_acc: 0.5850\n",
      "Epoch 8/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2634 - acc: 0.6239Epoch 00008: val_acc improved from 0.58500 to 0.67240, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1525s 1s/step - loss: 1.2634 - acc: 0.6239 - val_loss: 1.1439 - val_acc: 0.6724\n",
      "Epoch 9/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2266 - acc: 0.6392Epoch 00009: val_acc did not improve\n",
      "1406/1406 [==============================] - 1513s 1s/step - loss: 1.2266 - acc: 0.6392 - val_loss: 1.6791 - val_acc: 0.5682\n",
      "Epoch 10/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1999 - acc: 0.6488Epoch 00010: val_acc did not improve\n",
      "1406/1406 [==============================] - 1527s 1s/step - loss: 1.1999 - acc: 0.6488 - val_loss: 1.3858 - val_acc: 0.6154\n",
      "Epoch 11/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1774 - acc: 0.6555Epoch 00011: val_acc improved from 0.67240 to 0.69640, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1518s 1s/step - loss: 1.1774 - acc: 0.6554 - val_loss: 1.0886 - val_acc: 0.6964\n",
      "Epoch 12/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1499 - acc: 0.6654Epoch 00012: val_acc improved from 0.69640 to 0.71780, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1523s 1s/step - loss: 1.1501 - acc: 0.6653 - val_loss: 1.0337 - val_acc: 0.7178\n",
      "Epoch 13/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1301 - acc: 0.6752Epoch 00013: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 1.1301 - acc: 0.6752 - val_loss: 1.1469 - val_acc: 0.6884\n",
      "Epoch 14/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1107 - acc: 0.6835Epoch 00014: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 1.1106 - acc: 0.6835 - val_loss: 1.0477 - val_acc: 0.7066\n",
      "Epoch 15/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1040 - acc: 0.6863Epoch 00015: val_acc did not improve\n",
      "1406/1406 [==============================] - 1520s 1s/step - loss: 1.1044 - acc: 0.6863 - val_loss: 1.3857 - val_acc: 0.6210\n",
      "Epoch 16/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0888 - acc: 0.6908Epoch 00016: val_acc improved from 0.71780 to 0.74700, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1509s 1s/step - loss: 1.0887 - acc: 0.6908 - val_loss: 0.9517 - val_acc: 0.7470\n",
      "Epoch 17/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0762 - acc: 0.6946Epoch 00017: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 1.0762 - acc: 0.6946 - val_loss: 1.1329 - val_acc: 0.6914\n",
      "Epoch 18/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0593 - acc: 0.7024Epoch 00018: val_acc did not improve\n",
      "1406/1406 [==============================] - 1500s 1s/step - loss: 1.0593 - acc: 0.7025 - val_loss: 0.9889 - val_acc: 0.7344\n",
      "Epoch 19/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0534 - acc: 0.7048Epoch 00019: val_acc improved from 0.74700 to 0.75600, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1471s 1s/step - loss: 1.0536 - acc: 0.7047 - val_loss: 0.9044 - val_acc: 0.7560\n",
      "Epoch 20/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0421 - acc: 0.7077Epoch 00020: val_acc did not improve\n",
      "1406/1406 [==============================] - 1487s 1s/step - loss: 1.0420 - acc: 0.7078 - val_loss: 0.9510 - val_acc: 0.7324\n",
      "Epoch 21/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0262 - acc: 0.7162Epoch 00021: val_acc improved from 0.75600 to 0.76800, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1482s 1s/step - loss: 1.0263 - acc: 0.7161 - val_loss: 0.8999 - val_acc: 0.7680\n",
      "Epoch 22/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0252 - acc: 0.7154Epoch 00022: val_acc did not improve\n",
      "1406/1406 [==============================] - 1499s 1s/step - loss: 1.0253 - acc: 0.7153 - val_loss: 0.9522 - val_acc: 0.7468\n",
      "Epoch 23/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0094 - acc: 0.7216Epoch 00023: val_acc did not improve\n",
      "1406/1406 [==============================] - 1493s 1s/step - loss: 1.0093 - acc: 0.7216 - val_loss: 1.0024 - val_acc: 0.7274\n",
      "Epoch 24/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0098 - acc: 0.7211Epoch 00024: val_acc did not improve\n",
      "1406/1406 [==============================] - 1487s 1s/step - loss: 1.0099 - acc: 0.7210 - val_loss: 1.0499 - val_acc: 0.7052\n",
      "Epoch 25/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9999 - acc: 0.7236Epoch 00025: val_acc did not improve\n",
      "1406/1406 [==============================] - 1487s 1s/step - loss: 0.9999 - acc: 0.7236 - val_loss: 1.1276 - val_acc: 0.7122\n",
      "Epoch 26/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9888 - acc: 0.7284Epoch 00026: val_acc did not improve\n",
      "1406/1406 [==============================] - 1490s 1s/step - loss: 0.9888 - acc: 0.7284 - val_loss: 1.0025 - val_acc: 0.7506\n",
      "Epoch 27/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9851 - acc: 0.7308Epoch 00027: val_acc did not improve\n",
      "1406/1406 [==============================] - 1497s 1s/step - loss: 0.9851 - acc: 0.7308 - val_loss: 0.8873 - val_acc: 0.7606\n",
      "Epoch 28/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8697 - acc: 0.7682Epoch 00028: val_acc improved from 0.76800 to 0.83080, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1489s 1s/step - loss: 0.8698 - acc: 0.7682 - val_loss: 0.6975 - val_acc: 0.8308\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8432 - acc: 0.7789Epoch 00029: val_acc did not improve\n",
      "1406/1406 [==============================] - 1483s 1s/step - loss: 0.8433 - acc: 0.7788 - val_loss: 0.7150 - val_acc: 0.8216\n",
      "Epoch 30/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8268 - acc: 0.7813Epoch 00030: val_acc did not improve\n",
      "1406/1406 [==============================] - 1494s 1s/step - loss: 0.8266 - acc: 0.7814 - val_loss: 0.7270 - val_acc: 0.8220\n",
      "Epoch 31/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8153 - acc: 0.7848Epoch 00031: val_acc did not improve\n",
      "1406/1406 [==============================] - 1489s 1s/step - loss: 0.8154 - acc: 0.7848 - val_loss: 0.6934 - val_acc: 0.8276\n",
      "Epoch 32/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8033 - acc: 0.7873Epoch 00032: val_acc did not improve\n",
      "1406/1406 [==============================] - 1485s 1s/step - loss: 0.8032 - acc: 0.7874 - val_loss: 0.7081 - val_acc: 0.8184\n",
      "Epoch 33/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7902 - acc: 0.7905Epoch 00033: val_acc did not improve\n",
      "1406/1406 [==============================] - 1501s 1s/step - loss: 0.7901 - acc: 0.7905 - val_loss: 0.6837 - val_acc: 0.8296\n",
      "Epoch 34/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7806 - acc: 0.7920Epoch 00034: val_acc improved from 0.83080 to 0.83320, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1520s 1s/step - loss: 0.7807 - acc: 0.7920 - val_loss: 0.6690 - val_acc: 0.8332\n",
      "Epoch 35/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7724 - acc: 0.7958Epoch 00035: val_acc improved from 0.83320 to 0.83360, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1499s 1s/step - loss: 0.7725 - acc: 0.7958 - val_loss: 0.6678 - val_acc: 0.8336\n",
      "Epoch 36/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7717 - acc: 0.7935Epoch 00036: val_acc improved from 0.83360 to 0.83600, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1549s 1s/step - loss: 0.7716 - acc: 0.7935 - val_loss: 0.6597 - val_acc: 0.8360\n",
      "Epoch 37/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7599 - acc: 0.7985Epoch 00037: val_acc did not improve\n",
      "1406/1406 [==============================] - 2014s 1s/step - loss: 0.7602 - acc: 0.7984 - val_loss: 0.6759 - val_acc: 0.8298\n",
      "Epoch 38/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7475 - acc: 0.8012Epoch 00038: val_acc did not improve\n",
      "1406/1406 [==============================] - 2392s 2s/step - loss: 0.7476 - acc: 0.8012 - val_loss: 0.6571 - val_acc: 0.8356\n",
      "Epoch 39/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7501 - acc: 0.8015Epoch 00039: val_acc did not improve\n",
      "1406/1406 [==============================] - 2667s 2s/step - loss: 0.7501 - acc: 0.8015 - val_loss: 0.6959 - val_acc: 0.8224\n",
      "Epoch 40/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7451 - acc: 0.8037Epoch 00040: val_acc did not improve\n",
      "1406/1406 [==============================] - 1971s 1s/step - loss: 0.7449 - acc: 0.8038 - val_loss: 0.6719 - val_acc: 0.8298\n",
      "Epoch 41/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7399 - acc: 0.8019Epoch 00041: val_acc did not improve\n",
      "1406/1406 [==============================] - 1852s 1s/step - loss: 0.7402 - acc: 0.8018 - val_loss: 0.6442 - val_acc: 0.8348\n",
      "Epoch 42/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7355 - acc: 0.8046Epoch 00042: val_acc improved from 0.83600 to 0.85660, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1891s 1s/step - loss: 0.7357 - acc: 0.8045 - val_loss: 0.5818 - val_acc: 0.8566\n",
      "Epoch 43/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7288 - acc: 0.8080Epoch 00043: val_acc did not improve\n",
      "1406/1406 [==============================] - 1854s 1s/step - loss: 0.7287 - acc: 0.8081 - val_loss: 0.6189 - val_acc: 0.8446\n",
      "Epoch 44/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7272 - acc: 0.8067Epoch 00044: val_acc did not improve\n",
      "1406/1406 [==============================] - 1870s 1s/step - loss: 0.7274 - acc: 0.8066 - val_loss: 0.6122 - val_acc: 0.8426\n",
      "Epoch 45/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7261 - acc: 0.8057Epoch 00045: val_acc did not improve\n",
      "1406/1406 [==============================] - 1887s 1s/step - loss: 0.7262 - acc: 0.8057 - val_loss: 0.6224 - val_acc: 0.8454\n",
      "Epoch 46/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7172 - acc: 0.8097Epoch 00046: val_acc did not improve\n",
      "1406/1406 [==============================] - 1902s 1s/step - loss: 0.7172 - acc: 0.8098 - val_loss: 0.6441 - val_acc: 0.8382\n",
      "Epoch 47/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7137 - acc: 0.8105Epoch 00047: val_acc did not improve\n",
      "1406/1406 [==============================] - 2136s 2s/step - loss: 0.7137 - acc: 0.8105 - val_loss: 0.6211 - val_acc: 0.8502\n",
      "Epoch 48/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 0.7146 - acc: 0.8101Epoch 00048: val_acc did not improve\n",
      "1406/1406 [==============================] - 3130s 2s/step - loss: 0.7144 - acc: 0.8102 - val_loss: 0.6420 - val_acc: 0.8342\n",
      "Epoch 49/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6636 - acc: 0.8263Epoch 00049: val_acc improved from 0.85660 to 0.85940, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 2658s 2s/step - loss: 0.6636 - acc: 0.8263 - val_loss: 0.5582 - val_acc: 0.8594\n",
      "Epoch 50/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6479 - acc: 0.8307Epoch 00050: val_acc improved from 0.85940 to 0.86260, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 2225s 2s/step - loss: 0.6478 - acc: 0.8307 - val_loss: 0.5650 - val_acc: 0.8626\n",
      "Epoch 51/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6375 - acc: 0.8343Epoch 00051: val_acc improved from 0.86260 to 0.86280, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 2298s 2s/step - loss: 0.6375 - acc: 0.8343 - val_loss: 0.5521 - val_acc: 0.8628\n",
      "Epoch 52/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6384 - acc: 0.8344Epoch 00052: val_acc did not improve\n",
      "1406/1406 [==============================] - 1842s 1s/step - loss: 0.6386 - acc: 0.8343 - val_loss: 0.5664 - val_acc: 0.8576\n",
      "Epoch 53/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6311 - acc: 0.8370Epoch 00053: val_acc improved from 0.86280 to 0.86720, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1833s 1s/step - loss: 0.6313 - acc: 0.8369 - val_loss: 0.5476 - val_acc: 0.8672\n",
      "Epoch 54/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6294 - acc: 0.8380Epoch 00054: val_acc improved from 0.86720 to 0.87040, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1837s 1s/step - loss: 0.6294 - acc: 0.8380 - val_loss: 0.5336 - val_acc: 0.8704\n",
      "Epoch 55/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6258 - acc: 0.8379Epoch 00055: val_acc did not improve\n",
      "1406/1406 [==============================] - 1840s 1s/step - loss: 0.6258 - acc: 0.8379 - val_loss: 0.5378 - val_acc: 0.8640\n",
      "Epoch 56/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6190 - acc: 0.8401Epoch 00056: val_acc did not improve\n",
      "1406/1406 [==============================] - 1848s 1s/step - loss: 0.6190 - acc: 0.8401 - val_loss: 0.5517 - val_acc: 0.8612\n",
      "Epoch 57/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6203 - acc: 0.8385Epoch 00057: val_acc did not improve\n",
      "1406/1406 [==============================] - 1822s 1s/step - loss: 0.6207 - acc: 0.8385 - val_loss: 0.5488 - val_acc: 0.8644\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6151 - acc: 0.8424Epoch 00058: val_acc did not improve\n",
      "1406/1406 [==============================] - 1823s 1s/step - loss: 0.6152 - acc: 0.8423 - val_loss: 0.5506 - val_acc: 0.8628\n",
      "Epoch 59/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6118 - acc: 0.8422Epoch 00059: val_acc did not improve\n",
      "1406/1406 [==============================] - 1825s 1s/step - loss: 0.6118 - acc: 0.8422 - val_loss: 0.5302 - val_acc: 0.8684\n",
      "Epoch 60/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6147 - acc: 0.8392Epoch 00060: val_acc did not improve\n",
      "1406/1406 [==============================] - 1835s 1s/step - loss: 0.6148 - acc: 0.8392 - val_loss: 0.5521 - val_acc: 0.8616\n",
      "Epoch 61/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5951 - acc: 0.8468Epoch 00061: val_acc did not improve\n",
      "1406/1406 [==============================] - 1839s 1s/step - loss: 0.5950 - acc: 0.8468 - val_loss: 0.5188 - val_acc: 0.8688\n",
      "Epoch 62/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5893 - acc: 0.8500Epoch 00062: val_acc improved from 0.87040 to 0.87080, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1838s 1s/step - loss: 0.5892 - acc: 0.8500 - val_loss: 0.5218 - val_acc: 0.8708\n",
      "Epoch 63/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5887 - acc: 0.8486Epoch 00063: val_acc improved from 0.87080 to 0.87500, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1823s 1s/step - loss: 0.5887 - acc: 0.8486 - val_loss: 0.5109 - val_acc: 0.8750\n",
      "Epoch 64/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5845 - acc: 0.8515Epoch 00064: val_acc did not improve\n",
      "1406/1406 [==============================] - 1827s 1s/step - loss: 0.5846 - acc: 0.8516 - val_loss: 0.5177 - val_acc: 0.8732\n",
      "Epoch 65/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5771 - acc: 0.8530Epoch 00065: val_acc did not improve\n",
      "1406/1406 [==============================] - 2271s 2s/step - loss: 0.5772 - acc: 0.8530 - val_loss: 0.5148 - val_acc: 0.8734\n",
      "Epoch 66/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5806 - acc: 0.8508Epoch 00066: val_acc did not improve\n",
      "1406/1406 [==============================] - 2185s 2s/step - loss: 0.5804 - acc: 0.8508 - val_loss: 0.5179 - val_acc: 0.8726\n",
      "Epoch 67/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5776 - acc: 0.8519Epoch 00067: val_acc did not improve\n",
      "1406/1406 [==============================] - 1817s 1s/step - loss: 0.5775 - acc: 0.8519 - val_loss: 0.5242 - val_acc: 0.8728\n",
      "Epoch 68/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5788 - acc: 0.8513Epoch 00068: val_acc did not improve\n",
      "1406/1406 [==============================] - 1549s 1s/step - loss: 0.5789 - acc: 0.8513 - val_loss: 0.5143 - val_acc: 0.8746\n",
      "Epoch 69/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5752 - acc: 0.8545Epoch 00069: val_acc did not improve\n",
      "1406/1406 [==============================] - 1556s 1s/step - loss: 0.5752 - acc: 0.8545 - val_loss: 0.5133 - val_acc: 0.8706\n",
      "Epoch 70/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5680 - acc: 0.8562Epoch 00070: val_acc improved from 0.87500 to 0.87740, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1539s 1s/step - loss: 0.5680 - acc: 0.8562 - val_loss: 0.5021 - val_acc: 0.8774\n",
      "Epoch 71/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5755 - acc: 0.8529Epoch 00071: val_acc improved from 0.87740 to 0.87800, saving model to weights-with-augmentation-val-acc-lr.best.hdf5\n",
      "1406/1406 [==============================] - 1541s 1s/step - loss: 0.5755 - acc: 0.8528 - val_loss: 0.5040 - val_acc: 0.8780\n",
      "Epoch 72/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5653 - acc: 0.8552Epoch 00072: val_acc did not improve\n",
      "1406/1406 [==============================] - 1541s 1s/step - loss: 0.5653 - acc: 0.8552 - val_loss: 0.5064 - val_acc: 0.8752\n",
      "Epoch 73/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5655 - acc: 0.8584Epoch 00073: val_acc did not improve\n",
      "1406/1406 [==============================] - 1551s 1s/step - loss: 0.5653 - acc: 0.8584 - val_loss: 0.5094 - val_acc: 0.8752\n",
      "Epoch 74/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5726 - acc: 0.8533Epoch 00074: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 0.5727 - acc: 0.8532 - val_loss: 0.5075 - val_acc: 0.8776\n",
      "Epoch 75/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5736 - acc: 0.8535Epoch 00075: val_acc did not improve\n",
      "1406/1406 [==============================] - 1536s 1s/step - loss: 0.5736 - acc: 0.8535 - val_loss: 0.5060 - val_acc: 0.8752\n",
      "Epoch 76/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5596 - acc: 0.8579Epoch 00076: val_acc did not improve\n",
      "1406/1406 [==============================] - 1522s 1s/step - loss: 0.5596 - acc: 0.8579 - val_loss: 0.5077 - val_acc: 0.8758\n",
      "Epoch 77/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5703 - acc: 0.8544Epoch 00077: val_acc did not improve\n",
      "1406/1406 [==============================] - 1528s 1s/step - loss: 0.5703 - acc: 0.8544 - val_loss: 0.5073 - val_acc: 0.8744\n",
      "Epoch 78/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5633 - acc: 0.8575Epoch 00078: val_acc did not improve\n",
      "1406/1406 [==============================] - 1513s 1s/step - loss: 0.5633 - acc: 0.8575 - val_loss: 0.5068 - val_acc: 0.8746\n",
      "Epoch 79/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5659 - acc: 0.8562Epoch 00079: val_acc did not improve\n",
      "1406/1406 [==============================] - 1513s 1s/step - loss: 0.5660 - acc: 0.8562 - val_loss: 0.5078 - val_acc: 0.8748\n",
      "Epoch 80/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5653 - acc: 0.8575Epoch 00080: val_acc did not improve\n",
      "1406/1406 [==============================] - 1528s 1s/step - loss: 0.5654 - acc: 0.8574 - val_loss: 0.5071 - val_acc: 0.8748\n",
      "Epoch 81/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5634 - acc: 0.8563Epoch 00081: val_acc did not improve\n",
      "1406/1406 [==============================] - 1517s 1s/step - loss: 0.5636 - acc: 0.8563 - val_loss: 0.5066 - val_acc: 0.8754\n",
      "Epoch 82/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5649 - acc: 0.8550Epoch 00082: val_acc did not improve\n",
      "1406/1406 [==============================] - 1518s 1s/step - loss: 0.5648 - acc: 0.8551 - val_loss: 0.5053 - val_acc: 0.8738\n",
      "Epoch 83/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5642 - acc: 0.8575Epoch 00083: val_acc did not improve\n",
      "1406/1406 [==============================] - 1519s 1s/step - loss: 0.5644 - acc: 0.8574 - val_loss: 0.5056 - val_acc: 0.8734\n",
      "Epoch 84/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5630 - acc: 0.8580Epoch 00084: val_acc did not improve\n",
      "1406/1406 [==============================] - 1530s 1s/step - loss: 0.5630 - acc: 0.8580 - val_loss: 0.5068 - val_acc: 0.8730\n",
      "Epoch 85/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5638 - acc: 0.8567Epoch 00085: val_acc did not improve\n",
      "1406/1406 [==============================] - 1526s 1s/step - loss: 0.5637 - acc: 0.8567 - val_loss: 0.5061 - val_acc: 0.8746\n",
      "Epoch 86/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.5647 - acc: 0.8554Epoch 00086: val_acc did not improve\n",
      "1406/1406 [==============================] - 1518s 1s/step - loss: 0.5648 - acc: 0.8554 - val_loss: 0.5048 - val_acc: 0.8748\n",
      "Epoch 00086: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(traingen.flow(x_train, y_train, batch_size=32),\n",
    "                              steps_per_epoch=x_train.shape[0]//32, \n",
    "                              validation_data=(x_valid,y_valid),\n",
    "                              epochs=200, \n",
    "                              verbose=1, \n",
    "                              max_queue_size=128,\n",
    "                              shuffle=True,\n",
    "                              callbacks=[checkpoint, early, lr_reducer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 117s 12ms/step\n",
      "Loss: 0.53 Accuracy: 87.13%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(x_test, y_test, batch_size=512)\n",
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nST2TgiSpZ1KQJPVMCpKk3ifRR7JRe5of0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4ac796dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIn78AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff45628f9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
