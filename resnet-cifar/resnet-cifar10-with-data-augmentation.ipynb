{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Add\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import backend\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "xx_train = xx_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = xx_train[:45000]\n",
    "y_train = yy_train[:45000]\n",
    "x_valid = xx_train[45000:50000]\n",
    "y_valid = yy_train[45000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traingen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        fill_mode='nearest')\n",
    "traingen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    shape1 = backend.int_shape(a)\n",
    "    shape2 = backend.int_shape(b)\n",
    "    w = int(round(shape1[1]/shape2[1]))\n",
    "    h = int(round(shape1[2]/shape2[2]))\n",
    "    eq = shape1[3] == shape2[3]\n",
    "    \n",
    "    tmp = a\n",
    "    print w,h,eq\n",
    "    print shape1, shape2\n",
    "    if w>1 or h>1 or not eq:\n",
    "        tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(a)\n",
    "    print backend.int_shape(tmp)\n",
    "    print\n",
    "    return Add()([tmp, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n",
      "1 1 True\n",
      "(None, 16, 16, 128) (None, 16, 16, 128)\n",
      "(None, 16, 16, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def layer(num_filt, size, strides, inp):\n",
    "    tmp = BatchNormalization(axis=3)(inp)\n",
    "    tmp = Activation('relu')(tmp)\n",
    "    tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(tmp)\n",
    "    return tmp\n",
    "\n",
    "l = Input(x_train.shape[1:])\n",
    "\n",
    "x = Conv2D(128, (7,7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#first layer\n",
    "num = 32\n",
    "xtmp = Conv2D(num, (1,1), strides=1, \n",
    "              padding='same', \n",
    "              kernel_initializer='he_normal', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "\n",
    "x = add(x,xtmp)\n",
    "\n",
    "#other layer\n",
    "xtmp = layer(num, (1,1), 1, x)\n",
    "xtmp = layer(num, (3,3), 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "x = add(x,xtmp)\n",
    "\n",
    "for i in xrange(9):\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    #other layer\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "xshape = backend.int_shape(x)\n",
    "x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights-with-augmentation.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 16, 16, 128)  18944       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 16, 16, 128)  512         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 16, 16, 128)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 16, 16, 32)   4128        activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 16, 16, 32)   128         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 16, 16, 32)   0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 16, 16, 32)   9248        activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 16, 16, 32)   128         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 16, 16, 32)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 16, 16, 128)  4224        activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 16, 16, 128)  0           activation_123[0][0]             \n",
      "                                                                 conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 16, 16, 128)  512         add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 16, 16, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 16, 16, 32)   4128        activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 16, 16, 32)   128         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 16, 16, 32)   0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 16, 16, 32)   9248        activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 16, 16, 32)   128         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 16, 16, 32)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 16, 16, 128)  4224        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 16, 16, 128)  0           add_41[0][0]                     \n",
      "                                                                 conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 16, 16, 128)  512         add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 16, 16, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 16, 16, 32)   4128        activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 16, 16, 32)   128         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 16, 16, 32)   0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 16, 16, 32)   9248        activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 16, 16, 32)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 16, 16, 128)  4224        activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 16, 16, 128)  0           add_42[0][0]                     \n",
      "                                                                 conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 16, 16, 128)  512         add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 16, 16, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 16, 16, 32)   4128        activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 16, 16, 32)   128         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 16, 16, 32)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 16, 16, 32)   9248        activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 16, 16, 32)   128         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 16, 16, 32)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 16, 16, 128)  4224        activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 16, 16, 128)  0           add_43[0][0]                     \n",
      "                                                                 conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 128)  512         add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 16, 16, 128)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 32)   4128        activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 32)   128         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 16, 16, 32)   0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 32)   9248        activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 16, 16, 32)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 16, 16, 128)  4224        activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 16, 16, 128)  0           add_44[0][0]                     \n",
      "                                                                 conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 128)  512         add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 16, 16, 128)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 16, 16, 32)   4128        activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 32)   128         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 16, 16, 32)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 16, 16, 32)   9248        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 32)   128         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 16, 16, 32)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 16, 16, 128)  4224        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 128)  0           add_45[0][0]                     \n",
      "                                                                 conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 128)  512         add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 16, 16, 128)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 32)   4128        activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 32)   128         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 32)   0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 32)   9248        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 32)   128         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 32)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 128)  4224        activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 128)  0           add_46[0][0]                     \n",
      "                                                                 conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 16, 16, 128)  512         add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 16, 16, 128)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 32)   4128        activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 16, 16, 32)   128         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 16, 16, 32)   0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 32)   9248        activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 16, 16, 32)   128         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 16, 16, 32)   0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 128)  4224        activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 128)  0           add_47[0][0]                     \n",
      "                                                                 conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 16, 16, 128)  512         add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 16, 16, 128)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 32)   4128        activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 16, 16, 32)   128         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 16, 16, 32)   0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 32)   9248        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 16, 16, 32)   128         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 16, 16, 32)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 128)  4224        activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 16, 16, 128)  0           add_48[0][0]                     \n",
      "                                                                 conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 16, 16, 128)  512         add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 16, 16, 128)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 32)   4128        activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 16, 16, 32)   128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 16, 16, 32)   0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 32)   9248        activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 16, 16, 32)   128         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 16, 16, 32)   0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 128)  4224        activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 16, 16, 128)  0           add_49[0][0]                     \n",
      "                                                                 conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 16, 16, 128)  512         add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 16, 16, 128)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 32)   4128        activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 16, 16, 32)   128         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 16, 16, 32)   0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 16, 16, 32)   9248        activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 16, 16, 32)   128         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 16, 16, 32)   0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 16, 16, 128)  4224        activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 16, 16, 128)  0           add_50[0][0]                     \n",
      "                                                                 conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 16, 16, 128)  512         add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 16, 16, 128)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 16, 16, 32)   4128        activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 16, 16, 32)   128         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 16, 16, 32)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 16, 16, 32)   9248        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 16, 16, 32)   128         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 16, 16, 32)   0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 16, 16, 128)  4224        activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 16, 16, 128)  0           add_51[0][0]                     \n",
      "                                                                 conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 16, 16, 128)  512         add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 16, 16, 128)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 16, 16, 32)   4128        activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 16, 16, 32)   128         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 16, 16, 32)   0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 32)   9248        activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 16, 16, 32)   128         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 16, 16, 32)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 128)  4224        activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 16, 16, 128)  0           add_52[0][0]                     \n",
      "                                                                 conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 16, 16, 128)  512         add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 16, 16, 128)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 32)   4128        activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 16, 16, 32)   128         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 16, 16, 32)   0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 16, 16, 32)   9248        activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 16, 16, 32)   128         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 16, 16, 32)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 16, 16, 128)  4224        activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 16, 16, 128)  0           add_53[0][0]                     \n",
      "                                                                 conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 128)  512         add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 16, 16, 128)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 16, 16, 32)   4128        activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 32)   128         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 16, 16, 32)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 16, 16, 32)   9248        activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 32)   128         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 16, 16, 32)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 16, 16, 128)  4224        activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 16, 16, 128)  0           add_54[0][0]                     \n",
      "                                                                 conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 16, 16, 128)  512         add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 16, 16, 128)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 16, 16, 32)   4128        activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 16, 16, 32)   128         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 16, 16, 32)   0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 16, 16, 32)   9248        activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 16, 16, 32)   128         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 16, 16, 32)   0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 16, 16, 128)  4224        activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 128)  0           add_55[0][0]                     \n",
      "                                                                 conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 16, 16, 128)  512         add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 16, 16, 128)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 16, 16, 32)   4128        activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 16, 16, 32)   128         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 16, 16, 32)   0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 16, 16, 32)   9248        activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 16, 16, 32)   128         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 16, 16, 32)   0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 16, 16, 128)  4224        activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 16, 16, 128)  0           add_56[0][0]                     \n",
      "                                                                 conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 16, 16, 128)  512         add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 16, 16, 128)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 16, 16, 32)   4128        activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 16, 16, 32)   128         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 16, 16, 32)   0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 16, 16, 32)   9248        activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 16, 16, 32)   128         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 16, 16, 32)   0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 16, 16, 128)  4224        activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 16, 16, 128)  0           add_57[0][0]                     \n",
      "                                                                 conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 16, 16, 128)  512         add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 16, 16, 128)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 16, 16, 32)   4128        activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 16, 16, 32)   128         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 16, 16, 32)   0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 16, 16, 32)   9248        activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 16, 16, 32)   128         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 16, 16, 32)   0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 16, 16, 128)  4224        activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 16, 16, 128)  0           add_58[0][0]                     \n",
      "                                                                 conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 16, 16, 128)  512         add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 16, 16, 128)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 16, 16, 32)   4128        activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 16, 16, 32)   128         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 16, 16, 32)   0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 16, 16, 32)   9248        activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 16, 16, 32)   128         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 16, 16, 32)   0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 16, 16, 128)  4224        activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 16, 16, 128)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 16, 16, 128)  512         add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 16, 16, 128)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 128)    0           activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 388,106\n",
      "Trainable params: 380,170\n",
      "Non-trainable params: 7,936\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if training was cut short\n",
    "model = load_model('weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 2.1334 - acc: 0.3861Epoch 00001: val_acc improved from -inf to 0.39860, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 5471s 4s/step - loss: 2.1331 - acc: 0.3862 - val_loss: 2.0329 - val_acc: 0.3986\n",
      "Epoch 2/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.7273 - acc: 0.4857Epoch 00002: val_acc improved from 0.39860 to 0.45180, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1551s 1s/step - loss: 1.7271 - acc: 0.4858 - val_loss: 2.1599 - val_acc: 0.4518\n",
      "Epoch 3/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.5694 - acc: 0.5237Epoch 00003: val_acc improved from 0.45180 to 0.48920, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1529s 1s/step - loss: 1.5692 - acc: 0.5238 - val_loss: 1.7589 - val_acc: 0.4892\n",
      "Epoch 4/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.4683 - acc: 0.5546Epoch 00004: val_acc did not improve\n",
      "1406/1406 [==============================] - 1539s 1s/step - loss: 1.4686 - acc: 0.5545 - val_loss: 1.9538 - val_acc: 0.4444\n",
      "Epoch 5/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.3960 - acc: 0.5765Epoch 00005: val_acc improved from 0.48920 to 0.53280, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1554s 1s/step - loss: 1.3961 - acc: 0.5765 - val_loss: 1.6535 - val_acc: 0.5328\n",
      "Epoch 6/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.3395 - acc: 0.5971Epoch 00006: val_acc did not improve\n",
      "1406/1406 [==============================] - 1544s 1s/step - loss: 1.3397 - acc: 0.5971 - val_loss: 1.6553 - val_acc: 0.4994\n",
      "Epoch 7/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2985 - acc: 0.6103Epoch 00007: val_acc improved from 0.53280 to 0.59480, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 1.2986 - acc: 0.6103 - val_loss: 1.4312 - val_acc: 0.5948\n",
      "Epoch 8/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2596 - acc: 0.6274Epoch 00008: val_acc did not improve\n",
      "1406/1406 [==============================] - 1536s 1s/step - loss: 1.2594 - acc: 0.6274 - val_loss: 1.4077 - val_acc: 0.5848\n",
      "Epoch 9/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.2297 - acc: 0.6386Epoch 00009: val_acc improved from 0.59480 to 0.61580, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1522s 1s/step - loss: 1.2295 - acc: 0.6386 - val_loss: 1.3898 - val_acc: 0.6158\n",
      "Epoch 10/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1958 - acc: 0.6490Epoch 00010: val_acc did not improve\n",
      "1406/1406 [==============================] - 1524s 1s/step - loss: 1.1956 - acc: 0.6490 - val_loss: 1.5056 - val_acc: 0.6064\n",
      "Epoch 11/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1730 - acc: 0.6597Epoch 00011: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 1.1727 - acc: 0.6598 - val_loss: 1.7519 - val_acc: 0.5842\n",
      "Epoch 12/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1433 - acc: 0.6709Epoch 00012: val_acc improved from 0.61580 to 0.62260, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1539s 1s/step - loss: 1.1433 - acc: 0.6709 - val_loss: 1.4011 - val_acc: 0.6226\n",
      "Epoch 13/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1310 - acc: 0.6728Epoch 00013: val_acc improved from 0.62260 to 0.71400, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1544s 1s/step - loss: 1.1311 - acc: 0.6727 - val_loss: 1.0441 - val_acc: 0.7140\n",
      "Epoch 14/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.1154 - acc: 0.6829Epoch 00014: val_acc did not improve\n",
      "1406/1406 [==============================] - 1552s 1s/step - loss: 1.1152 - acc: 0.6830 - val_loss: 1.1233 - val_acc: 0.6914\n",
      "Epoch 15/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0902 - acc: 0.6895Epoch 00015: val_acc did not improve\n",
      "1406/1406 [==============================] - 1542s 1s/step - loss: 1.0900 - acc: 0.6895 - val_loss: 1.2832 - val_acc: 0.6516\n",
      "Epoch 16/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0799 - acc: 0.6957Epoch 00016: val_acc improved from 0.71400 to 0.73580, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1527s 1s/step - loss: 1.0797 - acc: 0.6957 - val_loss: 0.9840 - val_acc: 0.7358\n",
      "Epoch 17/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0637 - acc: 0.7004Epoch 00017: val_acc did not improve\n",
      "1406/1406 [==============================] - 1663s 1s/step - loss: 1.0635 - acc: 0.7004 - val_loss: 1.1694 - val_acc: 0.6744\n",
      "Epoch 18/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0530 - acc: 0.7043Epoch 00018: val_acc did not improve\n",
      "1406/1406 [==============================] - 1623s 1s/step - loss: 1.0531 - acc: 0.7042 - val_loss: 1.0975 - val_acc: 0.7010\n",
      "Epoch 19/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0509 - acc: 0.7063Epoch 00019: val_acc did not improve\n",
      "1406/1406 [==============================] - 1533s 1s/step - loss: 1.0509 - acc: 0.7063 - val_loss: 1.0214 - val_acc: 0.7160\n",
      "Epoch 20/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0362 - acc: 0.7137Epoch 00020: val_acc did not improve\n",
      "1406/1406 [==============================] - 1731s 1s/step - loss: 1.0362 - acc: 0.7137 - val_loss: 1.1052 - val_acc: 0.7216\n",
      "Epoch 21/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0241 - acc: 0.7155Epoch 00021: val_acc did not improve\n",
      "1406/1406 [==============================] - 1694s 1s/step - loss: 1.0240 - acc: 0.7155 - val_loss: 1.0013 - val_acc: 0.7344\n",
      "Epoch 22/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0158 - acc: 0.7198Epoch 00022: val_acc improved from 0.73580 to 0.74860, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1581s 1s/step - loss: 1.0157 - acc: 0.7198 - val_loss: 0.9603 - val_acc: 0.7486\n",
      "Epoch 23/200\n",
      "1405/1406 [============================>.] - ETA: 2s - loss: 1.0057 - acc: 0.7242Epoch 00023: val_acc improved from 0.74860 to 0.74960, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 3980s 3s/step - loss: 1.0059 - acc: 0.7241 - val_loss: 0.9338 - val_acc: 0.7496\n",
      "Epoch 24/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 1.0035 - acc: 0.7253Epoch 00024: val_acc did not improve\n",
      "1406/1406 [==============================] - 1509s 1s/step - loss: 1.0035 - acc: 0.7253 - val_loss: 0.9934 - val_acc: 0.7398\n",
      "Epoch 25/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9981 - acc: 0.7289Epoch 00025: val_acc did not improve\n",
      "1406/1406 [==============================] - 1509s 1s/step - loss: 0.9981 - acc: 0.7289 - val_loss: 1.0050 - val_acc: 0.7374\n",
      "Epoch 26/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9916 - acc: 0.7275Epoch 00026: val_acc did not improve\n",
      "1406/1406 [==============================] - 1510s 1s/step - loss: 0.9915 - acc: 0.7275 - val_loss: 1.1520 - val_acc: 0.7032\n",
      "Epoch 27/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9821 - acc: 0.7320Epoch 00027: val_acc improved from 0.74960 to 0.77100, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1509s 1s/step - loss: 0.9821 - acc: 0.7320 - val_loss: 0.9003 - val_acc: 0.7710\n",
      "Epoch 28/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9744 - acc: 0.7356Epoch 00028: val_acc did not improve\n",
      "1406/1406 [==============================] - 1513s 1s/step - loss: 0.9742 - acc: 0.7357 - val_loss: 1.0101 - val_acc: 0.7354\n",
      "Epoch 29/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9725 - acc: 0.7377Epoch 00029: val_acc did not improve\n",
      "1406/1406 [==============================] - 1520s 1s/step - loss: 0.9729 - acc: 0.7375 - val_loss: 1.0218 - val_acc: 0.7442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9672 - acc: 0.7407Epoch 00030: val_acc did not improve\n",
      "1406/1406 [==============================] - 1521s 1s/step - loss: 0.9671 - acc: 0.7407 - val_loss: 1.0630 - val_acc: 0.7348\n",
      "Epoch 31/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9580 - acc: 0.7425Epoch 00031: val_acc did not improve\n",
      "1406/1406 [==============================] - 1523s 1s/step - loss: 0.9580 - acc: 0.7425 - val_loss: 1.1879 - val_acc: 0.6992\n",
      "Epoch 32/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9540 - acc: 0.7421Epoch 00032: val_acc did not improve\n",
      "1406/1406 [==============================] - 1557s 1s/step - loss: 0.9537 - acc: 0.7423 - val_loss: 0.9183 - val_acc: 0.7644\n",
      "Epoch 33/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.9486 - acc: 0.7462Epoch 00033: val_acc did not improve\n",
      "1406/1406 [==============================] - 1541s 1s/step - loss: 0.9487 - acc: 0.7462 - val_loss: 1.2936 - val_acc: 0.6798\n",
      "Epoch 34/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8354 - acc: 0.7844Epoch 00034: val_acc improved from 0.77100 to 0.84660, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1539s 1s/step - loss: 0.8354 - acc: 0.7844 - val_loss: 0.6612 - val_acc: 0.8466\n",
      "Epoch 35/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.8050 - acc: 0.7936Epoch 00035: val_acc improved from 0.84660 to 0.84680, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1527s 1s/step - loss: 0.8051 - acc: 0.7935 - val_loss: 0.6641 - val_acc: 0.8468\n",
      "Epoch 36/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7816 - acc: 0.8001Epoch 00036: val_acc did not improve\n",
      "1406/1406 [==============================] - 1522s 1s/step - loss: 0.7816 - acc: 0.8001 - val_loss: 0.6966 - val_acc: 0.8356\n",
      "Epoch 37/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7779 - acc: 0.8009Epoch 00037: val_acc did not improve\n",
      "1406/1406 [==============================] - 1520s 1s/step - loss: 0.7779 - acc: 0.8009 - val_loss: 0.7100 - val_acc: 0.8338\n",
      "Epoch 38/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7713 - acc: 0.8025Epoch 00038: val_acc did not improve\n",
      "1406/1406 [==============================] - 1536s 1s/step - loss: 0.7711 - acc: 0.8026 - val_loss: 0.6813 - val_acc: 0.8396\n",
      "Epoch 39/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7592 - acc: 0.8040Epoch 00039: val_acc improved from 0.84680 to 0.85540, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1517s 1s/step - loss: 0.7594 - acc: 0.8039 - val_loss: 0.6199 - val_acc: 0.8554\n",
      "Epoch 40/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7519 - acc: 0.8073Epoch 00040: val_acc did not improve\n",
      "1406/1406 [==============================] - 1512s 1s/step - loss: 0.7519 - acc: 0.8073 - val_loss: 0.6416 - val_acc: 0.8468\n",
      "Epoch 41/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7395 - acc: 0.8102Epoch 00041: val_acc did not improve\n",
      "1406/1406 [==============================] - 1529s 1s/step - loss: 0.7397 - acc: 0.8102 - val_loss: 0.6620 - val_acc: 0.8390\n",
      "Epoch 42/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7362 - acc: 0.8068Epoch 00042: val_acc did not improve\n",
      "1406/1406 [==============================] - 1538s 1s/step - loss: 0.7361 - acc: 0.8068 - val_loss: 0.6334 - val_acc: 0.8492\n",
      "Epoch 43/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7308 - acc: 0.8106Epoch 00043: val_acc did not improve\n",
      "1406/1406 [==============================] - 1532s 1s/step - loss: 0.7308 - acc: 0.8107 - val_loss: 0.6514 - val_acc: 0.8460\n",
      "Epoch 44/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7285 - acc: 0.8102Epoch 00044: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 0.7284 - acc: 0.8103 - val_loss: 0.6437 - val_acc: 0.8466\n",
      "Epoch 45/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.7176 - acc: 0.8131Epoch 00045: val_acc did not improve\n",
      "1406/1406 [==============================] - 1528s 1s/step - loss: 0.7177 - acc: 0.8131 - val_loss: 0.7279 - val_acc: 0.8162\n",
      "Epoch 46/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6712 - acc: 0.8309Epoch 00046: val_acc improved from 0.85540 to 0.86260, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1536s 1s/step - loss: 0.6712 - acc: 0.8309 - val_loss: 0.5805 - val_acc: 0.8626\n",
      "Epoch 47/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6684 - acc: 0.8295Epoch 00047: val_acc improved from 0.86260 to 0.86660, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1538s 1s/step - loss: 0.6685 - acc: 0.8295 - val_loss: 0.5700 - val_acc: 0.8666\n",
      "Epoch 48/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6536 - acc: 0.8348Epoch 00048: val_acc improved from 0.86660 to 0.87020, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1536s 1s/step - loss: 0.6535 - acc: 0.8348 - val_loss: 0.5650 - val_acc: 0.8702\n",
      "Epoch 49/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6507 - acc: 0.8358Epoch 00049: val_acc improved from 0.87020 to 0.87520, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1530s 1s/step - loss: 0.6509 - acc: 0.8358 - val_loss: 0.5528 - val_acc: 0.8752\n",
      "Epoch 50/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6464 - acc: 0.8374Epoch 00050: val_acc did not improve\n",
      "1406/1406 [==============================] - 1533s 1s/step - loss: 0.6464 - acc: 0.8374 - val_loss: 0.5481 - val_acc: 0.8688\n",
      "Epoch 51/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6439 - acc: 0.8356Epoch 00051: val_acc improved from 0.87520 to 0.87940, saving model to weights-with-augmentation.best.hdf5\n",
      "1406/1406 [==============================] - 1535s 1s/step - loss: 0.6440 - acc: 0.8356 - val_loss: 0.5449 - val_acc: 0.8794\n",
      "Epoch 52/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6371 - acc: 0.8393Epoch 00052: val_acc did not improve\n",
      "1406/1406 [==============================] - 1533s 1s/step - loss: 0.6370 - acc: 0.8393 - val_loss: 0.5400 - val_acc: 0.8772\n",
      "Epoch 53/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6407 - acc: 0.8394Epoch 00053: val_acc did not improve\n",
      "1406/1406 [==============================] - 1539s 1s/step - loss: 0.6407 - acc: 0.8394 - val_loss: 0.5504 - val_acc: 0.8764\n",
      "Epoch 54/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6326 - acc: 0.8398Epoch 00054: val_acc did not improve\n",
      "1406/1406 [==============================] - 1534s 1s/step - loss: 0.6327 - acc: 0.8397 - val_loss: 0.5481 - val_acc: 0.8718\n",
      "Epoch 55/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6275 - acc: 0.8407Epoch 00055: val_acc did not improve\n",
      "1406/1406 [==============================] - 1537s 1s/step - loss: 0.6277 - acc: 0.8406 - val_loss: 0.5637 - val_acc: 0.8660\n",
      "Epoch 56/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6253 - acc: 0.8410Epoch 00056: val_acc did not improve\n",
      "1406/1406 [==============================] - 1543s 1s/step - loss: 0.6255 - acc: 0.8410 - val_loss: 0.5324 - val_acc: 0.8768\n",
      "Epoch 57/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6262 - acc: 0.8407Epoch 00057: val_acc did not improve\n",
      "1406/1406 [==============================] - 1548s 1s/step - loss: 0.6263 - acc: 0.8406 - val_loss: 0.5357 - val_acc: 0.8742\n",
      "Epoch 58/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6197 - acc: 0.8432Epoch 00058: val_acc did not improve\n",
      "1406/1406 [==============================] - 1542s 1s/step - loss: 0.6197 - acc: 0.8432 - val_loss: 0.5425 - val_acc: 0.8718\n",
      "Epoch 59/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6200 - acc: 0.8446Epoch 00059: val_acc did not improve\n",
      "1406/1406 [==============================] - 1542s 1s/step - loss: 0.6199 - acc: 0.8446 - val_loss: 0.5240 - val_acc: 0.8776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6132 - acc: 0.8460Epoch 00060: val_acc did not improve\n",
      "1406/1406 [==============================] - 1545s 1s/step - loss: 0.6132 - acc: 0.8460 - val_loss: 0.5392 - val_acc: 0.8770\n",
      "Epoch 61/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6123 - acc: 0.8452Epoch 00061: val_acc did not improve\n",
      "1406/1406 [==============================] - 1555s 1s/step - loss: 0.6123 - acc: 0.8452 - val_loss: 0.5473 - val_acc: 0.8706\n",
      "Epoch 62/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6087 - acc: 0.8469Epoch 00062: val_acc did not improve\n",
      "1406/1406 [==============================] - 1547s 1s/step - loss: 0.6088 - acc: 0.8469 - val_loss: 0.5410 - val_acc: 0.8742\n",
      "Epoch 63/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6104 - acc: 0.8470Epoch 00063: val_acc did not improve\n",
      "1406/1406 [==============================] - 1528s 1s/step - loss: 0.6106 - acc: 0.8470 - val_loss: 0.5265 - val_acc: 0.8780\n",
      "Epoch 64/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6051 - acc: 0.8460Epoch 00064: val_acc did not improve\n",
      "1406/1406 [==============================] - 1529s 1s/step - loss: 0.6050 - acc: 0.8460 - val_loss: 0.5333 - val_acc: 0.8724\n",
      "Epoch 65/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6023 - acc: 0.8469Epoch 00065: val_acc did not improve\n",
      "1406/1406 [==============================] - 1523s 1s/step - loss: 0.6022 - acc: 0.8469 - val_loss: 0.5192 - val_acc: 0.8786\n",
      "Epoch 66/200\n",
      "1405/1406 [============================>.] - ETA: 1s - loss: 0.6004 - acc: 0.8485Epoch 00066: val_acc did not improve\n",
      "1406/1406 [==============================] - 1521s 1s/step - loss: 0.6004 - acc: 0.8485 - val_loss: 0.5376 - val_acc: 0.8758\n",
      "Epoch 00066: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(traingen.flow(x_train, y_train, batch_size=32),\n",
    "                              steps_per_epoch=x_train.shape[0]//32, \n",
    "                              validation_data=(x_valid,y_valid),\n",
    "                              epochs=200, \n",
    "                              verbose=1, \n",
    "                              max_queue_size=128,\n",
    "                              shuffle=True,\n",
    "                              callbacks=[checkpoint, early, lr_reducer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result after stopping at 66 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 115s 12ms/step\n",
      "Loss: 0.55 Accuracy: 87.25%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(x_test, y_test, batch_size=512)\n",
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result of 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 142s 14ms/step\n",
      "Loss: 0.98 Accuracy: 80.06%\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model('weights.best.hdf5')\n",
    "res = model2.evaluate(x_test, y_test, batch_size=512)\n",
    "print('Loss: %.2f Accuracy: %.2f%%' % (res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1bb99c7290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1bb9b32e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
