{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.196238Z",
     "start_time": "2017-11-17T09:03:28.644004Z"
    },
    "_cell_guid": "679e0d3e-646d-4e96-9eb0-b362d8c6e51f",
    "_uuid": "0d05e5ce89af3e25d1c1fb244d021a1cfa1a058c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import cv2\n",
    "\n",
    "from scipy import signal\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Add, Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Input, Dense, Conv2D, AveragePooling2D, Activation, GlobalAveragePooling2D, Lambda\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import Initializer\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave, imresize\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, AvgPool1D, Add\n",
    "from keras import backend\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.210749Z",
     "start_time": "2017-11-17T09:03:29.19832Z"
    },
    "_cell_guid": "8ab00801-08b9-44d3-a063-32e82dbf8f58",
    "_uuid": "53c19941676690454dd4b91109976b6c59cb7a40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.325023Z",
     "start_time": "2017-11-17T09:03:29.215137Z"
    },
    "_cell_guid": "8d7ebf53-700b-4c06-b5c2-ccf9ed5f27e0",
    "_uuid": "133424c750b26df37900f9cebcfd2f2fb803cb8b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57929 train, 6798 val, and 6 bg noise samples\n"
     ]
    }
   ],
   "source": [
    "DATADIR = './data' # unzipped train and test data\n",
    "OUTDIR = './model-k' # just a random name\n",
    "\n",
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\" Return 2 lists of tuples:\n",
    "    [(class_id, user_id, path), ...] for train\n",
    "    [(class_id, user_id, path), ...] for validation\n",
    "    \"\"\"\n",
    "    # Just a simple regexp for paths with three groups:\n",
    "    # prefix, label, user_id\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "\n",
    "    with open(os.path.join(data_dir, 'train/validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "\n",
    "    possible = set(POSSIBLE_LABELS)\n",
    "    train, val, bg_noise = [], [], []\n",
    "    for entry in all_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_background_noise_':\n",
    "                bg_noise.append(entry)\n",
    "                label = 'silence'\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "\n",
    "            label_id = name2id[label]\n",
    "\n",
    "            sample = (label_id, uid, entry)\n",
    "            if uid in valset:\n",
    "                val.append(sample)\n",
    "            else:\n",
    "                train.append(sample)\n",
    "\n",
    "    print('There are {} train, {} val, and {} bg noise samples'.format(len(train), len(val), len(bg_noise)))\n",
    "    return train, val, bg_noise\n",
    "\n",
    "train_df, valid_df, noise_df = load_data(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:31.144688Z",
     "start_time": "2017-11-17T09:03:31.105987Z"
    },
    "_cell_guid": "9e32f039-712e-4f1d-9173-ed9804d7771f",
    "_uuid": "36562e906f4fd6739b55582239ab55d947a80766",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_wav_file(origWav):\n",
    "    wav = np.copy(origWav)\n",
    "#     wav = read_wav_file(fname)\n",
    "    \n",
    "    L = 16000  # 1 sec\n",
    "    \n",
    "    if len(wav) > L:\n",
    "        i = np.random.randint(0, len(wav) - L)\n",
    "        wav = wav[i:(i+L)]\n",
    "    elif len(wav) < L:\n",
    "        pad_len = L - len(wav)\n",
    "        silence_part_left  = np.random.uniform(-0.001,0.001,int(pad_len/2.))\n",
    "        silence_part_right = np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2.)))\n",
    "        wav = np.concatenate([silence_part_left, wav, silence_part_right])\n",
    "        \n",
    "    wav = signal.resample(wav, int(0.5 * wav.shape[0]))\n",
    "    wav = np.expand_dims(wav, axis=1)\n",
    "    wav = np.expand_dims(wav, axis=1)\n",
    "    return wav.astype(np.float32)\n",
    "    specgram = signal.stft(wav, 16000, nperseg = 400, noverlap = 240, nfft = 512, padded = False, boundary = None)\n",
    "    \n",
    "    phase = np.angle(specgram[2]) / np.pi\n",
    "    amp = np.log1p(np.abs(specgram[2]))\n",
    "    \n",
    "#     shape = (96,32)\n",
    "#     phase = imresize(phase, shape, mode='F')\n",
    "#     amp = imresize(amp, shape, mode='F')\n",
    "    \n",
    "    stacked = np.stack([phase, amp], axis = 2)\n",
    "    return stacked\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav = wav_read(train_df[np.random.randint(len(train_df))][2])\n",
    "p = process_wav_file(wav)\n",
    "a,b = cv2.split(p)\n",
    "shape = (96,32)\n",
    "a = a.astype(np.float32)\n",
    "b = b.astype(np.float32)\n",
    "print b.dtype\n",
    "\n",
    "log_spect = np.log(a)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "a = imresize(a, shape, mode='F')\n",
    "\n",
    "log_spect = np.log(a)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "log_spect = np.log(b)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "b = imresize(b, shape, mode='F')\n",
    "\n",
    "log_spect = np.log(b)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wav_read(fname):\n",
    "    wav, _ = librosa.load(fname, sr=None)\n",
    "    return wav\n",
    "\n",
    "def normalize_audio(wav):\n",
    "    return wav/max(wav)\n",
    "\n",
    "def time_shift(wav, shift):\n",
    "    start_ = int(shift)\n",
    "    if start_ >= 0:\n",
    "        wav_time_shift = np.r_[wav[start_:], np.random.uniform(-0.001,0.001, start_)]\n",
    "    else:\n",
    "        wav_time_shift = np.r_[np.random.uniform(-0.001,0.001, -start_), wav[:start_]]\n",
    "    return normalize_audio(wav_time_shift)\n",
    "\n",
    "def speed_change(wav, speed_rate):\n",
    "    # rate: lower is faster\n",
    "    wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "    if len(wav_speed_tune) < 16000:\n",
    "        pad_len = 16000 - len(wav_speed_tune)\n",
    "        wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len/2.)),\n",
    "                               wav_speed_tune,\n",
    "                               np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2.)))]\n",
    "    else: \n",
    "        cut_len = len(wav_speed_tune) - 16000\n",
    "        wav_speed_tune = wav_speed_tune[int(cut_len/2.):int(cut_len/2.)+16000]\n",
    "    return normalize_audio(wav_speed_tune)\n",
    "\n",
    "def noise_add(wav, percent, ind):\n",
    "    bg = wav_read(noise_df[ind])\n",
    "    bg = normalize_audio(bg)\n",
    "    start_ = np.random.randint(bg.shape[0]-16000)\n",
    "    bg_slice = bg[start_ : start_+16000]\n",
    "    wav_with_bg = wav * percent + bg_slice * (1-percent)\n",
    "    return normalize_audio(wav_with_bg)\n",
    "\n",
    "def get_spectrogram(wav):\n",
    "    v = 600\n",
    "    D = librosa.stft(wav, n_fft=v, hop_length=50,\n",
    "                     win_length=v, window='hamming')\n",
    "    spect, phase = librosa.magphase(D)\n",
    "    spect = scipy.ndimage.zoom(spect,1./7, order=1)\n",
    "    spect = spect.reshape(np.expand_dims(spect, axis=2).shape)\n",
    "    return spect\n",
    "\n",
    "def frange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump\n",
    "        \n",
    "def all_aug(wav, params):\n",
    "    time, speed, noise_percent, noise_ind = params\n",
    "    aug = wav\n",
    "    aug = time_shift(aug, time)\n",
    "    aug = speed_change(wav, speed)\n",
    "    if noise_ind == -1:\n",
    "        aug = noise_add(aug, 1, noise_ind)\n",
    "    else:\n",
    "        aug = noise_add(aug, noise_percent, noise_ind)\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def layer(filt, inp, comp):\n",
    "    x = Conv2D(filt, 8, strides=1, padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filt, 5, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filt, 3, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if not(inp_shape[-1] == filt):\n",
    "        tmp = Conv2D(filt, 1, strides=1, padding='same')(inp)\n",
    "        tmp = BatchNormalization()(tmp)\n",
    "    else:\n",
    "        tmp = BatchNormalization()(x)\n",
    "\n",
    "    x = Add()([x,tmp])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "inp_shape = process_wav_file(wav_read(train_df[0][2])).shape\n",
    "filt = 16\n",
    "\n",
    "l = Input(inp_shape)\n",
    "x = Conv2D(128, 7, strides=4, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = layer(filt, x, inp_shape[-1])\n",
    "\n",
    "######\n",
    "filt *= 2\n",
    "\n",
    "x = layer(filt, x, inp_shape[-1])\n",
    "\n",
    "filt *= 2\n",
    "x = layer(filt, x, inp_shape[-1])\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(len(POSSIBLE_LABELS), activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 8000, 1, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 2000, 1, 128) 6400        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2000, 1, 128) 512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 2000, 1, 16)  131088      batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2000, 1, 16)  64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 2000, 1, 16)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2000, 1, 16)  6416        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2000, 1, 16)  64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 2000, 1, 16)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2000, 1, 16)  2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 2000, 1, 16)  2064        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2000, 1, 16)  64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 2000, 1, 16)  64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 2000, 1, 16)  0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 2000, 1, 16)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 2000, 1, 32)  32800       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 2000, 1, 32)  128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 2000, 1, 32)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2000, 1, 32)  25632       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 2000, 1, 32)  128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2000, 1, 32)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 2000, 1, 32)  9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 2000, 1, 32)  544         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 2000, 1, 32)  128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 2000, 1, 32)  128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 2000, 1, 32)  0           batch_normalization_21[0][0]     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 2000, 1, 32)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 2000, 1, 64)  131136      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 2000, 1, 64)  256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2000, 1, 64)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 2000, 1, 64)  102464      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 2000, 1, 64)  256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2000, 1, 64)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 2000, 1, 64)  36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 2000, 1, 64)  2112        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 2000, 1, 64)  256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 2000, 1, 64)  256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 2000, 1, 64)  0           batch_normalization_25[0][0]     \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 2000, 1, 64)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 64)           0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           780         global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 492,236\n",
      "Trainable params: 491,084\n",
      "Non-trainable params: 1,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:32.519795Z",
     "start_time": "2017-11-17T09:03:32.483881Z"
    },
    "_cell_guid": "144c6e60-8a83-437d-8b8a-ea065af90923",
    "_uuid": "22e0e6c718171167089fb6df36d3dc43a1029992",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIV = 100.0\n",
    "\n",
    "def get_variations():\n",
    "    variations = [[0,1,1,-1]]\n",
    "\n",
    "#     time_shift_range = 4000\n",
    "#     for time in xrange(-time_shift_range, time_shift_range+1, 1000):\n",
    "#         for speed in frange(0.4,1.7,0.3):\n",
    "#             for noise_percentage in frange(0.2,1,0.1):\n",
    "#                 for noise_ind in xrange(-1,len(noise_df)):\n",
    "#                     variations.append([time, speed, noise_percentage, noise_ind])\n",
    "    return variations\n",
    "\n",
    "def get_variations_valid():\n",
    "    variations = [[0,1,1,-1]]\n",
    "\n",
    "    time_shift_range = 4000\n",
    "    for time in xrange(-time_shift_range, time_shift_range+1, 1000):\n",
    "        for speed in frange(0.4,1.7,0.3):\n",
    "            for noise_percentage in frange(0.2,1,0.1):\n",
    "                for noise_ind in xrange(-1,len(noise_df)):\n",
    "                    variations.append([time, speed, noise_percentage, noise_ind])\n",
    "    return variations\n",
    "\n",
    "var = get_variations()\n",
    "multiplier = np.ceil(len(var)/DIV)\n",
    "\n",
    "def train_generator(train_batch_size):\n",
    "    while True:\n",
    "        variations = get_variations()\n",
    "        len_var = len(variations)\n",
    "        selected_indices = np.random.choice(len_var, int(np.ceil(len_var/DIV)))\n",
    "        \n",
    "        wavs = []\n",
    "        tmp_train_df = np.array(train_df)\n",
    "        np.random.shuffle(tmp_train_df)\n",
    "        tmp_train_df = tmp_train_df.tolist()\n",
    "        \n",
    "        while True:\n",
    "            while len(wavs) < train_batch_size:\n",
    "                label_id, uid, fname = tmp_train_df.pop(0)\n",
    "                wav = wav_read(fname)\n",
    "                for i in selected_indices:\n",
    "                    augmented = all_aug(wav, variations[i])\n",
    "                    arr = [label_id, augmented]\n",
    "                    wavs.append(arr)\n",
    "            \n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for x in xrange(train_batch_size):\n",
    "                label_id, wav = wavs.pop(0)\n",
    "                x_batch.append(process_wav_file(wav))\n",
    "                y_batch.append(label_id)\n",
    "            \n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes=len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "            if len(tmp_train_df) == 0:\n",
    "                break\n",
    "            \n",
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "\n",
    "        variations = get_variations_valid()\n",
    "        \n",
    "        len_var = len(variations)\n",
    "        selected_indices = np.random.choice(len_var, int(np.ceil(len_var/DIV)))\n",
    "        \n",
    "        wavs = []\n",
    "        tmp_valid_df = np.array(valid_df)\n",
    "        np.random.shuffle(tmp_valid_df)\n",
    "        tmp_valid_df = tmp_valid_df.tolist()\n",
    "        \n",
    "        while True:\n",
    "            while len(wavs) < valid_batch_size:\n",
    "                label_id, uid, fname = tmp_valid_df.pop(0)\n",
    "                wav = wav_read(fname)\n",
    "                for i in selected_indices:\n",
    "                    augmented = all_aug(wav, variations[i])\n",
    "                    arr = [label_id, augmented]\n",
    "                    wavs.append(arr)\n",
    "            \n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for x in xrange(valid_batch_size):\n",
    "                label_id, wav = wavs.pop(0)\n",
    "                x_batch.append(process_wav_file(wav))\n",
    "                y_batch.append(label_id)\n",
    "            \n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes=len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "            if len(tmp_valid_df) == 0:\n",
    "                break\n",
    "            \n",
    "def test_generator(test_batch_size):\n",
    "    while True:\n",
    "        for start in range(0, len(test_paths), test_batch_size):\n",
    "            x_batch = []\n",
    "            end = min(start + test_batch_size, len(test_paths))\n",
    "            this_paths = test_paths[start:end]\n",
    "            for x in this_paths:\n",
    "                x_batch.append(process_wav_file(x))\n",
    "            x_batch = np.array(x_batch)\n",
    "            yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:42:31.48233Z",
     "start_time": "2017-11-17T09:03:33.355603Z"
    },
    "_cell_guid": "5f3d1b09-500f-410e-820a-8eaab24b6ebb",
    "_uuid": "528ec66a0a6caca952273ab916e609625839b19e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb07f7d9eff64099a45767aa9c126d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0868878886564634b9a83cfd87c69556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1745/1811 [===========================>..] - ETA: 7:35 - loss: 1.1811 - acc: 0.6541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommy/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1811 [============================>.] - ETA: 1:15 - loss: nan - acc: 0.6387"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/site-packages/keras/utils/data_utils.py\", line 579, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-12-06b383f943f4>\", line 41, in train_generator\n",
      "    label_id, uid, fname = tmp_train_df.pop(0)\n",
      "IndexError: pop from empty list\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1801/1811 [============================>.] - ETA: 1:08 - loss: nan - acc: 0.6384"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a88df73d4fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                               validation_steps=int(np.ceil(len(valid_df)*multiplier/batch_size)))\n\u001b[0m",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2046\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/tommy/anaconda2/lib/python2.7/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1,\n",
    "                           min_delta=0.01,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                               mode='min'),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='starter.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min',\n",
    "                            verbose=1),\n",
    "             TQDMNotebookCallback()]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(generator=train_generator(batch_size),\n",
    "                              steps_per_epoch=int(len(train_df)*multiplier/batch_size),\n",
    "                              epochs=120,\n",
    "                              verbose=1,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator(batch_size),\n",
    "                              validation_steps=int(len(valid_df)*multiplier/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:24:59.198625Z",
     "start_time": "2017-11-17T10:24:59.081762Z"
    },
    "_cell_guid": "0c99ba3b-e8ca-40cb-8d29-2b0e89a385c7",
    "_uuid": "429139ca4f71487c6cfe3e8dfbb6a659eb9bb9c8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./weights/starter.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:28:14.451612Z",
     "start_time": "2017-11-17T10:28:13.307142Z"
    },
    "_cell_guid": "72f27090-c0d1-4d0b-8027-34c915429a79",
    "_uuid": "1007977fccadecdae582ec5d8d52dd3c4c3010aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_paths = glob(os.path.join('./data/', 'test/audio/*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:32:14.882322Z",
     "start_time": "2017-11-17T10:32:14.863617Z"
    },
    "_cell_guid": "c6d9b369-9979-4bcd-8540-4653e6544f84",
    "_uuid": "6a0bb3c22b7b5c43db0ec5673333ab3de8f08724",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-17T10:32:45.947Z"
    },
    "_cell_guid": "1fb8aed4-de12-43c5-84bf-b803e3d640fa",
    "_uuid": "631a38cb0013e5772f6987854145ad76ecf6c430",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator(64), int(np.ceil(len(test_paths)/64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:30:44.236246Z",
     "start_time": "2017-11-17T11:30:44.21858Z"
    },
    "_cell_guid": "b1cdab5c-9816-4690-87d8-de2c97cf0e7d",
    "_uuid": "24eb7e512eace4567494e0a8e356a826f4283c4d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:31:11.212517Z",
     "start_time": "2017-11-17T11:31:10.786357Z"
    },
    "_cell_guid": "1da523cf-fdbf-4ab1-9300-0147155aa247",
    "_uuid": "f25d4e626202aa115bd0460f4de8d07f9727c83e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last batch will contain padding, so remove duplicates\n",
    "submission = dict()\n",
    "for i in range(len(test_paths)):\n",
    "    fname, label = os.path.basename(test_paths[i]), id2name[classes[i]]\n",
    "    submission[fname] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:32:05.154527Z",
     "start_time": "2017-11-17T11:32:04.983371Z"
    },
    "_cell_guid": "9a95d147-3f4b-4386-8597-5fa60be43542",
    "_uuid": "bdf63bce43a0525a02ac18ca3f90aeba06ce6e99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('starter_submission.csv', 'w') as fout:\n",
    "    fout.write('fname,label\\n')\n",
    "    for fname, label in submission.items():\n",
    "        fout.write('{},{}\\n'.format(fname, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bea6850-15c6-44e7-bdb4-9555ad196f85",
    "_uuid": "555315ef622793711ff5643928dac874c8cb0ed2",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:33.180074Z",
     "start_time": "2017-11-17T09:03:32.625939Z"
    },
    "_cell_guid": "0e13c01e-5662-4679-9b31-bf9347080ae5",
    "_uuid": "a17b3ea5c15c781260f3473f37dd0d36a932a565"
   },
   "outputs": [],
   "source": [
    "#Define model parameters\n",
    "model_depth = 16\n",
    "num_dense_blocks = 3\n",
    "growth_rate = 12\n",
    "number_filters = 16\n",
    "compression = 0.5\n",
    "num_layers_per_block = (model_depth - 4) // num_dense_blocks\n",
    "\n",
    "def dense_block(x,num_layers_per_block,growth_rate):\n",
    "    for i in range(num_layers_per_block//2):\n",
    "        x_ = BatchNormalization()(x)\n",
    "        x_ = Activation('relu')(x_)\n",
    "        x_ = Conv2D(number_filters,(3,3),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x_)\n",
    "        x_ = Dropout(0.2)(x_)\n",
    "        x_ = BatchNormalization()(x_)\n",
    "        x_ = Activation('relu')(x_)\n",
    "        x = Concatenate()([x,x_])\n",
    "    return x \n",
    "\n",
    "def transition_layers(x,compression):\n",
    "    updated_num_filters = int(x.get_shape().as_list()[-1] * compression)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(updated_num_filters,(1,1),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = AveragePooling2D()(x)\n",
    "    return x\n",
    "\n",
    "# #Let's define the model\n",
    "# inp = Input(shape = process_wav_file(wav_read(train_df[0][2])).shape)\n",
    "# x = Conv2D(number_filters,(1,1),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(inp)\n",
    "# for i in range(num_dense_blocks):\n",
    "#     x = dense_block(x,num_layers_per_block,growth_rate)\n",
    "#     if (i != num_dense_blocks-1):\n",
    "#         x = transition_layers(x,compression)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(32, activation = 'relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(len(POSSIBLE_LABELS), activation='softmax',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "# model = Model(inp, x)\n",
    "# # model.compile(Adam(), loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.compile(Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# plot_model(model, to_file='model.png')\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, AvgPool1D, Add\n",
    "from keras import backend\n",
    "\n",
    "def add(a,b):\n",
    "    shape1 = backend.int_shape(a)\n",
    "    shape2 = backend.int_shape(b)\n",
    "    w = int(round(shape1[1]/shape2[1]))\n",
    "    h = int(round(shape1[2]/shape2[2]))\n",
    "    eq = shape1[3] == shape2[3]\n",
    "    \n",
    "    tmp = a\n",
    "    print w,h,eq\n",
    "    print shape1, shape2\n",
    "    if w>1 or h>1 or not eq:\n",
    "        tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(a)\n",
    "    print backend.int_shape(tmp)\n",
    "    print\n",
    "    return Add()([tmp, b])\n",
    "\n",
    "def layer(num_filt, size, strides, inp):\n",
    "    tmp = BatchNormalization(axis=3)(inp)\n",
    "    tmp = Activation('relu')(tmp)\n",
    "    tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(tmp)\n",
    "    return tmp\n",
    "\n",
    "l = Input(process_wav_file(wav_read(train_df[0][2])).shape)\n",
    "\n",
    "x = Conv2D(128, 7, strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(l)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#first layer\n",
    "num = 32\n",
    "xtmp = Conv2D(num, (1,1), strides=1, \n",
    "              padding='same', \n",
    "              kernel_initializer='he_normal', \n",
    "              kernel_regularizer=l2(0.0001))(x)\n",
    "xtmp = layer(num, 3, 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "\n",
    "x = add(x,xtmp)\n",
    "\n",
    "#other layer\n",
    "xtmp = layer(num, (1,1), 1, x)\n",
    "xtmp = layer(num, 3, 1, xtmp)\n",
    "xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "x = add(x,xtmp)\n",
    "\n",
    "for i in xrange(4):\n",
    "    num *= 2\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, 3, 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    #other layer\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, 3, 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "xshape = backend.int_shape(x)\n",
    "x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(len(POSSIBLE_LABELS), activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "# x_in = Input(shape = process_wav_file(wav_read(train_df[0][2])).shape)\n",
    "# x = BatchNormalization()(x_in)\n",
    "# for i in range(4):\n",
    "#     x = Conv2D(16*(2 ** i),(3,3),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "# #     x = Conv2D(16*(2 ** i), (3,3))(x)\n",
    "#     x = Activation('elu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling2D((2,2))(x)\n",
    "# x = Conv2D(128, (1,1))(x)\n",
    "# x_branch_1 = GlobalAveragePooling2D()(x)\n",
    "# x_branch_2 = GlobalMaxPool2D()(x)\n",
    "# x = concatenate([x_branch_1, x_branch_2])\n",
    "# x = Dense(256, activation = 'relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(len(POSSIBLE_LABELS), activation = 'softmax')(x)\n",
    "# model = Model(inputs = x_in, outputs = x)\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
