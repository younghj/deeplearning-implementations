{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.196238Z",
     "start_time": "2017-11-17T09:03:28.644004Z"
    },
    "_cell_guid": "679e0d3e-646d-4e96-9eb0-b362d8c6e51f",
    "_uuid": "0d05e5ce89af3e25d1c1fb244d021a1cfa1a058c",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import cv2\n",
    "\n",
    "from scipy import signal\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import Input, Dense, Conv2D, AveragePooling2D, Activation, GlobalAveragePooling2D, Lambda\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import Initializer\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imsave, imresize\n",
    "import pydot\n",
    "import graphviz\n",
    "from keras.utils import plot_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.210749Z",
     "start_time": "2017-11-17T09:03:29.19832Z"
    },
    "_cell_guid": "8ab00801-08b9-44d3-a063-32e82dbf8f58",
    "_uuid": "53c19941676690454dd4b91109976b6c59cb7a40",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:29.325023Z",
     "start_time": "2017-11-17T09:03:29.215137Z"
    },
    "_cell_guid": "8d7ebf53-700b-4c06-b5c2-ccf9ed5f27e0",
    "_uuid": "133424c750b26df37900f9cebcfd2f2fb803cb8b",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57929 train, 6798 val, and 6 bg noise samples\n"
     ]
    }
   ],
   "source": [
    "DATADIR = './data' # unzipped train and test data\n",
    "OUTDIR = './model-k' # just a random name\n",
    "\n",
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\" Return 2 lists of tuples:\n",
    "    [(class_id, user_id, path), ...] for train\n",
    "    [(class_id, user_id, path), ...] for validation\n",
    "    \"\"\"\n",
    "    # Just a simple regexp for paths with three groups:\n",
    "    # prefix, label, user_id\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "\n",
    "    with open(os.path.join(data_dir, 'train/validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "\n",
    "    possible = set(POSSIBLE_LABELS)\n",
    "    train, val, bg_noise = [], [], []\n",
    "    for entry in all_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_background_noise_':\n",
    "                bg_noise.append(entry)\n",
    "                label = 'silence'\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "\n",
    "            label_id = name2id[label]\n",
    "\n",
    "            sample = (label_id, uid, entry)\n",
    "            if uid in valset:\n",
    "                val.append(sample)\n",
    "            else:\n",
    "                train.append(sample)\n",
    "\n",
    "    print('There are {} train, {} val, and {} bg noise samples'.format(len(train), len(val), len(bg_noise)))\n",
    "    return train, val, bg_noise\n",
    "\n",
    "train_df, valid_df, noise_df = load_data(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:31.144688Z",
     "start_time": "2017-11-17T09:03:31.105987Z"
    },
    "_cell_guid": "9e32f039-712e-4f1d-9173-ed9804d7771f",
    "_uuid": "36562e906f4fd6739b55582239ab55d947a80766",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_wav_file(origWav):\n",
    "    wav = np.copy(origWav)\n",
    "#     wav = read_wav_file(fname)\n",
    "    \n",
    "    L = 16000  # 1 sec\n",
    "    \n",
    "    if len(wav) > L:\n",
    "        i = np.random.randint(0, len(wav) - L)\n",
    "        wav = wav[i:(i+L)]\n",
    "    elif len(wav) < L:\n",
    "        pad_len = L - len(wav)\n",
    "        silence_part_left  = np.random.uniform(-0.001,0.001,int(pad_len/2.))\n",
    "        silence_part_right = np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2.)))\n",
    "        wav = np.concatenate([silence_part_left, wav, silence_part_right])\n",
    "        \n",
    "    wav = signal.resample(wav, int(0.5 * wav.shape[0]))\n",
    "    \n",
    "    specgram = signal.stft(wav, 16000, nperseg = 400, noverlap = 240, nfft = 512, padded = False, boundary = None)\n",
    "    \n",
    "    phase = np.angle(specgram[2]) / np.pi\n",
    "    amp = np.log1p(np.abs(specgram[2]))\n",
    "    \n",
    "#     shape = (96,32)\n",
    "#     phase = imresize(phase, shape, mode='F')\n",
    "#     amp = imresize(amp, shape, mode='F')\n",
    "    \n",
    "    stacked = np.stack([phase, amp], axis = 2)\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wav = wav_read(train_df[np.random.randint(len(train_df))][2])\n",
    "p = process_wav_file(wav)\n",
    "a,b = cv2.split(p)\n",
    "shape = (96,32)\n",
    "a = a.astype(np.float32)\n",
    "b = b.astype(np.float32)\n",
    "print b.dtype\n",
    "\n",
    "log_spect = np.log(a)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "a = imresize(a, shape, mode='F')\n",
    "\n",
    "log_spect = np.log(a)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "log_spect = np.log(b)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()\n",
    "\n",
    "b = imresize(b, shape, mode='F')\n",
    "\n",
    "log_spect = np.log(b)\n",
    "print('spectrogram shape:', log_spect.shape)\n",
    "plt.imshow(log_spect, aspect='auto', origin='lower',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wav_read(fname):\n",
    "    wav, _ = librosa.load(fname, sr=None)\n",
    "    return wav\n",
    "\n",
    "def normalize_audio(wav):\n",
    "    return wav/max(wav)\n",
    "\n",
    "def time_shift(wav, shift):\n",
    "    start_ = int(shift)\n",
    "    if start_ >= 0:\n",
    "        wav_time_shift = np.r_[wav[start_:], np.random.uniform(-0.001,0.001, start_)]\n",
    "    else:\n",
    "        wav_time_shift = np.r_[np.random.uniform(-0.001,0.001, -start_), wav[:start_]]\n",
    "    return normalize_audio(wav_time_shift)\n",
    "\n",
    "def speed_change(wav, speed_rate):\n",
    "    # rate: lower is faster\n",
    "    wav_speed_tune = cv2.resize(wav, (1, int(len(wav) * speed_rate))).squeeze()\n",
    "    if len(wav_speed_tune) < 16000:\n",
    "        pad_len = 16000 - len(wav_speed_tune)\n",
    "        wav_speed_tune = np.r_[np.random.uniform(-0.001,0.001,int(pad_len/2.)),\n",
    "                               wav_speed_tune,\n",
    "                               np.random.uniform(-0.001,0.001,int(np.ceil(pad_len/2.)))]\n",
    "    else: \n",
    "        cut_len = len(wav_speed_tune) - 16000\n",
    "        wav_speed_tune = wav_speed_tune[int(cut_len/2.):int(cut_len/2.)+16000]\n",
    "    return normalize_audio(wav_speed_tune)\n",
    "\n",
    "def noise_add(wav, percent, ind):\n",
    "    bg = wav_read(noise_df[ind])\n",
    "    bg = normalize_audio(bg)\n",
    "    start_ = np.random.randint(bg.shape[0]-16000)\n",
    "    bg_slice = bg[start_ : start_+16000]\n",
    "    wav_with_bg = wav * percent + bg_slice * (1-percent)\n",
    "    return normalize_audio(wav_with_bg)\n",
    "\n",
    "def get_spectrogram(wav):\n",
    "    v = 600\n",
    "    D = librosa.stft(wav, n_fft=v, hop_length=50,\n",
    "                     win_length=v, window='hamming')\n",
    "    spect, phase = librosa.magphase(D)\n",
    "    spect = scipy.ndimage.zoom(spect,1./7, order=1)\n",
    "    spect = spect.reshape(np.expand_dims(spect, axis=2).shape)\n",
    "    return spect\n",
    "\n",
    "def frange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump\n",
    "        \n",
    "def all_aug(wav, params):\n",
    "    time, speed, noise_percent, noise_ind = params\n",
    "    aug = wav\n",
    "    aug = time_shift(aug, time)\n",
    "    aug = speed_change(wav, speed)\n",
    "    if noise_ind == -1:\n",
    "        aug = noise_add(aug, 1, noise_ind)\n",
    "    else:\n",
    "        aug = noise_add(aug, noise_percent, noise_ind)\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:33.180074Z",
     "start_time": "2017-11-17T09:03:32.625939Z"
    },
    "_cell_guid": "0e13c01e-5662-4679-9b31-bf9347080ae5",
    "_uuid": "a17b3ea5c15c781260f3473f37dd0d36a932a565",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 257, 48, 2)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 257, 48, 16)  48          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 257, 48, 16)  64          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 257, 48, 16)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 257, 48, 16)  2320        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 257, 48, 16)  0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 257, 48, 16)  64          dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 257, 48, 16)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 257, 48, 32)  0           conv2d_57[0][0]                  \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 257, 48, 32)  128         concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 257, 48, 32)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 257, 48, 16)  4624        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 257, 48, 16)  0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 257, 48, 16)  64          dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 257, 48, 16)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 257, 48, 48)  0           concatenate_37[0][0]             \n",
      "                                                                 activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 257, 48, 48)  192         concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 257, 48, 48)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 257, 48, 24)  1176        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 257, 48, 24)  0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 128, 24, 24)  0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 128, 24, 24)  96          average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 128, 24, 24)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 128, 24, 16)  3472        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 128, 24, 16)  0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 128, 24, 16)  64          dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 128, 24, 16)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 128, 24, 40)  0           average_pooling2d_13[0][0]       \n",
      "                                                                 activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 128, 24, 40)  160         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 128, 24, 40)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 128, 24, 16)  5776        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 128, 24, 16)  0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 128, 24, 16)  64          dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 128, 24, 16)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 128, 24, 56)  0           concatenate_39[0][0]             \n",
      "                                                                 activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 128, 24, 56)  224         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 128, 24, 56)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 128, 24, 28)  1596        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 128, 24, 28)  0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 64, 12, 28)   0           dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 64, 12, 28)   112         average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 64, 12, 28)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 12, 16)   4048        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 64, 12, 16)   0           conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 64, 12, 16)   64          dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 64, 12, 16)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 64, 12, 44)   0           average_pooling2d_14[0][0]       \n",
      "                                                                 activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 64, 12, 44)   176         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 64, 12, 44)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 64, 12, 16)   6352        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 64, 12, 16)   0           conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 64, 12, 16)   64          dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 64, 12, 16)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 64, 12, 60)   0           concatenate_41[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 64, 12, 60)   240         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 64, 12, 60)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 60)           0           activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           1952        global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, 32)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 12)           396         dropout_58[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 33,536\n",
      "Trainable params: 32,648\n",
      "Non-trainable params: 888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define model parameters\n",
    "model_depth = 16\n",
    "num_dense_blocks = 3\n",
    "growth_rate = 12\n",
    "number_filters = 16\n",
    "compression = 0.5\n",
    "num_layers_per_block = (model_depth - 4) // num_dense_blocks\n",
    "\n",
    "def dense_block(x,num_layers_per_block,growth_rate):\n",
    "    for i in range(num_layers_per_block//2):\n",
    "        x_ = BatchNormalization()(x)\n",
    "        x_ = Activation('relu')(x_)\n",
    "        x_ = Conv2D(number_filters,(3,3),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x_)\n",
    "        x_ = Dropout(0.2)(x_)\n",
    "        x_ = BatchNormalization()(x_)\n",
    "        x_ = Activation('relu')(x_)\n",
    "        x = Concatenate()([x,x_])\n",
    "    return x \n",
    "\n",
    "def transition_layers(x,compression):\n",
    "    updated_num_filters = int(x.get_shape().as_list()[-1] * compression)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(updated_num_filters,(1,1),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = AveragePooling2D()(x)\n",
    "    return x\n",
    "\n",
    "#Let's define the model\n",
    "inp = Input(shape = process_wav_file(wav_read(train_df[0][2])).shape)\n",
    "x = Conv2D(number_filters,(1,1),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(inp)\n",
    "for i in range(num_dense_blocks):\n",
    "    x = dense_block(x,num_layers_per_block,growth_rate)\n",
    "    if (i != num_dense_blocks-1):\n",
    "        x = transition_layers(x,compression)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(32, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(len(POSSIBLE_LABELS), activation='softmax',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model = Model(inp, x)\n",
    "# model.compile(Adam(), loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "# x_in = Input(shape = process_wav_file(wav_read(train_df[0][2])).shape)\n",
    "# x = BatchNormalization()(x_in)\n",
    "# for i in range(4):\n",
    "#     x = Conv2D(16*(2 ** i),(3,3),padding='same',kernel_initializer='he_normal',kernel_regularizer=l2(0.0001))(x)\n",
    "# #     x = Conv2D(16*(2 ** i), (3,3))(x)\n",
    "#     x = Activation('elu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling2D((2,2))(x)\n",
    "# x = Conv2D(128, (1,1))(x)\n",
    "# x_branch_1 = GlobalAveragePooling2D()(x)\n",
    "# x_branch_2 = GlobalMaxPool2D()(x)\n",
    "# x = concatenate([x_branch_1, x_branch_2])\n",
    "# x = Dense(256, activation = 'relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(len(POSSIBLE_LABELS), activation = 'softmax')(x)\n",
    "# model = Model(inputs = x_in, outputs = x)\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:03:32.519795Z",
     "start_time": "2017-11-17T09:03:32.483881Z"
    },
    "_cell_guid": "144c6e60-8a83-437d-8b8a-ea065af90923",
    "_uuid": "22e0e6c718171167089fb6df36d3dc43a1029992",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIV = 100.0\n",
    "\n",
    "def get_variations():\n",
    "    variations = [[0,1,1,-1]]\n",
    "\n",
    "#     time_shift_range = 4000\n",
    "#     for time in xrange(-time_shift_range, time_shift_range+1, 1000):\n",
    "#         for speed in frange(0.4,1.7,0.3):\n",
    "#             for noise_percentage in frange(0.2,1,0.1):\n",
    "#                 for noise_ind in xrange(-1,len(noise_df)):\n",
    "#                     variations.append([time, speed, noise_percentage, noise_ind])\n",
    "    return variations\n",
    "\n",
    "def get_variations_valid():\n",
    "    variations = [[0,1,1,-1]]\n",
    "\n",
    "    time_shift_range = 4000\n",
    "    for time in xrange(-time_shift_range, time_shift_range+1, 1000):\n",
    "        for speed in frange(0.4,1.7,0.3):\n",
    "            for noise_percentage in frange(0.2,1,0.1):\n",
    "                for noise_ind in xrange(-1,len(noise_df)):\n",
    "                    variations.append([time, speed, noise_percentage, noise_ind])\n",
    "    return variations\n",
    "\n",
    "var = get_variations()\n",
    "multiplier = np.ceil(len(var)/DIV)\n",
    "\n",
    "def train_generator(train_batch_size):\n",
    "    while True:\n",
    "        variations = get_variations()\n",
    "        len_var = len(variations)\n",
    "        selected_indices = np.random.choice(len_var, int(np.ceil(len_var/DIV)))\n",
    "        \n",
    "        wavs = []\n",
    "        tmp_train_df = np.array(train_df)\n",
    "        np.random.shuffle(tmp_train_df)\n",
    "        tmp_train_df = tmp_train_df.tolist()\n",
    "        \n",
    "        while True:\n",
    "            while len(wavs) < train_batch_size:\n",
    "                label_id, uid, fname = tmp_train_df.pop(0)\n",
    "                wav = wav_read(fname)\n",
    "                for i in selected_indices:\n",
    "                    augmented = all_aug(wav, variations[i])\n",
    "                    arr = [label_id, augmented]\n",
    "                    wavs.append(arr)\n",
    "            \n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for x in xrange(train_batch_size):\n",
    "                label_id, wav = wavs.pop(0)\n",
    "                x_batch.append(process_wav_file(wav))\n",
    "                y_batch.append(label_id)\n",
    "            \n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes=len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "            if len(tmp_train_df) == 0:\n",
    "                break\n",
    "            \n",
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "\n",
    "        variations = get_variations_valid()\n",
    "        \n",
    "        len_var = len(variations)\n",
    "        selected_indices = np.random.choice(len_var, int(np.ceil(len_var/DIV)))\n",
    "        \n",
    "        wavs = []\n",
    "        tmp_valid_df = np.array(valid_df)\n",
    "        np.random.shuffle(tmp_valid_df)\n",
    "        tmp_valid_df = tmp_valid_df.tolist()\n",
    "        \n",
    "        while True:\n",
    "            while len(wavs) < valid_batch_size:\n",
    "                label_id, uid, fname = tmp_valid_df.pop(0)\n",
    "                wav = wav_read(fname)\n",
    "                for i in selected_indices:\n",
    "                    augmented = all_aug(wav, variations[i])\n",
    "                    arr = [label_id, augmented]\n",
    "                    wavs.append(arr)\n",
    "            \n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for x in xrange(valid_batch_size):\n",
    "                label_id, wav = wavs.pop(0)\n",
    "                x_batch.append(process_wav_file(wav))\n",
    "                y_batch.append(label_id)\n",
    "            \n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = to_categorical(y_batch, num_classes=len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "            if len(tmp_valid_df) == 0:\n",
    "                break\n",
    "            \n",
    "def test_generator(test_batch_size):\n",
    "    while True:\n",
    "        for start in range(0, len(test_paths), test_batch_size):\n",
    "            x_batch = []\n",
    "            end = min(start + test_batch_size, len(test_paths))\n",
    "            this_paths = test_paths[start:end]\n",
    "            for x in this_paths:\n",
    "                x_batch.append(process_wav_file(x))\n",
    "            x_batch = np.array(x_batch)\n",
    "            yield x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T09:42:31.48233Z",
     "start_time": "2017-11-17T09:03:33.355603Z"
    },
    "_cell_guid": "5f3d1b09-500f-410e-820a-8eaab24b6ebb",
    "_uuid": "528ec66a0a6caca952273ab916e609625839b19e",
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ee65411369461cb2c3266acee00c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4ea15cd53b4c8ca9725c43ec89a71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c53b4e6c358b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                               validation_steps=int(np.ceil(len(valid_df)*multiplier/batch_size)))\n\u001b[0m",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           verbose=1,\n",
    "                           min_delta=0.01,\n",
    "                           mode='min'),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=0.01,\n",
    "                               mode='min'),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='starter.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='min',\n",
    "                            verbose=1),\n",
    "             TQDMNotebookCallback()]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(generator=train_generator(batch_size),\n",
    "                              steps_per_epoch=int(np.ceil(len(train_df)*multiplier/batch_size)),\n",
    "                              epochs=120,\n",
    "                              verbose=2,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=valid_generator(batch_size),\n",
    "                              validation_steps=int(np.ceil(len(valid_df)*multiplier/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:24:59.198625Z",
     "start_time": "2017-11-17T10:24:59.081762Z"
    },
    "_cell_guid": "0c99ba3b-e8ca-40cb-8d29-2b0e89a385c7",
    "_uuid": "429139ca4f71487c6cfe3e8dfbb6a659eb9bb9c8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./weights/starter.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:28:14.451612Z",
     "start_time": "2017-11-17T10:28:13.307142Z"
    },
    "_cell_guid": "72f27090-c0d1-4d0b-8027-34c915429a79",
    "_uuid": "1007977fccadecdae582ec5d8d52dd3c4c3010aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_paths = glob(os.path.join('./data/', 'test/audio/*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T10:32:14.882322Z",
     "start_time": "2017-11-17T10:32:14.863617Z"
    },
    "_cell_guid": "c6d9b369-9979-4bcd-8540-4653e6544f84",
    "_uuid": "6a0bb3c22b7b5c43db0ec5673333ab3de8f08724",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-17T10:32:45.947Z"
    },
    "_cell_guid": "1fb8aed4-de12-43c5-84bf-b803e3d640fa",
    "_uuid": "631a38cb0013e5772f6987854145ad76ecf6c430",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator(64), int(np.ceil(len(test_paths)/64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:30:44.236246Z",
     "start_time": "2017-11-17T11:30:44.21858Z"
    },
    "_cell_guid": "b1cdab5c-9816-4690-87d8-de2c97cf0e7d",
    "_uuid": "24eb7e512eace4567494e0a8e356a826f4283c4d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:31:11.212517Z",
     "start_time": "2017-11-17T11:31:10.786357Z"
    },
    "_cell_guid": "1da523cf-fdbf-4ab1-9300-0147155aa247",
    "_uuid": "f25d4e626202aa115bd0460f4de8d07f9727c83e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last batch will contain padding, so remove duplicates\n",
    "submission = dict()\n",
    "for i in range(len(test_paths)):\n",
    "    fname, label = os.path.basename(test_paths[i]), id2name[classes[i]]\n",
    "    submission[fname] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-17T11:32:05.154527Z",
     "start_time": "2017-11-17T11:32:04.983371Z"
    },
    "_cell_guid": "9a95d147-3f4b-4386-8597-5fa60be43542",
    "_uuid": "bdf63bce43a0525a02ac18ca3f90aeba06ce6e99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('starter_submission.csv', 'w') as fout:\n",
    "    fout.write('fname,label\\n')\n",
    "    for fname, label in submission.items():\n",
    "        fout.write('{},{}\\n'.format(fname, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bea6850-15c6-44e7-bdb4-9555ad196f85",
    "_uuid": "555315ef622793711ff5643928dac874c8cb0ed2",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
