{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, MaxPooling2D, Conv2D, AveragePooling2D, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = xx_train[:40000]\n",
    "y_train = yy_train[:40000]\n",
    "x_valid = xx_train[40000:50000]\n",
    "y_valid = yy_train[40000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dim = 150\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory('dogscats/train',\n",
    "                                       target_size=(dim,dim),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='binary')\n",
    "\n",
    "valid_gen = test_datagen.flow_from_directory('dogscats/valid',\n",
    "                                       target_size=(dim,dim),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Input((dim, dim, 3))\n",
    "pool = MaxPooling2D(pool_size=(3,3), strides=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_block(conv_val, x, repeat_num):\n",
    "#     input_layer = pool(input_layer)\n",
    "#     conv = Conv2D(conv_val, (3,3), padding='same')\n",
    "#     input_layer_tmp = conv(input_layer)\n",
    "#     for i in xrange(repeat_num-1):\n",
    "#         input_layer_tmp = conv(input_layer_tmp)\n",
    "#     input_layer = Add()([input_layer,input_layer_tmp])\n",
    "#     return Activation('relu')(input_layer)\n",
    "    x = pool(x)\n",
    "    conv = Conv2D(conv_val, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "    for i in xrange(repeat_num):\n",
    "        xtmp = BatchNormalization()(x)\n",
    "        xtmp = Activation('relu')(xtmp)\n",
    "        xtmp = conv(xtmp)\n",
    "    x = Concatenate()([x,xtmp])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Concatenate, Add, Flatten, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "x = Conv2D(64, (7,7), strides=2, padding='same', kernel_initializer='he_normal')(l)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = resnet_block(64, x, 2)\n",
    "x = resnet_block(128, x, 2)\n",
    "x = resnet_block(256, x, 2)\n",
    "x = resnet_block(512, x, 2)\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(128, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(256, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(512, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = Flatten()(x)\n",
    "\n",
    "# x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1, activation='sigmoid')(x)\n",
    "# x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dense(1, activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('weights.best.hdf5')\n",
    "\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(l,x)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(x_train,y_train,\n",
    "          batch_size=2000, epochs=50,\n",
    "          shuffle=True,\n",
    "         validation_data=(x_valid, y_valid), callbacks=[early])\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1978 - acc: 0.9228Epoch 00001: val_acc improved from -inf to 0.89375, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 98s 2s/step - loss: 0.2002 - acc: 0.9215 - val_loss: 0.2398 - val_acc: 0.8938\n",
      "Epoch 2/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1979 - acc: 0.9185Epoch 00002: val_acc improved from 0.89375 to 0.90750, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.1967 - acc: 0.9189 - val_loss: 0.2192 - val_acc: 0.9075\n",
      "Epoch 3/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2101 - acc: 0.9191Epoch 00003: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.2109 - acc: 0.9194 - val_loss: 0.4082 - val_acc: 0.8275\n",
      "Epoch 4/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2247 - acc: 0.9011Epoch 00004: val_acc did not improve\n",
      "62/62 [==============================] - 89s 1s/step - loss: 0.2303 - acc: 0.8997 - val_loss: 0.3553 - val_acc: 0.8550\n",
      "Epoch 5/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2179 - acc: 0.9170Epoch 00005: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.2173 - acc: 0.9173 - val_loss: 0.2987 - val_acc: 0.8738\n",
      "Epoch 6/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2279 - acc: 0.9011Epoch 00006: val_acc did not improve\n",
      "62/62 [==============================] - 89s 1s/step - loss: 0.2271 - acc: 0.9012 - val_loss: 0.2131 - val_acc: 0.9025\n",
      "Epoch 7/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2205 - acc: 0.8981Epoch 00007: val_acc improved from 0.90750 to 0.90875, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 91s 1s/step - loss: 0.2220 - acc: 0.8967 - val_loss: 0.2247 - val_acc: 0.9087\n",
      "Epoch 8/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2080 - acc: 0.9119Epoch 00008: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.2088 - acc: 0.9118 - val_loss: 0.4828 - val_acc: 0.7837\n",
      "Epoch 9/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1984 - acc: 0.9155Epoch 00009: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.1987 - acc: 0.9153 - val_loss: 0.2390 - val_acc: 0.8925\n",
      "Epoch 10/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2088 - acc: 0.9103Epoch 00010: val_acc did not improve\n",
      "62/62 [==============================] - 93s 1s/step - loss: 0.2082 - acc: 0.9113 - val_loss: 0.2558 - val_acc: 0.8838\n",
      "Epoch 11/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2014 - acc: 0.9139Epoch 00011: val_acc did not improve\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.2042 - acc: 0.9123 - val_loss: 0.2588 - val_acc: 0.8888\n",
      "Epoch 12/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1912 - acc: 0.9257Epoch 00012: val_acc did not improve\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.1924 - acc: 0.9244 - val_loss: 0.2369 - val_acc: 0.9050\n",
      "Epoch 13/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1948 - acc: 0.9201Epoch 00013: val_acc did not improve\n",
      "62/62 [==============================] - 91s 1s/step - loss: 0.1939 - acc: 0.9209 - val_loss: 0.4043 - val_acc: 0.8413\n",
      "Epoch 14/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1736 - acc: 0.9273Epoch 00014: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.1749 - acc: 0.9269 - val_loss: 0.2065 - val_acc: 0.9087\n",
      "Epoch 15/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1823 - acc: 0.9267Epoch 00015: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.1818 - acc: 0.9269 - val_loss: 0.2598 - val_acc: 0.8888\n",
      "Epoch 16/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2104 - acc: 0.9129Epoch 00016: val_acc did not improve\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.2088 - acc: 0.9133 - val_loss: 0.2449 - val_acc: 0.9050\n",
      "Epoch 17/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1927 - acc: 0.9160Epoch 00017: val_acc did not improve\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.1909 - acc: 0.9168 - val_loss: 0.2241 - val_acc: 0.8975\n",
      "Epoch 18/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1893 - acc: 0.9203Epoch 00018: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.1882 - acc: 0.9205 - val_loss: 0.3424 - val_acc: 0.8512\n",
      "Epoch 19/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2097 - acc: 0.9180Epoch 00019: val_acc did not improve\n",
      "62/62 [==============================] - 92s 1s/step - loss: 0.2095 - acc: 0.9183 - val_loss: 0.2181 - val_acc: 0.8988\n",
      "Epoch 20/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2140 - acc: 0.9088Epoch 00020: val_acc did not improve\n",
      "62/62 [==============================] - 91s 1s/step - loss: 0.2163 - acc: 0.9078 - val_loss: 0.2152 - val_acc: 0.9038\n",
      "Epoch 21/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2147 - acc: 0.9093Epoch 00021: val_acc did not improve\n",
      "62/62 [==============================] - 91s 1s/step - loss: 0.2155 - acc: 0.9088 - val_loss: 0.2061 - val_acc: 0.9075\n",
      "Epoch 22/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.1885 - acc: 0.9175Epoch 00022: val_acc did not improve\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.1893 - acc: 0.9168 - val_loss: 0.2230 - val_acc: 0.9075\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a7489b990>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=150,\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=800 // batch_size,\n",
    "        callbacks=[early,checkpoint],\n",
    "        shuffle=True,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
