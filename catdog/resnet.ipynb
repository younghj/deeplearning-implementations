{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '5054' (I am process '11015')\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN Mixed dnn version. The header is from one version, but we link with a different version (5103, 6021))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, MaxPooling2D, Conv2D, AveragePooling2D, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(xx_train, yy_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = xx_train[:40000]\n",
    "y_train = yy_train[:40000]\n",
    "x_valid = xx_train[40000:50000]\n",
    "y_valid = yy_train[40000:50000]\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_valid = x_valid/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_valid = np_utils.to_categorical(y_valid)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dim = 150\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory('dogscats/train',\n",
    "                                       target_size=(dim,dim),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='binary')\n",
    "\n",
    "valid_gen = test_datagen.flow_from_directory('dogscats/valid',\n",
    "                                       target_size=(dim,dim),\n",
    "                                       batch_size=batch_size,\n",
    "                                       class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = Input((dim, dim, 3))\n",
    "pool = MaxPooling2D(pool_size=(3,3), strides=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_block(conv_val, x, repeat_num):\n",
    "#     input_layer = pool(input_layer)\n",
    "#     conv = Conv2D(conv_val, (3,3), padding='same')\n",
    "#     input_layer_tmp = conv(input_layer)\n",
    "#     for i in xrange(repeat_num-1):\n",
    "#         input_layer_tmp = conv(input_layer_tmp)\n",
    "#     input_layer = Add()([input_layer,input_layer_tmp])\n",
    "#     return Activation('relu')(input_layer)\n",
    "    x = pool(x)\n",
    "    conv = Conv2D(conv_val, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "    for i in xrange(repeat_num):\n",
    "        xtmp = BatchNormalization()(x)\n",
    "        xtmp = Activation('relu')(xtmp)\n",
    "        xtmp = conv(xtmp)\n",
    "    x = Concatenate()([x,xtmp])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Concatenate, Add, Flatten, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Conv2D(64, (7,7), strides=2, padding='same', kernel_initializer='he_normal')(l)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = resnet_block(64, x, 2)\n",
    "x = resnet_block(128, x, 2)\n",
    "x = resnet_block(256, x, 2)\n",
    "x = resnet_block(512, x, 2)\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(128, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(256, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = pool(x)\n",
    "# conv = Conv2D(512, (3,3), strides=1, padding='same', kernel_initializer='he_normal')\n",
    "# xtmp = BatchNormalization()(x)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# xtmp = BatchNormalization()(xtmp)\n",
    "# xtmp = Activation('relu')(xtmp)\n",
    "# xtmp = conv(xtmp)\n",
    "# x = Concatenate()([x,xtmp])\n",
    "\n",
    "# x = Flatten()(x)\n",
    "\n",
    "# x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1, activation='sigmoid')(x)\n",
    "# x = Dense(10, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)\n",
    "x = Dense(1, activation='sigmoid', kernel_initializer='he_normal', kernel_regularizer=l2(0.0001))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 150, 150, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 75, 75, 64)    9472        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 75, 75, 64)    256         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 75, 75, 64)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   multiple              0           activation_12[0][0]              \n",
      "                                                                   concatenate_5[0][0]              \n",
      "                                                                   concatenate_6[0][0]              \n",
      "                                                                   concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 37, 37, 64)    256         max_pooling2d_1[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 37, 37, 64)    0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 37, 37, 64)    36928       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 37, 37, 128)   0           max_pooling2d_1[5][0]            \n",
      "                                                                   conv2d_10[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 18, 18, 128)   512         max_pooling2d_1[6][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 18, 18, 128)   0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 18, 18, 128)   147584      activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 18, 18, 256)   0           max_pooling2d_1[6][0]            \n",
      "                                                                   conv2d_11[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 8, 8, 256)     1024        max_pooling2d_1[7][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 8, 8, 256)     0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 8, 8, 256)     590080      activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 8, 8, 512)     0           max_pooling2d_1[7][0]            \n",
      "                                                                   conv2d_12[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 3, 3, 512)     2048        max_pooling2d_1[8][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 3, 3, 512)     0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 3, 3, 512)     2359808     activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 3, 3, 1024)    0           max_pooling2d_1[8][0]            \n",
      "                                                                   conv2d_13[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 3, 3, 1024)    4096        concatenate_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 3, 3, 1024)    0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glob (None, 1024)          0           activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             1025        global_average_pooling2d_2[0][0] \n",
      "====================================================================================================\n",
      "Total params: 3,153,089\n",
      "Trainable params: 3,148,993\n",
      "Non-trainable params: 4,096\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(l,x)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss='sparse_categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "early=EarlyStopping(monitor='val_acc', patience=15, verbose=1, mode='auto')\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(x_train,y_train,\n",
    "          batch_size=2000, epochs=50,\n",
    "          shuffle=True,\n",
    "         validation_data=(x_valid, y_valid), callbacks=[early])\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.5890 - acc: 0.6947Epoch 00000: val_acc improved from -inf to 0.49235, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 76s - loss: 0.5882 - acc: 0.6951 - val_loss: 1.0872 - val_acc: 0.4923\n",
      "Epoch 2/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.5474 - acc: 0.7182Epoch 00001: val_acc improved from 0.49235 to 0.64125, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.5492 - acc: 0.7172 - val_loss: 0.6358 - val_acc: 0.6412\n",
      "Epoch 3/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.5316 - acc: 0.7341Epoch 00002: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.5309 - acc: 0.7354 - val_loss: 1.5818 - val_acc: 0.4875\n",
      "Epoch 4/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4749 - acc: 0.7708Epoch 00003: val_acc improved from 0.64125 to 0.68500, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.4754 - acc: 0.7695 - val_loss: 0.5632 - val_acc: 0.6850\n",
      "Epoch 5/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4734 - acc: 0.7874Epoch 00004: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.4766 - acc: 0.7853 - val_loss: 1.1167 - val_acc: 0.5675\n",
      "Epoch 6/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4544 - acc: 0.7848Epoch 00005: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.4562 - acc: 0.7848 - val_loss: 0.7013 - val_acc: 0.6518\n",
      "Epoch 7/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4695 - acc: 0.7869Epoch 00006: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.4685 - acc: 0.7868 - val_loss: 1.3444 - val_acc: 0.5600\n",
      "Epoch 8/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4741 - acc: 0.7679Epoch 00007: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.4734 - acc: 0.7686 - val_loss: 0.6905 - val_acc: 0.6594\n",
      "Epoch 9/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4257 - acc: 0.8007Epoch 00008: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.4261 - acc: 0.7994 - val_loss: 1.2215 - val_acc: 0.5687\n",
      "Epoch 10/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4131 - acc: 0.8058Epoch 00009: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.4099 - acc: 0.8080 - val_loss: 0.7139 - val_acc: 0.6531\n",
      "Epoch 11/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4328 - acc: 0.7956Epoch 00010: val_acc improved from 0.68500 to 0.79500, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.4318 - acc: 0.7959 - val_loss: 0.4405 - val_acc: 0.7950\n",
      "Epoch 12/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4004 - acc: 0.8212Epoch 00011: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3970 - acc: 0.8231 - val_loss: 0.5051 - val_acc: 0.7450\n",
      "Epoch 13/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4187 - acc: 0.7956Epoch 00012: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.4157 - acc: 0.7979 - val_loss: 1.9414 - val_acc: 0.5300\n",
      "Epoch 14/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.4035 - acc: 0.8202Epoch 00013: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.4069 - acc: 0.8180 - val_loss: 1.1293 - val_acc: 0.6000\n",
      "Epoch 15/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3934 - acc: 0.8289Epoch 00014: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.3970 - acc: 0.8276 - val_loss: 0.7857 - val_acc: 0.7105\n",
      "Epoch 16/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3839 - acc: 0.8226Epoch 00015: val_acc improved from 0.79500 to 0.80000, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 76s - loss: 0.3815 - acc: 0.8239 - val_loss: 0.3977 - val_acc: 0.8000\n",
      "Epoch 17/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3776 - acc: 0.8299Epoch 00016: val_acc did not improve\n",
      "62/62 [==============================] - 76s - loss: 0.3781 - acc: 0.8286 - val_loss: 2.5237 - val_acc: 0.5255\n",
      "Epoch 18/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3966 - acc: 0.8263Epoch 00017: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3951 - acc: 0.8266 - val_loss: 1.9734 - val_acc: 0.5175\n",
      "Epoch 19/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3568 - acc: 0.8417Epoch 00018: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3561 - acc: 0.8422 - val_loss: 0.9493 - val_acc: 0.5548\n",
      "Epoch 20/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3344 - acc: 0.8540Epoch 00019: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3334 - acc: 0.8548 - val_loss: 2.8702 - val_acc: 0.4838\n",
      "Epoch 21/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3706 - acc: 0.8309Epoch 00020: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3703 - acc: 0.8306 - val_loss: 0.5222 - val_acc: 0.7525\n",
      "Epoch 22/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3569 - acc: 0.8325Epoch 00021: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3595 - acc: 0.8317 - val_loss: 0.6180 - val_acc: 0.6887\n",
      "Epoch 23/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3576 - acc: 0.8386Epoch 00022: val_acc improved from 0.80000 to 0.81250, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 76s - loss: 0.3559 - acc: 0.8397 - val_loss: 0.4157 - val_acc: 0.8125\n",
      "Epoch 24/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3532 - acc: 0.8427Epoch 00023: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.3531 - acc: 0.8432 - val_loss: 0.9020 - val_acc: 0.6684\n",
      "Epoch 25/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3487 - acc: 0.8473Epoch 00024: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3492 - acc: 0.8463 - val_loss: 0.5802 - val_acc: 0.6925\n",
      "Epoch 26/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3323 - acc: 0.8494Epoch 00025: val_acc did not improve\n",
      "62/62 [==============================] - 76s - loss: 0.3312 - acc: 0.8503 - val_loss: 0.4325 - val_acc: 0.7972\n",
      "Epoch 27/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3381 - acc: 0.8403Epoch 00026: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3400 - acc: 0.8384 - val_loss: 0.5661 - val_acc: 0.6975\n",
      "Epoch 28/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3405 - acc: 0.8525Epoch 00027: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3390 - acc: 0.8538 - val_loss: 0.5266 - val_acc: 0.7232\n",
      "Epoch 29/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3148 - acc: 0.8678Epoch 00028: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3164 - acc: 0.8664 - val_loss: 0.4953 - val_acc: 0.7638\n",
      "Epoch 30/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2882 - acc: 0.8740Epoch 00029: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2902 - acc: 0.8735 - val_loss: 0.4318 - val_acc: 0.7913\n",
      "Epoch 31/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3040 - acc: 0.8694Epoch 00030: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3011 - acc: 0.8715 - val_loss: 0.7778 - val_acc: 0.6850\n",
      "Epoch 32/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3134 - acc: 0.8586Epoch 00031: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3124 - acc: 0.8589 - val_loss: 0.4545 - val_acc: 0.8125\n",
      "Epoch 33/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3275 - acc: 0.8545Epoch 00032: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3268 - acc: 0.8553 - val_loss: 0.4661 - val_acc: 0.7934\n",
      "Epoch 34/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3106 - acc: 0.8601Epoch 00033: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3090 - acc: 0.8609 - val_loss: 0.6338 - val_acc: 0.7312\n",
      "Epoch 35/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2823 - acc: 0.8740Epoch 00034: val_acc improved from 0.81250 to 0.86097, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 74s - loss: 0.2842 - acc: 0.8725 - val_loss: 0.3077 - val_acc: 0.8610\n",
      "Epoch 36/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3244 - acc: 0.8540Epoch 00035: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.3240 - acc: 0.8538 - val_loss: 0.4838 - val_acc: 0.7600\n",
      "Epoch 37/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3146 - acc: 0.8560Epoch 00036: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.3185 - acc: 0.8543 - val_loss: 1.5822 - val_acc: 0.5383\n",
      "Epoch 38/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2898 - acc: 0.8668Epoch 00037: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2884 - acc: 0.8674 - val_loss: 0.4068 - val_acc: 0.8163\n",
      "Epoch 39/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3069 - acc: 0.8654Epoch 00038: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.3058 - acc: 0.8656 - val_loss: 0.3723 - val_acc: 0.8063\n",
      "Epoch 40/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2608 - acc: 0.8934Epoch 00039: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2599 - acc: 0.8936 - val_loss: 0.4831 - val_acc: 0.7825\n",
      "Epoch 41/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2513 - acc: 0.8929Epoch 00040: val_acc improved from 0.86097 to 0.86125, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.2500 - acc: 0.8931 - val_loss: 0.3083 - val_acc: 0.8612\n",
      "Epoch 42/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2954 - acc: 0.8714Epoch 00041: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2957 - acc: 0.8715 - val_loss: 0.3424 - val_acc: 0.8495\n",
      "Epoch 43/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2700 - acc: 0.8878Epoch 00042: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2710 - acc: 0.8876 - val_loss: 0.3485 - val_acc: 0.8488\n",
      "Epoch 44/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2839 - acc: 0.8817Epoch 00043: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2818 - acc: 0.8826 - val_loss: 0.4386 - val_acc: 0.7793\n",
      "Epoch 45/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2800 - acc: 0.8781Epoch 00044: val_acc did not improve\n",
      "62/62 [==============================] - 74s - loss: 0.2796 - acc: 0.8790 - val_loss: 0.3593 - val_acc: 0.8400\n",
      "Epoch 46/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2575 - acc: 0.8955Epoch 00045: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2567 - acc: 0.8957 - val_loss: 0.5556 - val_acc: 0.7398\n",
      "Epoch 47/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.3039 - acc: 0.8699Epoch 00046: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.3055 - acc: 0.8695 - val_loss: 0.5144 - val_acc: 0.7512\n",
      "Epoch 48/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2833 - acc: 0.8760Epoch 00047: val_acc did not improve\n",
      "62/62 [==============================] - 76s - loss: 0.2831 - acc: 0.8760 - val_loss: 0.5525 - val_acc: 0.7362\n",
      "Epoch 49/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2743 - acc: 0.8796Epoch 00048: val_acc did not improve\n",
      "62/62 [==============================] - 76s - loss: 0.2745 - acc: 0.8800 - val_loss: 0.3340 - val_acc: 0.8475\n",
      "Epoch 50/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2695 - acc: 0.8813Epoch 00049: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2711 - acc: 0.8807 - val_loss: 0.4039 - val_acc: 0.8475\n",
      "Epoch 51/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2511 - acc: 0.8904Epoch 00050: val_acc improved from 0.86125 to 0.86480, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.2492 - acc: 0.8911 - val_loss: 0.3091 - val_acc: 0.8648\n",
      "Epoch 52/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2522 - acc: 0.8924Epoch 00051: val_acc improved from 0.86480 to 0.89000, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.2523 - acc: 0.8916 - val_loss: 0.2547 - val_acc: 0.8900\n",
      "Epoch 53/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2646 - acc: 0.8868Epoch 00052: val_acc improved from 0.89000 to 0.89031, saving model to weights.best.hdf5\n",
      "62/62 [==============================] - 75s - loss: 0.2623 - acc: 0.8881 - val_loss: 0.2783 - val_acc: 0.8903\n",
      "Epoch 54/150\n",
      "61/62 [============================>.] - ETA: 1s - loss: 0.2729 - acc: 0.8888Epoch 00053: val_acc did not improve\n",
      "62/62 [==============================] - 75s - loss: 0.2719 - acc: 0.8886 - val_loss: 0.6939 - val_acc: 0.6438\n",
      "Epoch 55/150\n",
      "14/62 [=====>........................] - ETA: 51s - loss: 0.2623 - acc: 0.9018"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-58b6ca3d2e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         verbose=1)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=150,\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=800 // batch_size,\n",
    "        callbacks=[early,checkpoint],\n",
    "        shuffle=True,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
