First I implemented a modified Residual Network (ResNet) because I did not have the resources to train the photos on a regular ResNet (even the smallest size ResNet-18). Since ResNets were used for much smaller photos, the starting number of feature maps was reduced to 16 as opposed to the regular 64 that is used in the paper. And this allows for a 4 fold decrease in the number of parameters that needed to be learned. Also, the number of times the image was downsampled was also increased to 5 times (in total), in order to compensate for the reduction in feature maps.

As for preprocessing, I shuffled the image set each time I did a fresh train of the model in order to induce more randomness. Also, an image data generator was used to perform small transformations to the dataset in order to prevent high variance (overfitting) and for it to generalize better. Another form of regularization that was used was the l2 regularization set to a value of 0.001. I found that if the regularization constant was a magnitude lower, the model would show the symptoms of overfitting and so decided to stay at 0.001 since I did not have enough type to adjust for hyperparameters. K-Fold ensemble training was used for classification. The softmax activation function was used because the softmax outputs the percentage of likelihood for each category, which was the metric that wanted from each sub-model. The outputs from each sub-model was averaged then the highest value was chosen.

The model was optimized using mini-batch gradient descent with adam as the optimization algorithm. The batch size was chosen to be a manageable size of 8 due to a lack of resources, namely memory, and various assistive callback functions were used to make the training process more efficient. The EarlyStopping callback was to prevent overfitting, the ModelCheckpoint was used to be able to revert back to the best model when the EarlyStopping callback was activated, and the ReduceLROnPlateau was used to allow the model to reach the local minima as close as possible.
