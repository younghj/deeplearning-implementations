{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Model, clone_model\n",
    "# from keras.models import load_model\n",
    "from keras.layers import Dense, Input, Activation, Flatten, Add\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D, AveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras import backend\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test = np.load('X.npy'), np.load('Y.npy'), np.load('X_test.npy')\n",
    "X = np.expand_dims(X, axis=3).astype('float32') / 255.0\n",
    "X_test = np.expand_dims(X_test, axis=3).astype('float32') / 255.0\n",
    "y_cat = np_utils.to_categorical(y)\n",
    "\n",
    "#shuffling\n",
    "s = np.random.shuffle(np.arange(X.shape[0]))\n",
    "X = X[s]\n",
    "y_cat = y_cat[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/tommy/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 168, 308, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 84, 154, 128) 6400        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 84, 154, 128) 512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 84, 154, 128) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 84, 154, 16)  2064        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 84, 154, 16)  64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 84, 154, 16)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 84, 154, 16)  2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 84, 154, 16)  64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 84, 154, 16)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 84, 154, 64)  8256        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 84, 154, 64)  1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 84, 154, 64)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 84, 154, 64)  256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 84, 154, 64)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 84, 154, 16)  1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 84, 154, 16)  64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 84, 154, 16)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 84, 154, 16)  2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 84, 154, 16)  64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 84, 154, 16)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 84, 154, 64)  1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 84, 154, 64)  0           add_1[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 84, 154, 64)  256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 84, 154, 64)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 84, 154, 16)  1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 84, 154, 16)  64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 84, 154, 16)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 84, 154, 16)  2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 84, 154, 16)  64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 84, 154, 16)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 84, 154, 64)  1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 84, 154, 64)  0           add_2[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 84, 154, 64)  256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 84, 154, 64)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 84, 154, 16)  1040        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 84, 154, 16)  64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 84, 154, 16)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 84, 154, 16)  2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 84, 154, 16)  64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 84, 154, 16)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 84, 154, 64)  1088        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 84, 154, 64)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 84, 154, 64)  256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 84, 154, 64)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 42, 77, 16)   1040        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 42, 77, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 42, 77, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 42, 77, 32)   544         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 42, 77, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 42, 77, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 42, 77, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 42, 77, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 42, 77, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 42, 77, 128)  2176        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 42, 77, 128)  4224        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 42, 77, 128)  0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 42, 77, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 42, 77, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 42, 77, 32)   4128        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 42, 77, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 42, 77, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 42, 77, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 42, 77, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 42, 77, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 42, 77, 128)  4224        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 42, 77, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 42, 77, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 42, 77, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 21, 39, 32)   4128        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 21, 39, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 21, 39, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 21, 39, 64)   2112        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 21, 39, 64)   256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 21, 39, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 21, 39, 64)   36928       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 21, 39, 64)   256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 21, 39, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 21, 39, 256)  8448        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 21, 39, 256)  16640       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 21, 39, 256)  0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 21, 39, 256)  1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 21, 39, 256)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 21, 39, 64)   16448       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 21, 39, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 21, 39, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 21, 39, 64)   36928       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 21, 39, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 21, 39, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 21, 39, 256)  16640       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 21, 39, 256)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 21, 39, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 21, 39, 256)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 11, 20, 64)   16448       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 11, 20, 64)   256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 11, 20, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 11, 20, 128)  8320        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 11, 20, 128)  512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 11, 20, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 11, 20, 128)  147584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 11, 20, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 11, 20, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 11, 20, 512)  33280       conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 11, 20, 512)  66048       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 11, 20, 512)  0           conv2d_35[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 11, 20, 512)  2048        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 11, 20, 512)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 11, 20, 128)  65664       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 11, 20, 128)  512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 11, 20, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 11, 20, 128)  147584      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 11, 20, 128)  512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 11, 20, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 11, 20, 512)  66048       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 11, 20, 512)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 11, 20, 512)  2048        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 11, 20, 512)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 6, 10, 128)   65664       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 6, 10, 128)   512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 6, 10, 128)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 128)    0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           2580        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 839,556\n",
      "Trainable params: 832,676\n",
      "Non-trainable params: 6,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#modified resnet (could not run original resnet architecture on my computer)\n",
    "def resnet(img_dim, output):\n",
    "    def add(a,b):\n",
    "        shape1 = backend.int_shape(a)\n",
    "        shape2 = backend.int_shape(b)\n",
    "        w = int(round(shape1[1]/shape2[1]))\n",
    "        h = int(round(shape1[2]/shape2[2]))\n",
    "        eq = shape1[3] == shape2[3]\n",
    "\n",
    "        tmp = a\n",
    "        if w>1 or h>1 or not eq:\n",
    "            tmp = Conv2D(filters=shape2[3],kernel_size=(1,1),strides=(w,h),padding='valid',kernel_initializer='he_normal', kernel_regularizer=l2(regval))(a)\n",
    "        return Add()([tmp, b])\n",
    "\n",
    "    def layer(num_filt, size, strides, inp):\n",
    "        tmp = BatchNormalization(axis=3)(inp)\n",
    "        tmp = Activation('relu')(tmp)\n",
    "        tmp = Conv2D(num_filt, size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(regval))(tmp)\n",
    "        return tmp\n",
    "    \n",
    "    regval = 0.001\n",
    "    l = Input(img_dim)\n",
    "\n",
    "    x = Conv2D(128, (7,7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(regval))(l)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num = 16\n",
    "    xtmp = Conv2D(num, (1,1), strides=1, \n",
    "                  padding='same', \n",
    "                  kernel_initializer='he_normal', \n",
    "                  kernel_regularizer=l2(regval))(x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    xtmp = layer(num, (1,1), 1, x)\n",
    "    xtmp = layer(num, (3,3), 1, xtmp)\n",
    "    xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "    x = add(x,xtmp)\n",
    "\n",
    "    for i in xrange(4):\n",
    "        xtmp = layer(num, (1,1), 1, x)\n",
    "        xtmp = layer(num, (3,3), 1, xtmp)\n",
    "        xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "        x = add(x,xtmp)\n",
    "\n",
    "        xtmp = layer(num, (1,1), 1, x)\n",
    "        xtmp = layer(num, (3,3), 1, xtmp)\n",
    "        xtmp = layer(num*4, (1,1), 1, xtmp)\n",
    "        x = add(x,xtmp)\n",
    "        x = layer(num, (1,1), 2, x)\n",
    "\n",
    "        num*=2\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    xshape = backend.int_shape(x)\n",
    "    x = AveragePooling2D(pool_size=(xshape[1],xshape[2]), strides=(1,1))(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(output, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(regval))(x)\n",
    "    model = Model(l,x)\n",
    "    return model\n",
    "\n",
    "model = resnet(X.shape[1:], y_cat.shape[1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble\n",
    "def train_model(model, batch_size, epochs, x, y, test, kf, continue_training):\n",
    "    n_fold = kf.get_n_splits()\n",
    "    preds_test = np.zeros((len(test),y_cat.shape[-1]), dtype = np.float)\n",
    "    \n",
    "    models = []\n",
    "    results = []\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x[train_index];\n",
    "        x_valid = x[test_index]\n",
    "        y_train = y[train_index];\n",
    "        y_valid = y[test_index]\n",
    "\n",
    "        traingen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "        traingen.fit(x_train)\n",
    "\n",
    "        validgen = ImageDataGenerator()\n",
    "        validgen.fit(x_valid)\n",
    "        \n",
    "        filepath = 'mod_resnet.fold_{}.hdf5'.format(i)\n",
    "        early = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=1e-4, mode='auto')\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=1, patience=4, min_lr=1e-7)\n",
    "        \n",
    "        callbacks=[checkpoint, early, lr_reducer]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / batch_size\n",
    "\n",
    "        model_k = model\n",
    "        \n",
    "        if continue_training:\n",
    "            model_k.load_weights(filepath)\n",
    "\n",
    "        model_k.compile(optimizer='adam', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics = ['accuracy'])\n",
    "        \n",
    "        history = model_k.fit_generator(traingen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                      train_steps,\n",
    "                                      epochs,\n",
    "                                      verbose=1,\n",
    "                                      callbacks=callbacks,\n",
    "                                      validation_data=validgen.flow(x_valid, y_valid, batch_size=batch_size),\n",
    "                                      validation_steps=valid_steps,\n",
    "                                      shuffle=True)\n",
    "\n",
    "        model_k.load_weights(filepath=filepath)\n",
    "        \n",
    "        preds_test_fold = model_k.predict(test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "        \n",
    "        models.append(clone_model(model_k))\n",
    "        results.append(preds_test_fold)\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "\n",
    "    return preds_test, models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 2.0325 - acc: 0.7096Epoch 00001: val_loss improved from inf to 10.35000, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 210s 3s/step - loss: 2.0342 - acc: 0.7102 - val_loss: 10.3500 - val_acc: 0.1875\n",
      "Epoch 2/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.9155 - acc: 0.7731Epoch 00002: val_loss improved from 10.35000 to 8.99541, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 209s 3s/step - loss: 1.9185 - acc: 0.7708 - val_loss: 8.9954 - val_acc: 0.2812\n",
      "Epoch 3/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 1.9332 - acc: 0.7596Epoch 00003: val_loss improved from 8.99541 to 3.05732, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 252s 4s/step - loss: 1.9270 - acc: 0.7614 - val_loss: 3.0573 - val_acc: 0.5078\n",
      "Epoch 4/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 1.8741 - acc: 0.7750Epoch 00004: val_loss did not improve\n",
      "66/66 [==============================] - 233s 4s/step - loss: 1.8821 - acc: 0.7727 - val_loss: 3.6054 - val_acc: 0.5078\n",
      "Epoch 5/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 1.8246 - acc: 0.7865Epoch 00005: val_loss improved from 3.05732 to 2.09432, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 216s 3s/step - loss: 1.8238 - acc: 0.7879 - val_loss: 2.0943 - val_acc: 0.6797\n",
      "Epoch 6/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.8020 - acc: 0.7846Epoch 00006: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 1.7967 - acc: 0.7860 - val_loss: 2.7057 - val_acc: 0.6328\n",
      "Epoch 7/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.7619 - acc: 0.8154Epoch 00007: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.7568 - acc: 0.8163 - val_loss: 3.8081 - val_acc: 0.3750\n",
      "Epoch 8/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.7723 - acc: 0.7808Epoch 00008: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.7738 - acc: 0.7822 - val_loss: 4.4877 - val_acc: 0.3125\n",
      "Epoch 9/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.7321 - acc: 0.7942Epoch 00009: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.7306 - acc: 0.7936 - val_loss: 3.9822 - val_acc: 0.5703\n",
      "Epoch 10/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.6730 - acc: 0.7885Epoch 00010: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 1.6693 - acc: 0.7917 - val_loss: 3.8705 - val_acc: 0.3516\n",
      "Epoch 11/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.5004 - acc: 0.8558Epoch 00011: val_loss improved from 2.09432 to 1.70208, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.5014 - acc: 0.8561 - val_loss: 1.7021 - val_acc: 0.7031\n",
      "Epoch 12/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4934 - acc: 0.8731Epoch 00012: val_loss improved from 1.70208 to 1.56889, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.4880 - acc: 0.8750 - val_loss: 1.5689 - val_acc: 0.7500\n",
      "Epoch 13/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4131 - acc: 0.8788Epoch 00013: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.4113 - acc: 0.8807 - val_loss: 2.3022 - val_acc: 0.5938\n",
      "Epoch 14/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3608 - acc: 0.8885Epoch 00014: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.3638 - acc: 0.8883 - val_loss: 1.6002 - val_acc: 0.7734\n",
      "Epoch 15/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3644 - acc: 0.8769Epoch 00015: val_loss improved from 1.56889 to 1.33280, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.3716 - acc: 0.8750 - val_loss: 1.3328 - val_acc: 0.8594\n",
      "Epoch 16/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3536 - acc: 0.8904Epoch 00016: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.3489 - acc: 0.8920 - val_loss: 2.1512 - val_acc: 0.7031\n",
      "Epoch 17/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4038 - acc: 0.8731Epoch 00017: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.4037 - acc: 0.8750 - val_loss: 1.6022 - val_acc: 0.7188\n",
      "Epoch 18/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3226 - acc: 0.8865Epoch 00018: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3218 - acc: 0.8883 - val_loss: 1.3719 - val_acc: 0.8438\n",
      "Epoch 19/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3264 - acc: 0.8712Epoch 00019: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3262 - acc: 0.8712 - val_loss: 1.8123 - val_acc: 0.8203\n",
      "Epoch 20/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3023 - acc: 0.8846Epoch 00020: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.2996 - acc: 0.8864 - val_loss: 2.2520 - val_acc: 0.7266\n",
      "Epoch 21/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2742 - acc: 0.8942Epoch 00021: val_loss improved from 1.33280 to 1.15980, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.2767 - acc: 0.8920 - val_loss: 1.1598 - val_acc: 0.8906\n",
      "Epoch 22/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2441 - acc: 0.8962Epoch 00022: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.2426 - acc: 0.8977 - val_loss: 1.3196 - val_acc: 0.8516\n",
      "Epoch 23/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1628 - acc: 0.9385Epoch 00023: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1610 - acc: 0.9394 - val_loss: 1.1919 - val_acc: 0.8906\n",
      "Epoch 24/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1809 - acc: 0.9269Epoch 00024: val_loss improved from 1.15980 to 1.10648, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1797 - acc: 0.9280 - val_loss: 1.1065 - val_acc: 0.9297\n",
      "Epoch 25/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1679 - acc: 0.9462Epoch 00025: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1684 - acc: 0.9451 - val_loss: 1.2582 - val_acc: 0.8672\n",
      "Epoch 26/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1842 - acc: 0.9288Epoch 00026: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1851 - acc: 0.9280 - val_loss: 1.1523 - val_acc: 0.8906\n",
      "Epoch 27/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1425 - acc: 0.9365Epoch 00027: val_loss improved from 1.10648 to 1.04117, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1467 - acc: 0.9356 - val_loss: 1.0412 - val_acc: 0.9531\n",
      "Epoch 28/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2028 - acc: 0.9115Epoch 00028: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.2032 - acc: 0.9129 - val_loss: 1.1255 - val_acc: 0.9141\n",
      "Epoch 29/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1765 - acc: 0.9288Epoch 00029: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.1746 - acc: 0.9299 - val_loss: 1.2644 - val_acc: 0.8750\n",
      "Epoch 30/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1701 - acc: 0.9192Epoch 00030: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1683 - acc: 0.9205 - val_loss: 1.0801 - val_acc: 0.9219\n",
      "Epoch 31/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1542 - acc: 0.9346Epoch 00031: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1559 - acc: 0.9356 - val_loss: 1.0539 - val_acc: 0.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1262 - acc: 0.9365Epoch 00032: val_loss improved from 1.04117 to 1.00556, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1260 - acc: 0.9375 - val_loss: 1.0056 - val_acc: 0.9688\n",
      "Epoch 33/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1446 - acc: 0.9327Epoch 00033: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.1440 - acc: 0.9337 - val_loss: 1.0927 - val_acc: 0.9375\n",
      "Epoch 34/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1512 - acc: 0.9250Epoch 00034: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.1486 - acc: 0.9261 - val_loss: 1.0473 - val_acc: 0.9375\n",
      "Epoch 35/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1449 - acc: 0.9327Epoch 00035: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1423 - acc: 0.9337 - val_loss: 1.0683 - val_acc: 0.9297\n",
      "Epoch 36/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0956 - acc: 0.9481Epoch 00036: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1010 - acc: 0.9451 - val_loss: 1.0386 - val_acc: 0.9375\n",
      "Epoch 37/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0948 - acc: 0.9500Epoch 00037: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0945 - acc: 0.9508 - val_loss: 1.1112 - val_acc: 0.9062\n",
      "Epoch 38/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0714 - acc: 0.9577Epoch 00038: val_loss improved from 1.00556 to 0.99096, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0701 - acc: 0.9583 - val_loss: 0.9910 - val_acc: 0.9688\n",
      "Epoch 39/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0719 - acc: 0.9538Epoch 00039: val_loss improved from 0.99096 to 0.98464, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0702 - acc: 0.9545 - val_loss: 0.9846 - val_acc: 0.9609\n",
      "Epoch 40/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1024 - acc: 0.9269Epoch 00040: val_loss improved from 0.98464 to 0.95481, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.1000 - acc: 0.9280 - val_loss: 0.9548 - val_acc: 0.9766\n",
      "Epoch 41/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0891 - acc: 0.9308Epoch 00041: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0884 - acc: 0.9318 - val_loss: 0.9764 - val_acc: 0.9688\n",
      "Epoch 42/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0739 - acc: 0.9500Epoch 00042: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0721 - acc: 0.9508 - val_loss: 0.9738 - val_acc: 0.9688\n",
      "Epoch 43/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0882 - acc: 0.9462Epoch 00043: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0869 - acc: 0.9470 - val_loss: 0.9937 - val_acc: 0.9688\n",
      "Epoch 44/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0535 - acc: 0.9615Epoch 00044: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0535 - acc: 0.9621 - val_loss: 0.9808 - val_acc: 0.9688\n",
      "Epoch 45/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0325 - acc: 0.9654Epoch 00045: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0374 - acc: 0.9640 - val_loss: 0.9715 - val_acc: 0.9688\n",
      "Epoch 46/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0626 - acc: 0.9500Epoch 00046: val_loss improved from 0.95481 to 0.95388, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.0626 - acc: 0.9508 - val_loss: 0.9539 - val_acc: 0.9766\n",
      "Epoch 47/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0284 - acc: 0.9692Epoch 00047: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.0284 - acc: 0.9697 - val_loss: 0.9649 - val_acc: 0.9688\n",
      "Epoch 48/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0465 - acc: 0.9635Epoch 00048: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0453 - acc: 0.9640 - val_loss: 0.9668 - val_acc: 0.9688\n",
      "Epoch 49/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0987 - acc: 0.9404Epoch 00049: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0974 - acc: 0.9394 - val_loss: 0.9712 - val_acc: 0.9688\n",
      "Epoch 50/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0615 - acc: 0.9500Epoch 00050: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0694 - acc: 0.9470 - val_loss: 0.9661 - val_acc: 0.9688\n",
      "Epoch 51/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0544 - acc: 0.9423Epoch 00051: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0614 - acc: 0.9394 - val_loss: 0.9638 - val_acc: 0.9688\n",
      "Epoch 52/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0493 - acc: 0.9538Epoch 00052: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0486 - acc: 0.9545 - val_loss: 0.9629 - val_acc: 0.9688\n",
      "Epoch 53/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0442 - acc: 0.9635Epoch 00053: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0462 - acc: 0.9621 - val_loss: 0.9588 - val_acc: 0.9688\n",
      "Epoch 54/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0549 - acc: 0.9519Epoch 00054: val_loss improved from 0.95388 to 0.95010, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0537 - acc: 0.9527 - val_loss: 0.9501 - val_acc: 0.9766\n",
      "Epoch 55/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0397 - acc: 0.9519Epoch 00055: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0385 - acc: 0.9527 - val_loss: 0.9610 - val_acc: 0.9688\n",
      "Epoch 56/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0417 - acc: 0.9538Epoch 00056: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0418 - acc: 0.9545 - val_loss: 0.9580 - val_acc: 0.9688\n",
      "Epoch 57/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0483 - acc: 0.9654Epoch 00057: val_loss improved from 0.95010 to 0.94437, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0475 - acc: 0.9659 - val_loss: 0.9444 - val_acc: 0.9766\n",
      "Epoch 58/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0695 - acc: 0.9442Epoch 00058: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0721 - acc: 0.9432 - val_loss: 0.9590 - val_acc: 0.9688\n",
      "Epoch 59/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0542 - acc: 0.9442Epoch 00059: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0550 - acc: 0.9432 - val_loss: 0.9563 - val_acc: 0.9688\n",
      "Epoch 60/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0095 - acc: 0.9750Epoch 00060: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0080 - acc: 0.9754 - val_loss: 0.9582 - val_acc: 0.9688\n",
      "Epoch 61/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0269 - acc: 0.9654Epoch 00061: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0283 - acc: 0.9640 - val_loss: 0.9548 - val_acc: 0.9688\n",
      "Epoch 62/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0192 - acc: 0.9654Epoch 00062: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0204 - acc: 0.9640 - val_loss: 0.9509 - val_acc: 0.9688\n",
      "Epoch 63/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0726 - acc: 0.9500Epoch 00063: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0715 - acc: 0.9508 - val_loss: 0.9538 - val_acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0545 - acc: 0.9596Epoch 00064: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0531 - acc: 0.9602 - val_loss: 0.9537 - val_acc: 0.9688\n",
      "Epoch 65/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0454 - acc: 0.9500Epoch 00065: val_loss improved from 0.94437 to 0.93375, saving model to mod_resnet.fold_1.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0434 - acc: 0.9508 - val_loss: 0.9337 - val_acc: 0.9766\n",
      "Epoch 66/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0376 - acc: 0.9577Epoch 00066: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0353 - acc: 0.9583 - val_loss: 0.9541 - val_acc: 0.9688\n",
      "Epoch 67/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0188 - acc: 0.9673Epoch 00067: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0171 - acc: 0.9678 - val_loss: 0.9544 - val_acc: 0.9688\n",
      "Epoch 68/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0527 - acc: 0.9481Epoch 00068: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0527 - acc: 0.9470 - val_loss: 0.9500 - val_acc: 0.9688\n",
      "Epoch 69/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0541 - acc: 0.9596Epoch 00069: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0521 - acc: 0.9602 - val_loss: 0.9517 - val_acc: 0.9688\n",
      "Epoch 70/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0673 - acc: 0.9500Epoch 00070: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0685 - acc: 0.9489 - val_loss: 0.9393 - val_acc: 0.9766\n",
      "Epoch 71/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0379 - acc: 0.9519Epoch 00071: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0372 - acc: 0.9527 - val_loss: 0.9543 - val_acc: 0.9688\n",
      "Epoch 72/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0434 - acc: 0.9519Epoch 00072: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0476 - acc: 0.9508 - val_loss: 0.9586 - val_acc: 0.9688\n",
      "Epoch 73/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0595 - acc: 0.9538Epoch 00073: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0632 - acc: 0.9527 - val_loss: 0.9535 - val_acc: 0.9688\n",
      "Epoch 74/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0366 - acc: 0.9519Epoch 00074: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0376 - acc: 0.9527 - val_loss: 0.9556 - val_acc: 0.9688\n",
      "Epoch 75/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0344 - acc: 0.9615Epoch 00075: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.0349 - acc: 0.9602 - val_loss: 0.9491 - val_acc: 0.9688\n",
      "Epoch 00075: early stopping\n",
      "300/300 [==============================] - 30s 101ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 2\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.5687 - acc: 0.7538Epoch 00001: val_loss improved from inf to 10.09781, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 198s 3s/step - loss: 1.5594 - acc: 0.7576 - val_loss: 10.0978 - val_acc: 0.2266\n",
      "Epoch 2/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3213 - acc: 0.8423Epoch 00002: val_loss improved from 10.09781 to 8.26146, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3181 - acc: 0.8447 - val_loss: 8.2615 - val_acc: 0.3828\n",
      "Epoch 3/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3268 - acc: 0.8346Epoch 00003: val_loss improved from 8.26146 to 7.47521, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3202 - acc: 0.8371 - val_loss: 7.4752 - val_acc: 0.2812\n",
      "Epoch 4/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3762 - acc: 0.8173Epoch 00004: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3750 - acc: 0.8182 - val_loss: 12.5952 - val_acc: 0.1250\n",
      "Epoch 5/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3193 - acc: 0.8269Epoch 00005: val_loss improved from 7.47521 to 6.50982, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3198 - acc: 0.8258 - val_loss: 6.5098 - val_acc: 0.4531\n",
      "Epoch 6/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4238 - acc: 0.7865Epoch 00006: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.4194 - acc: 0.7860 - val_loss: 9.3066 - val_acc: 0.3281\n",
      "Epoch 7/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4636 - acc: 0.7654Epoch 00007: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.4694 - acc: 0.7652 - val_loss: 8.2813 - val_acc: 0.4219\n",
      "Epoch 8/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4620 - acc: 0.7827Epoch 00008: val_loss did not improve\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.4603 - acc: 0.7841 - val_loss: 6.5927 - val_acc: 0.3828\n",
      "Epoch 9/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3624 - acc: 0.8058Epoch 00009: val_loss improved from 6.50982 to 5.42047, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 194s 3s/step - loss: 1.3610 - acc: 0.8068 - val_loss: 5.4205 - val_acc: 0.3672\n",
      "Epoch 10/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3096 - acc: 0.8288Epoch 00010: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.3109 - acc: 0.8277 - val_loss: 8.2554 - val_acc: 0.2891\n",
      "Epoch 11/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2539 - acc: 0.8269Epoch 00011: val_loss improved from 5.42047 to 2.79780, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.2563 - acc: 0.8258 - val_loss: 2.7978 - val_acc: 0.5078\n",
      "Epoch 12/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4279 - acc: 0.7865Epoch 00012: val_loss improved from 2.79780 to 2.13864, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.4198 - acc: 0.7898 - val_loss: 2.1386 - val_acc: 0.6016\n",
      "Epoch 13/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2838 - acc: 0.8346Epoch 00013: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.2844 - acc: 0.8352 - val_loss: 7.2126 - val_acc: 0.3984\n",
      "Epoch 14/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2640 - acc: 0.8327Epoch 00014: val_loss improved from 2.13864 to 1.41593, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 197s 3s/step - loss: 1.2661 - acc: 0.8333 - val_loss: 1.4159 - val_acc: 0.7656\n",
      "Epoch 15/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2722 - acc: 0.8288Epoch 00015: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.2679 - acc: 0.8277 - val_loss: 5.9740 - val_acc: 0.4922\n",
      "Epoch 16/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3024 - acc: 0.8212Epoch 00016: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.3073 - acc: 0.8163 - val_loss: 8.5422 - val_acc: 0.3359\n",
      "Epoch 17/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2792 - acc: 0.8231Epoch 00017: val_loss improved from 1.41593 to 1.35282, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.2769 - acc: 0.8239 - val_loss: 1.3528 - val_acc: 0.8047\n",
      "Epoch 18/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1835 - acc: 0.8577Epoch 00018: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1927 - acc: 0.8542 - val_loss: 2.7437 - val_acc: 0.5391\n",
      "Epoch 19/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2255 - acc: 0.8462Epoch 00019: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.2212 - acc: 0.8485 - val_loss: 1.7351 - val_acc: 0.7109\n",
      "Epoch 20/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2017 - acc: 0.8462Epoch 00020: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.1987 - acc: 0.8466 - val_loss: 4.3969 - val_acc: 0.4766\n",
      "Epoch 21/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2431 - acc: 0.8231Epoch 00021: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 1.2393 - acc: 0.8239 - val_loss: 5.0202 - val_acc: 0.6484\n",
      "Epoch 22/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1864 - acc: 0.8365Epoch 00022: val_loss did not improve\n",
      "66/66 [==============================] - 197s 3s/step - loss: 1.1953 - acc: 0.8333 - val_loss: 2.1285 - val_acc: 0.5469\n",
      "Epoch 23/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0790 - acc: 0.8769Epoch 00023: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 1.0838 - acc: 0.8731 - val_loss: 1.4371 - val_acc: 0.7422\n",
      "Epoch 24/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9816 - acc: 0.9077Epoch 00024: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.9902 - acc: 0.9053 - val_loss: 5.5000 - val_acc: 0.4062\n",
      "Epoch 25/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9618 - acc: 0.9346Epoch 00025: val_loss improved from 1.35282 to 1.11785, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.9604 - acc: 0.9356 - val_loss: 1.1178 - val_acc: 0.8125\n",
      "Epoch 26/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9489 - acc: 0.9462Epoch 00026: val_loss did not improve\n",
      "66/66 [==============================] - 197s 3s/step - loss: 0.9496 - acc: 0.9451 - val_loss: 1.2323 - val_acc: 0.8047\n",
      "Epoch 27/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9524 - acc: 0.9288Epoch 00027: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.9498 - acc: 0.9299 - val_loss: 1.3581 - val_acc: 0.8203\n",
      "Epoch 28/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9422 - acc: 0.9077Epoch 00028: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.9404 - acc: 0.9072 - val_loss: 1.2790 - val_acc: 0.8047\n",
      "Epoch 29/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8885 - acc: 0.9423Epoch 00029: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8891 - acc: 0.9432 - val_loss: 1.3324 - val_acc: 0.7422\n",
      "Epoch 30/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9060 - acc: 0.9308Epoch 00030: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.9048 - acc: 0.9318 - val_loss: 1.1605 - val_acc: 0.7969\n",
      "Epoch 31/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8977 - acc: 0.9269Epoch 00031: val_loss improved from 1.11785 to 0.92135, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8949 - acc: 0.9280 - val_loss: 0.9213 - val_acc: 0.8906\n",
      "Epoch 32/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8440 - acc: 0.9500Epoch 00032: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8451 - acc: 0.9489 - val_loss: 0.9369 - val_acc: 0.8828\n",
      "Epoch 33/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8751 - acc: 0.9327Epoch 00033: val_loss improved from 0.92135 to 0.88267, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 197s 3s/step - loss: 0.8740 - acc: 0.9337 - val_loss: 0.8827 - val_acc: 0.9141\n",
      "Epoch 34/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8233 - acc: 0.9481Epoch 00034: val_loss improved from 0.88267 to 0.80192, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8233 - acc: 0.9489 - val_loss: 0.8019 - val_acc: 0.9531\n",
      "Epoch 35/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8160 - acc: 0.9635Epoch 00035: val_loss did not improve\n",
      "66/66 [==============================] - 197s 3s/step - loss: 0.8152 - acc: 0.9640 - val_loss: 1.0102 - val_acc: 0.8516\n",
      "Epoch 36/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8559 - acc: 0.9404Epoch 00036: val_loss improved from 0.80192 to 0.80008, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8537 - acc: 0.9413 - val_loss: 0.8001 - val_acc: 0.9297\n",
      "Epoch 37/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8113 - acc: 0.9615Epoch 00037: val_loss did not improve\n",
      "66/66 [==============================] - 197s 3s/step - loss: 0.8115 - acc: 0.9621 - val_loss: 0.9325 - val_acc: 0.8828\n",
      "Epoch 38/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8211 - acc: 0.9519Epoch 00038: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8187 - acc: 0.9527 - val_loss: 0.9163 - val_acc: 0.8672\n",
      "Epoch 39/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8481 - acc: 0.9308Epoch 00039: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8464 - acc: 0.9318 - val_loss: 1.2031 - val_acc: 0.8047\n",
      "Epoch 40/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8359 - acc: 0.9462Epoch 00040: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8367 - acc: 0.9451 - val_loss: 0.8993 - val_acc: 0.8906\n",
      "Epoch 41/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7982 - acc: 0.9577Epoch 00041: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7964 - acc: 0.9583 - val_loss: 1.0511 - val_acc: 0.8281\n",
      "Epoch 42/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8114 - acc: 0.9481Epoch 00042: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8101 - acc: 0.9489 - val_loss: 0.8538 - val_acc: 0.9062\n",
      "Epoch 43/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7829 - acc: 0.9596Epoch 00043: val_loss improved from 0.80008 to 0.78321, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7822 - acc: 0.9602 - val_loss: 0.7832 - val_acc: 0.9453\n",
      "Epoch 44/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8005 - acc: 0.9519Epoch 00044: val_loss improved from 0.78321 to 0.76827, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7989 - acc: 0.9527 - val_loss: 0.7683 - val_acc: 0.9531\n",
      "Epoch 45/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8376 - acc: 0.9346Epoch 00045: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.8396 - acc: 0.9337 - val_loss: 0.7703 - val_acc: 0.9453\n",
      "Epoch 46/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7947 - acc: 0.9385Epoch 00046: val_loss improved from 0.76827 to 0.75340, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7949 - acc: 0.9375 - val_loss: 0.7534 - val_acc: 0.9609\n",
      "Epoch 47/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7848 - acc: 0.9615Epoch 00047: val_loss improved from 0.75340 to 0.73633, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 197s 3s/step - loss: 0.7837 - acc: 0.9621 - val_loss: 0.7363 - val_acc: 0.9688\n",
      "Epoch 48/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7798 - acc: 0.9615Epoch 00048: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7784 - acc: 0.9621 - val_loss: 0.7443 - val_acc: 0.9609\n",
      "Epoch 49/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7749 - acc: 0.9596Epoch 00049: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7755 - acc: 0.9583 - val_loss: 0.7442 - val_acc: 0.9609\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 2s - loss: 0.7718 - acc: 0.9635Epoch 00050: val_loss improved from 0.73633 to 0.70921, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 195s 3s/step - loss: 0.7699 - acc: 0.9640 - val_loss: 0.7092 - val_acc: 0.9766\n",
      "Epoch 51/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7722 - acc: 0.9558Epoch 00051: val_loss did not improve\n",
      "66/66 [==============================] - 195s 3s/step - loss: 0.7712 - acc: 0.9564 - val_loss: 0.7764 - val_acc: 0.9609\n",
      "Epoch 52/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7827 - acc: 0.9558Epoch 00052: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7889 - acc: 0.9527 - val_loss: 0.7283 - val_acc: 0.9609\n",
      "Epoch 53/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7822 - acc: 0.9519Epoch 00053: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7886 - acc: 0.9508 - val_loss: 0.7624 - val_acc: 0.9453\n",
      "Epoch 54/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7809 - acc: 0.9577Epoch 00054: val_loss did not improve\n",
      "66/66 [==============================] - 196s 3s/step - loss: 0.7792 - acc: 0.9583 - val_loss: 0.7664 - val_acc: 0.9531\n",
      "Epoch 55/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7622 - acc: 0.9635Epoch 00055: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7621 - acc: 0.9640 - val_loss: 0.7115 - val_acc: 0.9688\n",
      "Epoch 56/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7586 - acc: 0.9673Epoch 00056: val_loss improved from 0.70921 to 0.69669, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7592 - acc: 0.9659 - val_loss: 0.6967 - val_acc: 0.9766\n",
      "Epoch 57/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7787 - acc: 0.9615Epoch 00057: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7771 - acc: 0.9621 - val_loss: 0.7170 - val_acc: 0.9688\n",
      "Epoch 58/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7662 - acc: 0.9654Epoch 00058: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7647 - acc: 0.9659 - val_loss: 0.7277 - val_acc: 0.9609\n",
      "Epoch 59/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8029 - acc: 0.9558Epoch 00059: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8009 - acc: 0.9564 - val_loss: 0.7210 - val_acc: 0.9609\n",
      "Epoch 60/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7683 - acc: 0.9577Epoch 00060: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7707 - acc: 0.9564 - val_loss: 0.7024 - val_acc: 0.9688\n",
      "Epoch 61/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7946 - acc: 0.9462Epoch 00061: val_loss improved from 0.69669 to 0.68753, saving model to mod_resnet.fold_2.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7924 - acc: 0.9470 - val_loss: 0.6875 - val_acc: 0.9766\n",
      "Epoch 62/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7487 - acc: 0.9731Epoch 00062: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7485 - acc: 0.9716 - val_loss: 0.7207 - val_acc: 0.9609\n",
      "Epoch 63/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7354 - acc: 0.9769Epoch 00063: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7350 - acc: 0.9773 - val_loss: 0.7122 - val_acc: 0.9609\n",
      "Epoch 64/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7680 - acc: 0.9654Epoch 00064: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7671 - acc: 0.9659 - val_loss: 0.7017 - val_acc: 0.9688\n",
      "Epoch 65/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7953 - acc: 0.9385Epoch 00065: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7943 - acc: 0.9394 - val_loss: 0.7015 - val_acc: 0.9688\n",
      "Epoch 66/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7656 - acc: 0.9615Epoch 00066: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7638 - acc: 0.9621 - val_loss: 0.6950 - val_acc: 0.9688\n",
      "Epoch 67/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7494 - acc: 0.9596Epoch 00067: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7521 - acc: 0.9583 - val_loss: 0.7070 - val_acc: 0.9609\n",
      "Epoch 68/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7506 - acc: 0.9673Epoch 00068: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7498 - acc: 0.9678 - val_loss: 0.7063 - val_acc: 0.9609\n",
      "Epoch 69/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7578 - acc: 0.9692Epoch 00069: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7569 - acc: 0.9697 - val_loss: 0.7067 - val_acc: 0.9609\n",
      "Epoch 70/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7800 - acc: 0.9654Epoch 00070: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7819 - acc: 0.9640 - val_loss: 0.7076 - val_acc: 0.9609\n",
      "Epoch 71/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7269 - acc: 0.9750Epoch 00071: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7298 - acc: 0.9735 - val_loss: 0.6975 - val_acc: 0.9688\n",
      "Epoch 00071: early stopping\n",
      "300/300 [==============================] - 34s 112ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 3\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4342 - acc: 0.7904Epoch 00001: val_loss improved from inf to 2.05840, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 210s 3s/step - loss: 1.4316 - acc: 0.7898 - val_loss: 2.0584 - val_acc: 0.5547\n",
      "Epoch 2/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 1.4180 - acc: 0.8173Epoch 00002: val_loss did not improve\n",
      "66/66 [==============================] - 219s 3s/step - loss: 1.4182 - acc: 0.8163 - val_loss: 9.0384 - val_acc: 0.2969\n",
      "Epoch 3/200\n",
      "65/66 [============================>.] - ETA: 33s - loss: 1.4798 - acc: 0.7904 Epoch 00003: val_loss did not improve\n",
      "66/66 [==============================] - 2220s 34s/step - loss: 1.4750 - acc: 0.7936 - val_loss: 7.8536 - val_acc: 0.2109\n",
      "Epoch 4/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4056 - acc: 0.8154Epoch 00004: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.4090 - acc: 0.8144 - val_loss: 3.7832 - val_acc: 0.5234\n",
      "Epoch 5/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3410 - acc: 0.8442Epoch 00005: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.3378 - acc: 0.8447 - val_loss: 5.2925 - val_acc: 0.4609\n",
      "Epoch 6/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3056 - acc: 0.8385Epoch 00006: val_loss improved from 2.05840 to 1.68300, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.2997 - acc: 0.8409 - val_loss: 1.6830 - val_acc: 0.6641\n",
      "Epoch 7/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.4235 - acc: 0.7904Epoch 00007: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.4284 - acc: 0.7841 - val_loss: 8.7255 - val_acc: 0.2969\n",
      "Epoch 8/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3485 - acc: 0.8269Epoch 00008: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.3431 - acc: 0.8277 - val_loss: 2.1571 - val_acc: 0.5938\n",
      "Epoch 9/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2874 - acc: 0.8519Epoch 00009: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.2985 - acc: 0.8485 - val_loss: 2.3511 - val_acc: 0.5000\n",
      "Epoch 10/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3592 - acc: 0.7981Epoch 00010: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 1.3596 - acc: 0.7992 - val_loss: 4.2927 - val_acc: 0.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.3145 - acc: 0.8212Epoch 00011: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 1.3217 - acc: 0.8201 - val_loss: 2.2901 - val_acc: 0.6016\n",
      "Epoch 12/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1688 - acc: 0.8654Epoch 00012: val_loss improved from 1.68300 to 1.13405, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 1.1670 - acc: 0.8655 - val_loss: 1.1341 - val_acc: 0.8359\n",
      "Epoch 13/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0574 - acc: 0.9154Epoch 00013: val_loss improved from 1.13405 to 0.95195, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0550 - acc: 0.9167 - val_loss: 0.9520 - val_acc: 0.9219\n",
      "Epoch 14/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0337 - acc: 0.9173Epoch 00014: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.0339 - acc: 0.9186 - val_loss: 1.4857 - val_acc: 0.7734\n",
      "Epoch 15/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0142 - acc: 0.9173Epoch 00015: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0137 - acc: 0.9186 - val_loss: 1.0021 - val_acc: 0.8984\n",
      "Epoch 16/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0501 - acc: 0.8942Epoch 00016: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0506 - acc: 0.8920 - val_loss: 1.0011 - val_acc: 0.8984\n",
      "Epoch 17/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9828 - acc: 0.9115Epoch 00017: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.9806 - acc: 0.9129 - val_loss: 1.6543 - val_acc: 0.7109\n",
      "Epoch 18/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9608 - acc: 0.9250Epoch 00018: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.9680 - acc: 0.9223 - val_loss: 1.4619 - val_acc: 0.7109\n",
      "Epoch 19/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9770 - acc: 0.9096Epoch 00019: val_loss improved from 0.95195 to 0.87366, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.9743 - acc: 0.9110 - val_loss: 0.8737 - val_acc: 0.9375\n",
      "Epoch 20/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9522 - acc: 0.9154Epoch 00020: val_loss improved from 0.87366 to 0.82496, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.9503 - acc: 0.9167 - val_loss: 0.8250 - val_acc: 0.9766\n",
      "Epoch 21/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9113 - acc: 0.9327Epoch 00021: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.9130 - acc: 0.9318 - val_loss: 1.0748 - val_acc: 0.8672\n",
      "Epoch 22/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9077 - acc: 0.9385Epoch 00022: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.9134 - acc: 0.9356 - val_loss: 0.8825 - val_acc: 0.9375\n",
      "Epoch 23/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9203 - acc: 0.9308Epoch 00023: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.9187 - acc: 0.9318 - val_loss: 0.8414 - val_acc: 0.9609\n",
      "Epoch 24/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9476 - acc: 0.9250Epoch 00024: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.9490 - acc: 0.9242 - val_loss: 0.8292 - val_acc: 0.9375\n",
      "Epoch 25/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9073 - acc: 0.9327Epoch 00025: val_loss improved from 0.82496 to 0.80364, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.9042 - acc: 0.9337 - val_loss: 0.8036 - val_acc: 0.9531\n",
      "Epoch 26/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9404 - acc: 0.9212Epoch 00026: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.9377 - acc: 0.9223 - val_loss: 0.8448 - val_acc: 0.9453\n",
      "Epoch 27/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8798 - acc: 0.9404Epoch 00027: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.8826 - acc: 0.9394 - val_loss: 0.8121 - val_acc: 0.9609\n",
      "Epoch 28/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8629 - acc: 0.9481Epoch 00028: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.8657 - acc: 0.9470 - val_loss: 0.8317 - val_acc: 0.9531\n",
      "Epoch 29/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8803 - acc: 0.9423Epoch 00029: val_loss improved from 0.80364 to 0.78248, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8782 - acc: 0.9432 - val_loss: 0.7825 - val_acc: 0.9766\n",
      "Epoch 30/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8768 - acc: 0.9385Epoch 00030: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.8760 - acc: 0.9394 - val_loss: 0.7915 - val_acc: 0.9609\n",
      "Epoch 31/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8643 - acc: 0.9577Epoch 00031: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.8668 - acc: 0.9545 - val_loss: 0.7858 - val_acc: 0.9453\n",
      "Epoch 32/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8545 - acc: 0.9519Epoch 00032: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8557 - acc: 0.9527 - val_loss: 0.8216 - val_acc: 0.9453\n",
      "Epoch 33/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8873 - acc: 0.9346Epoch 00033: val_loss improved from 0.78248 to 0.75866, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8885 - acc: 0.9337 - val_loss: 0.7587 - val_acc: 0.9688\n",
      "Epoch 34/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8264 - acc: 0.9519Epoch 00034: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8248 - acc: 0.9527 - val_loss: 0.7830 - val_acc: 0.9531\n",
      "Epoch 35/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8493 - acc: 0.9462Epoch 00035: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8494 - acc: 0.9470 - val_loss: 1.0023 - val_acc: 0.8438\n",
      "Epoch 36/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8418 - acc: 0.9538Epoch 00036: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8418 - acc: 0.9545 - val_loss: 0.8055 - val_acc: 0.9453\n",
      "Epoch 37/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8837 - acc: 0.9231Epoch 00037: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8810 - acc: 0.9242 - val_loss: 0.7977 - val_acc: 0.9453\n",
      "Epoch 38/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8295 - acc: 0.9615Epoch 00038: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8337 - acc: 0.9583 - val_loss: 1.3496 - val_acc: 0.8594\n",
      "Epoch 39/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8304 - acc: 0.9481Epoch 00039: val_loss improved from 0.75866 to 0.73474, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8284 - acc: 0.9489 - val_loss: 0.7347 - val_acc: 0.9766\n",
      "Epoch 40/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8039 - acc: 0.9615Epoch 00040: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8041 - acc: 0.9621 - val_loss: 0.7563 - val_acc: 0.9531\n",
      "Epoch 41/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8218 - acc: 0.9538Epoch 00041: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8213 - acc: 0.9545 - val_loss: 0.7674 - val_acc: 0.9688\n",
      "Epoch 42/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8189 - acc: 0.9519Epoch 00042: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8165 - acc: 0.9527 - val_loss: 0.7726 - val_acc: 0.9453\n",
      "Epoch 43/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7941 - acc: 0.9558Epoch 00043: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7939 - acc: 0.9545 - val_loss: 0.7466 - val_acc: 0.9688\n",
      "Epoch 44/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7754 - acc: 0.9692Epoch 00044: val_loss improved from 0.73474 to 0.70732, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.7745 - acc: 0.9697 - val_loss: 0.7073 - val_acc: 0.9844\n",
      "Epoch 45/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8167 - acc: 0.9538Epoch 00045: val_loss did not improve\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.8190 - acc: 0.9527 - val_loss: 0.7414 - val_acc: 0.9844\n",
      "Epoch 46/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7990 - acc: 0.9481Epoch 00046: val_loss improved from 0.70732 to 0.69874, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 209s 3s/step - loss: 0.8031 - acc: 0.9451 - val_loss: 0.6987 - val_acc: 0.9922\n",
      "Epoch 47/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7931 - acc: 0.9615Epoch 00047: val_loss did not improve\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.7911 - acc: 0.9621 - val_loss: 0.7219 - val_acc: 0.9766\n",
      "Epoch 48/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7938 - acc: 0.9577Epoch 00048: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7978 - acc: 0.9545 - val_loss: 0.7203 - val_acc: 0.9766\n",
      "Epoch 49/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7762 - acc: 0.9673Epoch 00049: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7750 - acc: 0.9678 - val_loss: 0.7410 - val_acc: 0.9688\n",
      "Epoch 50/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8072 - acc: 0.9500Epoch 00050: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8074 - acc: 0.9489 - val_loss: 0.7088 - val_acc: 0.9844\n",
      "Epoch 51/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8207 - acc: 0.9462Epoch 00051: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.8196 - acc: 0.9470 - val_loss: 0.7214 - val_acc: 0.9688\n",
      "Epoch 52/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8012 - acc: 0.9615Epoch 00052: val_loss improved from 0.69874 to 0.69605, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7994 - acc: 0.9621 - val_loss: 0.6961 - val_acc: 0.9844\n",
      "Epoch 53/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7880 - acc: 0.9615Epoch 00053: val_loss improved from 0.69605 to 0.69541, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.7884 - acc: 0.9621 - val_loss: 0.6954 - val_acc: 0.9844\n",
      "Epoch 54/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7909 - acc: 0.9538Epoch 00054: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7944 - acc: 0.9527 - val_loss: 0.7004 - val_acc: 0.9844\n",
      "Epoch 55/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8068 - acc: 0.9519Epoch 00055: val_loss did not improve\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.8055 - acc: 0.9527 - val_loss: 0.7049 - val_acc: 0.9844\n",
      "Epoch 56/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8116 - acc: 0.9519Epoch 00056: val_loss did not improve\n",
      "66/66 [==============================] - 209s 3s/step - loss: 0.8120 - acc: 0.9508 - val_loss: 0.6996 - val_acc: 0.9766\n",
      "Epoch 57/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7966 - acc: 0.9500Epoch 00057: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7952 - acc: 0.9508 - val_loss: 0.6994 - val_acc: 0.9844\n",
      "Epoch 58/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7750 - acc: 0.9673Epoch 00058: val_loss did not improve\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.7739 - acc: 0.9678 - val_loss: 0.6976 - val_acc: 0.9844\n",
      "Epoch 59/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7733 - acc: 0.9596Epoch 00059: val_loss improved from 0.69541 to 0.69470, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7716 - acc: 0.9602 - val_loss: 0.6947 - val_acc: 0.9844\n",
      "Epoch 60/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7633 - acc: 0.9673Epoch 00060: val_loss improved from 0.69470 to 0.68966, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7628 - acc: 0.9678 - val_loss: 0.6897 - val_acc: 0.9922\n",
      "Epoch 61/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7496 - acc: 0.9750Epoch 00061: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7514 - acc: 0.9754 - val_loss: 0.6914 - val_acc: 0.9922\n",
      "Epoch 62/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7708 - acc: 0.9731Epoch 00062: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7713 - acc: 0.9716 - val_loss: 0.6902 - val_acc: 0.9922\n",
      "Epoch 63/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7551 - acc: 0.9673Epoch 00063: val_loss improved from 0.68966 to 0.68455, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7566 - acc: 0.9659 - val_loss: 0.6846 - val_acc: 0.9922\n",
      "Epoch 64/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8126 - acc: 0.9404Epoch 00064: val_loss did not improve\n",
      "66/66 [==============================] - 208s 3s/step - loss: 0.8157 - acc: 0.9394 - val_loss: 0.6915 - val_acc: 0.9922\n",
      "Epoch 65/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7520 - acc: 0.9712Epoch 00065: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7536 - acc: 0.9678 - val_loss: 0.6939 - val_acc: 0.9922\n",
      "Epoch 66/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7768 - acc: 0.9673Epoch 00066: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7766 - acc: 0.9678 - val_loss: 0.6898 - val_acc: 0.9922\n",
      "Epoch 67/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8001 - acc: 0.9577Epoch 00067: val_loss improved from 0.68455 to 0.68439, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8004 - acc: 0.9564 - val_loss: 0.6844 - val_acc: 0.9922\n",
      "Epoch 68/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7771 - acc: 0.9596Epoch 00068: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7788 - acc: 0.9583 - val_loss: 0.6853 - val_acc: 0.9922\n",
      "Epoch 69/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7962 - acc: 0.9462Epoch 00069: val_loss improved from 0.68439 to 0.68431, saving model to mod_resnet.fold_3.hdf5\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7949 - acc: 0.9470 - val_loss: 0.6843 - val_acc: 0.9922\n",
      "Epoch 70/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7714 - acc: 0.9596Epoch 00070: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7733 - acc: 0.9583 - val_loss: 0.6876 - val_acc: 0.9922\n",
      "Epoch 71/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7906 - acc: 0.9558Epoch 00071: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7887 - acc: 0.9564 - val_loss: 0.6899 - val_acc: 0.9922\n",
      "Epoch 72/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8137 - acc: 0.9423Epoch 00072: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8126 - acc: 0.9432 - val_loss: 0.6933 - val_acc: 0.9922\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/66 [============================>.] - ETA: 2s - loss: 0.8012 - acc: 0.9500Epoch 00073: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.8001 - acc: 0.9508 - val_loss: 0.6917 - val_acc: 0.9922\n",
      "Epoch 74/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7693 - acc: 0.9673Epoch 00074: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7683 - acc: 0.9678 - val_loss: 0.6929 - val_acc: 0.9922\n",
      "Epoch 75/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7740 - acc: 0.9692Epoch 00075: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7735 - acc: 0.9697 - val_loss: 0.6845 - val_acc: 0.9922\n",
      "Epoch 76/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7907 - acc: 0.9654Epoch 00076: val_loss did not improve\n",
      "66/66 [==============================] - 207s 3s/step - loss: 0.7891 - acc: 0.9659 - val_loss: 0.6889 - val_acc: 0.9922\n",
      "Epoch 77/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7683 - acc: 0.9731Epoch 00077: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7686 - acc: 0.9716 - val_loss: 0.6906 - val_acc: 0.9922\n",
      "Epoch 00077: early stopping\n",
      "300/300 [==============================] - 34s 113ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 4\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.2517 - acc: 0.7846Epoch 00001: val_loss improved from inf to 2.46435, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 210s 3s/step - loss: 1.2450 - acc: 0.7879 - val_loss: 2.4643 - val_acc: 0.5859\n",
      "Epoch 2/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1010 - acc: 0.8615Epoch 00002: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.1021 - acc: 0.8598 - val_loss: 6.9192 - val_acc: 0.3672\n",
      "Epoch 3/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0648 - acc: 0.8596Epoch 00003: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0605 - acc: 0.8617 - val_loss: 9.2193 - val_acc: 0.2500\n",
      "Epoch 4/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1642 - acc: 0.8346Epoch 00004: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.1659 - acc: 0.8314 - val_loss: 10.6574 - val_acc: 0.1562\n",
      "Epoch 5/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1333 - acc: 0.8385Epoch 00005: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 1.1378 - acc: 0.8390 - val_loss: 5.7857 - val_acc: 0.4375\n",
      "Epoch 6/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0845 - acc: 0.8558Epoch 00006: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 1.0821 - acc: 0.8580 - val_loss: 3.2276 - val_acc: 0.4766\n",
      "Epoch 7/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9535 - acc: 0.9000Epoch 00007: val_loss improved from 2.46435 to 0.81914, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.9502 - acc: 0.9015 - val_loss: 0.8191 - val_acc: 0.9453\n",
      "Epoch 8/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8575 - acc: 0.9500Epoch 00008: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8577 - acc: 0.9489 - val_loss: 0.9229 - val_acc: 0.8984\n",
      "Epoch 9/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8514 - acc: 0.9288Epoch 00009: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8505 - acc: 0.9280 - val_loss: 1.3546 - val_acc: 0.7422\n",
      "Epoch 10/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8212 - acc: 0.9423Epoch 00010: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8206 - acc: 0.9413 - val_loss: 0.8968 - val_acc: 0.8750\n",
      "Epoch 11/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8401 - acc: 0.9346Epoch 00011: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8399 - acc: 0.9337 - val_loss: 1.6392 - val_acc: 0.7656\n",
      "Epoch 12/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8180 - acc: 0.9404Epoch 00012: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8187 - acc: 0.9394 - val_loss: 1.6616 - val_acc: 0.7656\n",
      "Epoch 13/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8293 - acc: 0.9308Epoch 00013: val_loss improved from 0.81914 to 0.74720, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.8283 - acc: 0.9318 - val_loss: 0.7472 - val_acc: 0.9453\n",
      "Epoch 14/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7763 - acc: 0.9577Epoch 00014: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7752 - acc: 0.9583 - val_loss: 0.7999 - val_acc: 0.9453\n",
      "Epoch 15/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7690 - acc: 0.9481Epoch 00015: val_loss improved from 0.74720 to 0.69833, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7713 - acc: 0.9489 - val_loss: 0.6983 - val_acc: 0.9609\n",
      "Epoch 16/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7757 - acc: 0.9404Epoch 00016: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7739 - acc: 0.9413 - val_loss: 0.7539 - val_acc: 0.9297\n",
      "Epoch 17/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7451 - acc: 0.9577Epoch 00017: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7439 - acc: 0.9583 - val_loss: 0.7119 - val_acc: 0.9688\n",
      "Epoch 18/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7575 - acc: 0.9577Epoch 00018: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7559 - acc: 0.9583 - val_loss: 0.7465 - val_acc: 0.9375\n",
      "Epoch 19/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7372 - acc: 0.9558Epoch 00019: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7365 - acc: 0.9564 - val_loss: 0.7095 - val_acc: 0.9609\n",
      "Epoch 20/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7552 - acc: 0.9481Epoch 00020: val_loss improved from 0.69833 to 0.68026, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7556 - acc: 0.9470 - val_loss: 0.6803 - val_acc: 0.9688\n",
      "Epoch 21/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7662 - acc: 0.9442Epoch 00021: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7704 - acc: 0.9451 - val_loss: 0.6837 - val_acc: 0.9609\n",
      "Epoch 22/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7661 - acc: 0.9423Epoch 00022: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7668 - acc: 0.9413 - val_loss: 0.7075 - val_acc: 0.9531\n",
      "Epoch 23/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7397 - acc: 0.9481Epoch 00023: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7387 - acc: 0.9489 - val_loss: 0.7540 - val_acc: 0.9375\n",
      "Epoch 24/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7004 - acc: 0.9654Epoch 00024: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7007 - acc: 0.9659 - val_loss: 0.6992 - val_acc: 0.9609\n",
      "Epoch 25/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7662 - acc: 0.9404Epoch 00025: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7659 - acc: 0.9413 - val_loss: 0.6934 - val_acc: 0.9453\n",
      "Epoch 26/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6932 - acc: 0.9712Epoch 00026: val_loss improved from 0.68026 to 0.67794, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6938 - acc: 0.9697 - val_loss: 0.6779 - val_acc: 0.9688\n",
      "Epoch 27/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7261 - acc: 0.9596Epoch 00027: val_loss improved from 0.67794 to 0.64337, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7250 - acc: 0.9602 - val_loss: 0.6434 - val_acc: 0.9688\n",
      "Epoch 28/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7237 - acc: 0.9577Epoch 00028: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.7225 - acc: 0.9583 - val_loss: 0.6709 - val_acc: 0.9766\n",
      "Epoch 29/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7214 - acc: 0.9558Epoch 00029: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7200 - acc: 0.9564 - val_loss: 0.6598 - val_acc: 0.9688\n",
      "Epoch 30/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7090 - acc: 0.9558Epoch 00030: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7092 - acc: 0.9564 - val_loss: 0.6471 - val_acc: 0.9766\n",
      "Epoch 31/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7114 - acc: 0.9577Epoch 00031: val_loss improved from 0.64337 to 0.62511, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7115 - acc: 0.9583 - val_loss: 0.6251 - val_acc: 0.9844\n",
      "Epoch 32/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6927 - acc: 0.9615Epoch 00032: val_loss improved from 0.62511 to 0.62507, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6936 - acc: 0.9602 - val_loss: 0.6251 - val_acc: 0.9766\n",
      "Epoch 33/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7274 - acc: 0.9481Epoch 00033: val_loss improved from 0.62507 to 0.62487, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7256 - acc: 0.9489 - val_loss: 0.6249 - val_acc: 0.9688\n",
      "Epoch 34/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7220 - acc: 0.9577Epoch 00034: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7210 - acc: 0.9583 - val_loss: 0.6289 - val_acc: 0.9766\n",
      "Epoch 35/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6849 - acc: 0.9673Epoch 00035: val_loss improved from 0.62487 to 0.62138, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6927 - acc: 0.9659 - val_loss: 0.6214 - val_acc: 0.9766\n",
      "Epoch 36/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6846 - acc: 0.9712Epoch 00036: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6847 - acc: 0.9697 - val_loss: 0.6393 - val_acc: 0.9766\n",
      "Epoch 37/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6977 - acc: 0.9558Epoch 00037: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.6964 - acc: 0.9564 - val_loss: 0.6383 - val_acc: 0.9766\n",
      "Epoch 38/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6785 - acc: 0.9673Epoch 00038: val_loss improved from 0.62138 to 0.62034, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 209s 3s/step - loss: 0.6779 - acc: 0.9678 - val_loss: 0.6203 - val_acc: 0.9844\n",
      "Epoch 39/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 0.6797 - acc: 0.9692Epoch 00039: val_loss improved from 0.62034 to 0.61270, saving model to mod_resnet.fold_4.hdf5\n",
      "66/66 [==============================] - 213s 3s/step - loss: 0.6782 - acc: 0.9697 - val_loss: 0.6127 - val_acc: 0.9844\n",
      "Epoch 40/200\n",
      "65/66 [============================>.] - ETA: 3s - loss: 0.7038 - acc: 0.9596Epoch 00040: val_loss did not improve\n",
      "66/66 [==============================] - 230s 3s/step - loss: 0.7029 - acc: 0.9602 - val_loss: 0.6403 - val_acc: 0.9688\n",
      "Epoch 41/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6856 - acc: 0.9635Epoch 00041: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.6845 - acc: 0.9640 - val_loss: 0.6207 - val_acc: 0.9844\n",
      "Epoch 42/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7004 - acc: 0.9615Epoch 00042: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7011 - acc: 0.9602 - val_loss: 0.6380 - val_acc: 0.9766\n",
      "Epoch 43/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6855 - acc: 0.9577Epoch 00043: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6893 - acc: 0.9564 - val_loss: 0.6318 - val_acc: 0.9844\n",
      "Epoch 44/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7252 - acc: 0.9558Epoch 00044: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7238 - acc: 0.9564 - val_loss: 0.6213 - val_acc: 0.9844\n",
      "Epoch 45/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7267 - acc: 0.9442Epoch 00045: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7264 - acc: 0.9451 - val_loss: 0.6261 - val_acc: 0.9844\n",
      "Epoch 46/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6750 - acc: 0.9673Epoch 00046: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6759 - acc: 0.9659 - val_loss: 0.6271 - val_acc: 0.9766\n",
      "Epoch 47/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6855 - acc: 0.9654Epoch 00047: val_loss did not improve\n",
      "66/66 [==============================] - 202s 3s/step - loss: 0.6863 - acc: 0.9640 - val_loss: 0.6282 - val_acc: 0.9766\n",
      "Epoch 48/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6749 - acc: 0.9673Epoch 00048: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6767 - acc: 0.9659 - val_loss: 0.6148 - val_acc: 0.9844\n",
      "Epoch 49/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6774 - acc: 0.9712Epoch 00049: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6760 - acc: 0.9716 - val_loss: 0.6170 - val_acc: 0.9844\n",
      "Epoch 00049: early stopping\n",
      "300/300 [==============================] - 33s 109ms/step\n",
      "\n",
      "\n",
      "\n",
      "Now beginning training for fold 5\n",
      "\n",
      "\n",
      "Epoch 1/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1368 - acc: 0.8558Epoch 00001: val_loss improved from inf to 1.30842, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 211s 3s/step - loss: 1.1326 - acc: 0.8561 - val_loss: 1.3084 - val_acc: 0.7422\n",
      "Epoch 2/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1597 - acc: 0.8558Epoch 00002: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 1.1582 - acc: 0.8561 - val_loss: 4.2293 - val_acc: 0.3828\n",
      "Epoch 3/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1626 - acc: 0.8231Epoch 00003: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.1590 - acc: 0.8258 - val_loss: 5.9371 - val_acc: 0.3984\n",
      "Epoch 4/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1213 - acc: 0.8346Epoch 00004: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 1.1183 - acc: 0.8352 - val_loss: 3.8115 - val_acc: 0.4844\n",
      "Epoch 5/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1369 - acc: 0.8404Epoch 00005: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 1.1311 - acc: 0.8428 - val_loss: 1.8529 - val_acc: 0.6172\n",
      "Epoch 6/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.1032 - acc: 0.8365Epoch 00006: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0979 - acc: 0.8390 - val_loss: 2.5528 - val_acc: 0.6641\n",
      "Epoch 7/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 1.0045 - acc: 0.8673Epoch 00007: val_loss improved from 1.30842 to 1.03099, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 1.0125 - acc: 0.8655 - val_loss: 1.0310 - val_acc: 0.8594\n",
      "Epoch 8/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9516 - acc: 0.9000Epoch 00008: val_loss improved from 1.03099 to 0.92507, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 202s 3s/step - loss: 0.9481 - acc: 0.9015 - val_loss: 0.9251 - val_acc: 0.8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.9065 - acc: 0.9058Epoch 00009: val_loss did not improve\n",
      "66/66 [==============================] - 202s 3s/step - loss: 0.9073 - acc: 0.9053 - val_loss: 1.3502 - val_acc: 0.7500\n",
      "Epoch 10/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8705 - acc: 0.9173Epoch 00010: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.8711 - acc: 0.9167 - val_loss: 0.9788 - val_acc: 0.8594\n",
      "Epoch 11/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8351 - acc: 0.9269Epoch 00011: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8317 - acc: 0.9280 - val_loss: 1.0381 - val_acc: 0.8281\n",
      "Epoch 12/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8296 - acc: 0.9308Epoch 00012: val_loss improved from 0.92507 to 0.82588, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8284 - acc: 0.9318 - val_loss: 0.8259 - val_acc: 0.9141\n",
      "Epoch 13/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8209 - acc: 0.9231Epoch 00013: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.8221 - acc: 0.9223 - val_loss: 3.8507 - val_acc: 0.6172\n",
      "Epoch 14/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8032 - acc: 0.9269Epoch 00014: val_loss improved from 0.82588 to 0.74260, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.8039 - acc: 0.9261 - val_loss: 0.7426 - val_acc: 0.9531\n",
      "Epoch 15/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7949 - acc: 0.9269Epoch 00015: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7936 - acc: 0.9280 - val_loss: 0.8539 - val_acc: 0.9219\n",
      "Epoch 16/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7945 - acc: 0.9269Epoch 00016: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7952 - acc: 0.9261 - val_loss: 1.1241 - val_acc: 0.7734\n",
      "Epoch 17/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.8175 - acc: 0.9115Epoch 00017: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.8178 - acc: 0.9110 - val_loss: 0.7495 - val_acc: 0.9453\n",
      "Epoch 18/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7720 - acc: 0.9423Epoch 00018: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7691 - acc: 0.9432 - val_loss: 1.1445 - val_acc: 0.8828\n",
      "Epoch 19/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7756 - acc: 0.9365Epoch 00019: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7725 - acc: 0.9375 - val_loss: 1.5181 - val_acc: 0.6875\n",
      "Epoch 20/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7518 - acc: 0.9404Epoch 00020: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7518 - acc: 0.9413 - val_loss: 0.7574 - val_acc: 0.9375\n",
      "Epoch 21/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7374 - acc: 0.9462Epoch 00021: val_loss improved from 0.74260 to 0.70553, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7375 - acc: 0.9451 - val_loss: 0.7055 - val_acc: 0.9531\n",
      "Epoch 22/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7272 - acc: 0.9481Epoch 00022: val_loss improved from 0.70553 to 0.64634, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.7262 - acc: 0.9489 - val_loss: 0.6463 - val_acc: 0.9688\n",
      "Epoch 23/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7014 - acc: 0.9462Epoch 00023: val_loss improved from 0.64634 to 0.59717, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 202s 3s/step - loss: 0.7000 - acc: 0.9470 - val_loss: 0.5972 - val_acc: 0.9922\n",
      "Epoch 24/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7186 - acc: 0.9538Epoch 00024: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.7179 - acc: 0.9545 - val_loss: 0.6684 - val_acc: 0.9531\n",
      "Epoch 25/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6929 - acc: 0.9673Epoch 00025: val_loss improved from 0.59717 to 0.58718, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6907 - acc: 0.9678 - val_loss: 0.5872 - val_acc: 0.9922\n",
      "Epoch 26/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.7091 - acc: 0.9577Epoch 00026: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.7079 - acc: 0.9583 - val_loss: 0.6749 - val_acc: 0.9453\n",
      "Epoch 27/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6771 - acc: 0.9615Epoch 00027: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6755 - acc: 0.9621 - val_loss: 0.7646 - val_acc: 0.8906\n",
      "Epoch 28/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6689 - acc: 0.9615Epoch 00028: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6698 - acc: 0.9602 - val_loss: 0.6121 - val_acc: 0.9609\n",
      "Epoch 29/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6906 - acc: 0.9538Epoch 00029: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6902 - acc: 0.9545 - val_loss: 0.6590 - val_acc: 0.9531\n",
      "Epoch 30/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6881 - acc: 0.9538Epoch 00030: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6878 - acc: 0.9527 - val_loss: 0.6472 - val_acc: 0.9609\n",
      "Epoch 31/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6282 - acc: 0.9788Epoch 00031: val_loss improved from 0.58718 to 0.58202, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6287 - acc: 0.9792 - val_loss: 0.5820 - val_acc: 0.9688\n",
      "Epoch 32/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6783 - acc: 0.9481Epoch 00032: val_loss did not improve\n",
      "66/66 [==============================] - 211s 3s/step - loss: 0.6767 - acc: 0.9489 - val_loss: 0.6033 - val_acc: 0.9688\n",
      "Epoch 33/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6828 - acc: 0.9538Epoch 00033: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6808 - acc: 0.9545 - val_loss: 0.5976 - val_acc: 0.9688\n",
      "Epoch 34/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6397 - acc: 0.9692Epoch 00034: val_loss improved from 0.58202 to 0.56379, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6433 - acc: 0.9659 - val_loss: 0.5638 - val_acc: 0.9844\n",
      "Epoch 35/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6762 - acc: 0.9635Epoch 00035: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6745 - acc: 0.9640 - val_loss: 0.5872 - val_acc: 0.9766\n",
      "Epoch 36/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6733 - acc: 0.9635Epoch 00036: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6717 - acc: 0.9640 - val_loss: 0.5982 - val_acc: 0.9766\n",
      "Epoch 37/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6640 - acc: 0.9654Epoch 00037: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6622 - acc: 0.9659 - val_loss: 0.5960 - val_acc: 0.9688\n",
      "Epoch 38/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6120 - acc: 0.9808Epoch 00038: val_loss improved from 0.56379 to 0.55550, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6118 - acc: 0.9811 - val_loss: 0.5555 - val_acc: 0.9922\n",
      "Epoch 39/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6611 - acc: 0.9577Epoch 00039: val_loss did not improve\n",
      "66/66 [==============================] - 203s 3s/step - loss: 0.6608 - acc: 0.9583 - val_loss: 0.5706 - val_acc: 0.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6451 - acc: 0.9635Epoch 00040: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6436 - acc: 0.9640 - val_loss: 0.5680 - val_acc: 0.9766\n",
      "Epoch 41/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6463 - acc: 0.9538Epoch 00041: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6476 - acc: 0.9527 - val_loss: 0.5683 - val_acc: 0.9844\n",
      "Epoch 42/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6180 - acc: 0.9731Epoch 00042: val_loss improved from 0.55550 to 0.55165, saving model to mod_resnet.fold_5.hdf5\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6173 - acc: 0.9735 - val_loss: 0.5516 - val_acc: 0.9766\n",
      "Epoch 43/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6364 - acc: 0.9769Epoch 00043: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6354 - acc: 0.9773 - val_loss: 0.5956 - val_acc: 0.9766\n",
      "Epoch 44/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6408 - acc: 0.9673Epoch 00044: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6424 - acc: 0.9659 - val_loss: 0.5749 - val_acc: 0.9688\n",
      "Epoch 45/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6528 - acc: 0.9596Epoch 00045: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6615 - acc: 0.9564 - val_loss: 0.5644 - val_acc: 0.9688\n",
      "Epoch 46/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6493 - acc: 0.9635Epoch 00046: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6479 - acc: 0.9640 - val_loss: 0.6343 - val_acc: 0.9375\n",
      "Epoch 47/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6664 - acc: 0.9596Epoch 00047: val_loss did not improve\n",
      "66/66 [==============================] - 204s 3s/step - loss: 0.6660 - acc: 0.9583 - val_loss: 0.5595 - val_acc: 0.9766\n",
      "Epoch 48/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6413 - acc: 0.9692Epoch 00048: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6398 - acc: 0.9697 - val_loss: 0.5526 - val_acc: 0.9766\n",
      "Epoch 49/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6247 - acc: 0.9769Epoch 00049: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6254 - acc: 0.9754 - val_loss: 0.5542 - val_acc: 0.9766\n",
      "Epoch 50/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6248 - acc: 0.9788Epoch 00050: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6237 - acc: 0.9792 - val_loss: 0.5583 - val_acc: 0.9688\n",
      "Epoch 51/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6256 - acc: 0.9769Epoch 00051: val_loss did not improve\n",
      "66/66 [==============================] - 206s 3s/step - loss: 0.6242 - acc: 0.9773 - val_loss: 0.5597 - val_acc: 0.9688\n",
      "Epoch 52/200\n",
      "65/66 [============================>.] - ETA: 2s - loss: 0.6249 - acc: 0.9692Epoch 00052: val_loss did not improve\n",
      "66/66 [==============================] - 205s 3s/step - loss: 0.6260 - acc: 0.9678 - val_loss: 0.5666 - val_acc: 0.9688\n",
      "Epoch 00052: early stopping\n",
      "300/300 [==============================] - 34s 115ms/step\n",
      "\n",
      "\n",
      "\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2**3\n",
    "epoch_num = 200\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "test_pred, models, results = train_model(model, batch_size, epoch_num, X, y_cat, X_test, kf, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.02144632e-11,   1.68444008e-05,   1.24929461e-06, ...,\n",
       "          2.60466611e-10,   2.42799421e-08,   8.17874746e-08],\n",
       "       [  1.02301290e-07,   1.29256962e-04,   3.75487085e-05, ...,\n",
       "          6.52675740e-08,   1.85423571e-06,   9.42297262e-08],\n",
       "       [  2.11103907e-05,   6.27645123e-04,   9.59709525e-01, ...,\n",
       "          1.54709763e-06,   1.16135895e-06,   1.53431431e-05],\n",
       "       ..., \n",
       "       [  9.99663341e-01,   2.73752899e-08,   1.97968924e-05, ...,\n",
       "          5.76375622e-07,   1.48610395e-08,   3.39200863e-06],\n",
       "       [  6.41156139e-04,   3.20117127e-03,   1.04118448e-04, ...,\n",
       "          3.70596951e-05,   6.04837896e-06,   3.40983289e-04],\n",
       "       [  1.23120831e-03,   4.46316041e-03,   2.64275972e-04, ...,\n",
       "          4.83353822e-05,   5.36010222e-05,   7.03687258e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(test_pred, axis=1)\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=['Class'])\n",
    "df['Id'] = df.index\n",
    "\n",
    "# df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>9</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>9</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>17</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>13</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>10</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>8</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>4</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>15</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>4</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>18</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>10</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>9</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>5</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>14</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>6</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>3</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>10</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>13</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>8</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>19</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>17</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>11</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>8</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>8</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class   Id\n",
       "0        4    0\n",
       "1       16    1\n",
       "2        2    2\n",
       "3       18    3\n",
       "4       11    4\n",
       "5        5    5\n",
       "6        6    6\n",
       "7       15    7\n",
       "8        8    8\n",
       "9        8    9\n",
       "10       3   10\n",
       "11       8   11\n",
       "12      15   12\n",
       "13       2   13\n",
       "14      14   14\n",
       "15       1   15\n",
       "16      10   16\n",
       "17      17   17\n",
       "18       8   18\n",
       "19      13   19\n",
       "20       1   20\n",
       "21       4   21\n",
       "22       6   22\n",
       "23      15   23\n",
       "24      10   24\n",
       "25      10   25\n",
       "26       8   26\n",
       "27      12   27\n",
       "28       2   28\n",
       "29      15   29\n",
       "..     ...  ...\n",
       "270      9  270\n",
       "271      9  271\n",
       "272     17  272\n",
       "273     13  273\n",
       "274     10  274\n",
       "275      8  275\n",
       "276      4  276\n",
       "277     15  277\n",
       "278      0  278\n",
       "279      4  279\n",
       "280     18  280\n",
       "281     10  281\n",
       "282      9  282\n",
       "283      5  283\n",
       "284     14  284\n",
       "285      6  285\n",
       "286      0  286\n",
       "287      3  287\n",
       "288     10  288\n",
       "289     13  289\n",
       "290      8  290\n",
       "291     19  291\n",
       "292      2  292\n",
       "293     17  293\n",
       "294      2  294\n",
       "295      2  295\n",
       "296     11  296\n",
       "297      0  297\n",
       "298      8  298\n",
       "299      8  299\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
